<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Installing with Cloud Lifecycle Manager | SUSE OpenStack Cloud 8</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DAPS 2.99.4 (https://opensuse.github.io/daps) using SUSE XSL Stylesheets 2.0.12 (based on DocBook XSL Stylesheets 1.78.1)" /><meta name="product-name" content="SUSE OpenStack Cloud" /><meta name="product-number" content="8" /><meta name="book-title" content="Installing with Cloud Lifecycle Manager" /><meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi" /><meta name="tracker-type" content="bsc" /><meta name="tracker-bsc-component" content="Documentation" /><meta name="tracker-bsc-product" content="SUSE OpenStack Cloud 8" /><script type="text/javascript">

var protocol = window.location.protocol.toLowerCase();
if ( protocol != 'file:' ) {
  var agent = navigator.userAgent.toLowerCase();
  var wanted = ( protocol == 'https:') ? 'https' : 'http';
  var file = 'fonts.css';
  document.write('<link rel="stylesheet" type="text/css" href="' + wanted + '://static.opensuse.org/fonts/'+ file +'"></link>');
}
else {
   document.write('<link rel="stylesheet" type="text/css" href="static/css/fonts-onlylocal.css"></link>');
}

</script><noscript><link rel="stylesheet" type="text/css" href="http://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="single offline js-off"><div id="_outer-wrap"><div id="_white-bg"><div id="_header"><div id="_logo"><img src="static/images/logo.png" alt="Logo" /></div><div class="crumbs inactive"><a class="single-crumb" href="#book.install" accesskey="c"><span class="single-contents-icon"></span>Installing with Cloud Lifecycle Manager</a><div class="bubble-corner active-contents"></div></div><div class="clearme"></div></div></div><div id="_fixed-header-wrap" class="inactive"><div id="_fixed-header"><div class="crumbs inactive"><a class="single-crumb" href="#book.install" accesskey="c"><span class="single-contents-icon"></span>Show Contents: Installing with Cloud Lifecycle Manager</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="clearme"></div></div><div class="clearme"></div></div><div class="active-contents bubble"><div class="bubble-container"><div id="_bubble-toc"><ol><li class="inactive"><a href="#install_overview"><span class="number"> </span><span class="name">Installation Overview</span></a></li><li class="inactive"><a href="#preinstall"><span class="number">I </span><span class="name">Pre-Installation</span></a><ol><li class="inactive"><a href="#preinstall_overview"><span class="number">1 </span><span class="name">Overview</span></a></li><li class="inactive"><a href="#preinstall_checklist"><span class="number">2 </span><span class="name">Pre-Installation Checklist</span></a></li><li class="inactive"><a href="#cha.depl.dep_inst"><span class="number">3 </span><span class="name">Installing the Cloud Lifecycle Manager server</span></a></li><li class="inactive"><a href="#app.deploy.smt_lcm"><span class="number">4 </span><span class="name">Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)</span></a></li><li class="inactive"><a href="#cha.depl.repo_conf_lcm"><span class="number">5 </span><span class="name">Software Repository Setup</span></a></li><li class="inactive"><a href="#multipath_boot_from_san"><span class="number">6 </span><span class="name">Boot from SAN and Multipath Configuration</span></a></li><li class="inactive"><a href="#cha.install.l2gw5930"><span class="number">7 </span><span class="name">Installing the L2 Gateway Agent for the Networking Service</span></a></li></ol></li><li class="inactive"><a href="#cloudinstallation"><span class="number">II </span><span class="name">Cloud Installation</span></a><ol><li class="inactive"><a href="#cloudinstallation_overview"><span class="number">8 </span><span class="name">Overview</span></a></li><li class="inactive"><a href="#install_gui"><span class="number">9 </span><span class="name">Installing with the Install UI</span></a></li><li class="inactive"><a href="#DesignateInstallOverview"><span class="number">10 </span><span class="name">DNS Service Installation Overview</span></a></li><li class="inactive"><a href="#MagnumOverview"><span class="number">11 </span><span class="name">Magnum Overview</span></a></li><li class="inactive"><a href="#using_git"><span class="number">12 </span><span class="name">Using Git for Configuration Management</span></a></li><li class="inactive"><a href="#install_kvm"><span class="number">13 </span><span class="name">Installing Mid-scale and Entry-scale KVM</span></a></li><li class="inactive"><a href="#install_ironic_overview"><span class="number">14 </span><span class="name">Installing Baremetal (Ironic)</span></a></li><li class="inactive"><a href="#install_swift"><span class="number">15 </span><span class="name">Installation for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale Cloud with Swift Only</span></a></li><li class="inactive"><a href="#install_sles_compute"><span class="number">16 </span><span class="name">Installing SLES Compute</span></a></li><li class="inactive"><a href="#install_heat_templates"><span class="number">17 </span><span class="name">Installation of SUSE CaaS Platform Heat Templates</span></a></li><li class="inactive"><a href="#integrations"><span class="number">18 </span><span class="name">Integrations</span></a></li><li class="inactive"><a href="#troubleshooting_installation"><span class="number">19 </span><span class="name">Troubleshooting the Installation</span></a></li><li class="inactive"><a href="#esx_troubleshooting_installation"><span class="number">20 </span><span class="name">Troubleshooting the ESX</span></a></li></ol></li><li class="inactive"><a href="#post_install"><span class="number">III </span><span class="name">Post-Installation</span></a><ol><li class="inactive"><a href="#post_install_overview"><span class="number">21 </span><span class="name">Overview</span></a></li><li class="inactive"><a href="#cloud_verification"><span class="number">22 </span><span class="name">Cloud Verification</span></a></li><li class="inactive"><a href="#ui_verification"><span class="number">23 </span><span class="name">UI Verification</span></a></li><li class="inactive"><a href="#install_openstack_clients"><span class="number">24 </span><span class="name">Installing OpenStack Clients</span></a></li><li class="inactive"><a href="#tls30"><span class="number">25 </span><span class="name">Configuring Transport Layer Security (TLS)</span></a></li><li class="inactive"><a href="#config_availability_zones"><span class="number">26 </span><span class="name">Configuring Availability Zones</span></a></li><li class="inactive"><a href="#OctaviaInstall"><span class="number">27 </span><span class="name">Configuring Load Balancer as a Service</span></a></li><li class="inactive"><a href="#postinstall_checklist"><span class="number">28 </span><span class="name">Other Common Post-Installation Tasks</span></a></li></ol></li><li class="inactive"><a href="#cha.inst.trouble"><span class="number">29 </span><span class="name">Support</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_toc-bubble-wrap"></div><div id="_content" class=""><div class="documentation"><div xml:lang="en" class="book" id="book.install" lang="en"><div class="titlepage"><div><h6 class="version-info"><span class="productname"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <span class="productnumber"><span class="phrase"><span class="phrase">8</span></span></span></h6><div><h1 class="title"><em class="citetitle ">Installing with Cloud Lifecycle Manager</em> <a title="Permalink" class="permalink" href="#book.install">#</a></h1></div><div class="date"><span class="imprint-label">Publication Date: </span>07/06/2018</div></div></div><div class="toc"><dl><dt><span class="preface"><a href="#install_overview"><span class="name">Installation Overview</span></a></span></dt><dd><dl><dt><span class="section"><a href="#sec.install.more"><span class="name">For More Information</span></a></span></dt><dt><span class="section"><a href="#lcm.ardana.note"><span class="name">Note on Cloud Lifecycle Manager and Ardana</span></a></span></dt></dl></dd><dt><span class="part"><a href="#preinstall"><span class="number">I </span><span class="name">Pre-Installation</span></a></span></dt><dd><dl><dt><span class="chapter"><a href="#preinstall_overview"><span class="number">1 </span><span class="name">Overview</span></a></span></dt><dt><span class="chapter"><a href="#preinstall_checklist"><span class="number">2 </span><span class="name">Pre-Installation Checklist</span></a></span></dt><dd><dl><dt><span class="section"><a href="#idm139651569880736"><span class="number">2.1 </span><span class="name">BIOS and IPMI Settings</span></a></span></dt><dt><span class="section"><a href="#idm139651570216864"><span class="number">2.2 </span><span class="name">Network Setup and Configuration</span></a></span></dt><dt><span class="section"><a href="#idm139651565635984"><span class="number">2.3 </span><span class="name">Cloud Lifecycle Manager</span></a></span></dt><dt><span class="section"><a href="#idm139651569667600"><span class="number">2.4 </span><span class="name">Information for the <code class="filename">nic_mappings.yml</code> Input File</span></a></span></dt><dt><span class="section"><a href="#idm139651570176416"><span class="number">2.5 </span><span class="name">Control Plane</span></a></span></dt><dt><span class="section"><a href="#idm139651565282832"><span class="number">2.6 </span><span class="name">Compute Hosts</span></a></span></dt><dt><span class="section"><a href="#idm139651567795680"><span class="number">2.7 </span><span class="name">Storage Hosts</span></a></span></dt><dt><span class="section"><a href="#idm139651565548736"><span class="number">2.8 </span><span class="name">Additional Comments</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#cha.depl.dep_inst"><span class="number">3 </span><span class="name">Installing the Cloud Lifecycle Manager server</span></a></span></dt><dd><dl><dt><span class="sect1"><a href="#sec.depl.adm_inst.os"><span class="number">3.1 </span><span class="name">Starting the Operating System Installation</span></a></span></dt><dt><span class="sect1"><a href="#sec.depl.adm_inst.online_update"><span class="number">3.2 </span><span class="name">Registration and Online Updates</span></a></span></dt><dt><span class="sect1"><a href="#sec.depl.adm_inst.add_on"><span class="number">3.3 </span><span class="name">Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Extension</span></a></span></dt><dt><span class="sect1"><a href="#sec.depl.adm_inst.partitioning"><span class="number">3.4 </span><span class="name">Partitioning</span></a></span></dt><dt><span class="sect1"><a href="#sec.depl.adm_inst.user"><span class="number">3.5 </span><span class="name">Creating a User</span></a></span></dt><dt><span class="sect1"><a href="#sec.depl.adm_inst.settings"><span class="number">3.6 </span><span class="name">Installation Settings</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#app.deploy.smt_lcm"><span class="number">4 </span><span class="name">Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)</span></a></span></dt><dd><dl><dt><span class="sect1"><a href="#app.deploy.smt.install"><span class="number">4.1 </span><span class="name">SMT Installation</span></a></span></dt><dt><span class="sect1"><a href="#app.deploy.smt.config"><span class="number">4.2 </span><span class="name">SMT Configuration</span></a></span></dt><dt><span class="sect1"><a href="#app.deploy.smt.repos"><span class="number">4.3 </span><span class="name">Setting up Repository Mirroring on the SMT Server</span></a></span></dt><dt><span class="sect1"><a href="#app.deploy.smt.info"><span class="number">4.4 </span><span class="name">For More Information</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#cha.depl.repo_conf_lcm"><span class="number">5 </span><span class="name">Software Repository Setup</span></a></span></dt><dd><dl><dt><span class="sect1"><a href="#sec.depl.adm_conf.repos.product"><span class="number">5.1 </span><span class="name">Copying the Product Media Repositories</span></a></span></dt><dt><span class="sect1"><a href="#sec.depl.adm_conf.repos.scc"><span class="number">5.2 </span><span class="name">Update and Pool Repositories</span></a></span></dt><dt><span class="sect1"><a href="#sec.deploy.repo_locations"><span class="number">5.3 </span><span class="name">Repository Locations</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#multipath_boot_from_san"><span class="number">6 </span><span class="name">Boot from SAN and Multipath Configuration</span></a></span></dt><dd><dl><dt><span class="section"><a href="#multipath_overview"><span class="number">6.1 </span><span class="name">Introduction</span></a></span></dt><dt><span class="section"><a href="#idm139651566103968"><span class="number">6.2 </span><span class="name">Install Phase Configuration</span></a></span></dt><dt><span class="section"><a href="#restriction2"><span class="number">6.3 </span><span class="name">QLogic FCoE restrictions and additional configurations</span></a></span></dt><dt><span class="section"><a href="#install_boot_from_san"><span class="number">6.4 </span><span class="name">Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> ISO for nodes that support Boot from SAN</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#cha.install.l2gw5930"><span class="number">7 </span><span class="name">Installing the L2 Gateway Agent for the Networking Service</span></a></span></dt><dd><dl><dt><span class="section"><a href="#idm139651570580944"><span class="number">7.1 </span><span class="name">Sample network topology (for illustration purposes)</span></a></span></dt><dt><span class="section"><a href="#idm139651570573344"><span class="number">7.2 </span><span class="name">Networks</span></a></span></dt><dt><span class="section"><a href="#idm139651570558528"><span class="number">7.3 </span><span class="name">HPE 5930 switch configuration</span></a></span></dt><dt><span class="section"><a href="#idm139651570538976"><span class="number">7.4 </span><span class="name">Configuring the Provider Data Path Network</span></a></span></dt><dt><span class="section"><a href="#idm139651570511984"><span class="number">7.5 </span><span class="name">Enabling and Configuring the L2 Gateway Agent</span></a></span></dt><dt><span class="section"><a href="#idm139651565876752"><span class="number">7.6 </span><span class="name">Routing Between Software and Hardware - VTEP Networks</span></a></span></dt><dt><span class="section"><a href="#idm139651565860512"><span class="number">7.7 </span><span class="name">Connecting a Bare-Metal Server to the HPE 5930 Switch</span></a></span></dt><dt><span class="section"><a href="#idm139651565855696"><span class="number">7.8 </span><span class="name">Configuration on a Bare-Metal Server</span></a></span></dt><dt><span class="section"><a href="#idm139651565847088"><span class="number">7.9 </span><span class="name">NIC Bonding and IRF Configuration</span></a></span></dt><dt><span class="section"><a href="#idm139651565843536"><span class="number">7.10 </span><span class="name">Scale Numbers Tested</span></a></span></dt><dt><span class="section"><a href="#idm139651565838048"><span class="number">7.11 </span><span class="name">L2 Gateway Commands</span></a></span></dt></dl></dd></dl></dd><dt><span class="part"><a href="#cloudinstallation"><span class="number">II </span><span class="name">Cloud Installation</span></a></span></dt><dd><dl><dt><span class="chapter"><a href="#cloudinstallation_overview"><span class="number">8 </span><span class="name">Overview</span></a></span></dt><dt><span class="chapter"><a href="#install_gui"><span class="number">9 </span><span class="name">Installing with the Install UI</span></a></span></dt><dd><dl><dt><span class="section"><a href="#idm139651565764176"><span class="number">9.1 </span><span class="name">Before You Start</span></a></span></dt><dt><span class="section"><a href="#idm139651565727344"><span class="number">9.2 </span><span class="name">Preparing to Run the Install UI</span></a></span></dt><dt><span class="section"><a href="#create_csv_file"><span class="number">9.3 </span><span class="name">Optional: Creating a CSV File to Import Server Data</span></a></span></dt><dt><span class="section"><a href="#discover_servers"><span class="number">9.4 </span><span class="name">Optional: Importing Certificates for SUSE Manager and HPE OneView</span></a></span></dt><dt><span class="section"><a href="#idm139651565208272"><span class="number">9.5 </span><span class="name">Running the Install UI</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#DesignateInstallOverview"><span class="number">10 </span><span class="name">DNS Service Installation Overview</span></a></span></dt><dd><dl><dt><span class="section"><a href="#DesignateBIND"><span class="number">10.1 </span><span class="name">Installing the DNS Service with BIND</span></a></span></dt><dt><span class="section"><a href="#DesignatePowerDNS"><span class="number">10.2 </span><span class="name">Install the DNS Service with PowerDNS</span></a></span></dt><dt><span class="section"><a href="#DesignateInfoBlox"><span class="number">10.3 </span><span class="name">Installing the DNS Service with InfoBlox</span></a></span></dt><dt><span class="section"><a href="#DNS_NS"><span class="number">10.4 </span><span class="name">Configure DNS Domain and NS Records</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#MagnumOverview"><span class="number">11 </span><span class="name">Magnum Overview</span></a></span></dt><dd><dl><dt><span class="section"><a href="#MagnumArchitecture"><span class="number">11.1 </span><span class="name">Magnum Architecture</span></a></span></dt><dt><span class="section"><a href="#MagnumInstall"><span class="number">11.2 </span><span class="name">Install the Magnum Service</span></a></span></dt><dt><span class="section"><a href="#MagnumIntegrateDNS"><span class="number">11.3 </span><span class="name">Integrate Magnum with the DNS Service</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#using_git"><span class="number">12 </span><span class="name">Using Git for Configuration Management</span></a></span></dt><dd><dl><dt><span class="section"><a href="#idm139651561257664"><span class="number">12.1 </span><span class="name">Initialization on a new deployment</span></a></span></dt><dt><span class="section"><a href="#updating-configuration-including-default-config"><span class="number">12.2 </span><span class="name">Updating any configuration, including the default configuration</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#install_kvm"><span class="number">13 </span><span class="name">Installing Mid-scale and Entry-scale KVM</span></a></span></dt><dd><dl><dt><span class="section"><a href="#sec.kvm.important_notes"><span class="number">13.1 </span><span class="name">Important Notes</span></a></span></dt><dt><span class="section"><a href="#sec.kvm.prereqs"><span class="number">13.2 </span><span class="name">Before You Start</span></a></span></dt><dt><span class="section"><a href="#sec.kvm.configuration"><span class="number">13.3 </span><span class="name">Configuring Your Environment</span></a></span></dt><dt><span class="section"><a href="#sec.kvm.provision"><span class="number">13.4 </span><span class="name">Provisioning Your Baremetal Nodes</span></a></span></dt><dt><span class="section"><a href="#sec.kvm.config_processor"><span class="number">13.5 </span><span class="name">Running the Configuration Processor</span></a></span></dt><dt><span class="section"><a href="#sec.kvm.security"><span class="number">13.6 </span><span class="name">Configuring TLS</span></a></span></dt><dt><span class="section"><a href="#sec.kvm.deploy"><span class="number">13.7 </span><span class="name">Deploying the Cloud</span></a></span></dt><dt><span class="section"><a href="#sec.kvm.configure_backend"><span class="number">13.8 </span><span class="name">Configuring a Block Storage Backend (Optional)</span></a></span></dt><dt><span class="section"><a href="#sec.kvm.post_installation"><span class="number">13.9 </span><span class="name">Post-Installation Verification and Administration</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#install_ironic_overview"><span class="number">14 </span><span class="name">Installing Baremetal (Ironic)</span></a></span></dt><dd><dl><dt><span class="section"><a href="#install_ironic"><span class="number">14.1 </span><span class="name">Installation for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale Cloud with Ironic Flat Network</span></a></span></dt><dt><span class="section"><a href="#ironic_multi_control_plane"><span class="number">14.2 </span><span class="name">Ironic in Multiple Control Plane</span></a></span></dt><dt><span class="section"><a href="#ironic-provisioning"><span class="number">14.3 </span><span class="name">Provisioning Bare-Metal Nodes with Flat Network Model</span></a></span></dt><dt><span class="section"><a href="#ironic-provisioning-multi-tenancy"><span class="number">14.4 </span><span class="name">Provisioning Baremetal Nodes with Multi-Tenancy</span></a></span></dt><dt><span class="section"><a href="#ironic-system-details"><span class="number">14.5 </span><span class="name">View Ironic System Details</span></a></span></dt><dt><span class="section"><a href="#ironic-toubleshooting"><span class="number">14.6 </span><span class="name">Troubleshooting Ironic Installation</span></a></span></dt><dt><span class="section"><a href="#ironic-node-cleaning"><span class="number">14.7 </span><span class="name">Node Cleaning</span></a></span></dt><dt><span class="section"><a href="#ironic_oneview"><span class="number">14.8 </span><span class="name">Ironic and HPE OneView</span></a></span></dt><dt><span class="section"><a href="#ironic_raid_config"><span class="number">14.9 </span><span class="name">RAID Configuration for Ironic</span></a></span></dt><dt><span class="section"><a href="#ironic_audit_support"><span class="number">14.10 </span><span class="name">Audit Support for Ironic</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#install_swift"><span class="number">15 </span><span class="name">Installation for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale Cloud with Swift Only</span></a></span></dt><dd><dl><dt><span class="section"><a href="#sec.swift.important_notes"><span class="number">15.1 </span><span class="name">Important Notes</span></a></span></dt><dt><span class="section"><a href="#sec.swift.prereqs"><span class="number">15.2 </span><span class="name">Before You Start</span></a></span></dt><dt><span class="section"><a href="#sec.swift.setup_deployer"><span class="number">15.3 </span><span class="name">Setting Up the Cloud Lifecycle Manager</span></a></span></dt><dt><span class="section"><a href="#idm139651560258336"><span class="number">15.4 </span><span class="name">Configure Your Environment</span></a></span></dt><dt><span class="section"><a href="#sec.swift.provision"><span class="number">15.5 </span><span class="name">Provisioning Your Baremetal Nodes</span></a></span></dt><dt><span class="section"><a href="#sec.swift.config_processor"><span class="number">15.6 </span><span class="name">Running the Configuration Processor</span></a></span></dt><dt><span class="section"><a href="#sec.swift.deploy"><span class="number">15.7 </span><span class="name">Deploying the Cloud</span></a></span></dt><dt><span class="section"><a href="#sec.swift.post_installation"><span class="number">15.8 </span><span class="name">Post-Installation Verification and Administration</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#install_sles_compute"><span class="number">16 </span><span class="name">Installing SLES Compute</span></a></span></dt><dd><dl><dt><span class="section"><a href="#sles_overview"><span class="number">16.1 </span><span class="name">SLES Compute Node Installation Overview</span></a></span></dt><dt><span class="section"><a href="#sles_support"><span class="number">16.2 </span><span class="name">SLES Support</span></a></span></dt><dt><span class="section"><a href="#install_sles"><span class="number">16.3 </span><span class="name">Using the Cloud Lifecycle Manager to Deploy SLES Compute Nodes</span></a></span></dt><dt><span class="section"><a href="#provisioning_sles"><span class="number">16.4 </span><span class="name">Provisioning SLES Yourself</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#install_heat_templates"><span class="number">17 </span><span class="name">Installation of SUSE CaaS Platform Heat Templates</span></a></span></dt><dd><dl><dt><span class="section"><a href="#sec.heat.templates.install"><span class="number">17.1 </span><span class="name">SUSE CaaS Platform Heat Installation Procedure</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#integrations"><span class="number">18 </span><span class="name">Integrations</span></a></span></dt><dd><dl><dt><span class="section"><a href="#config_3par"><span class="number">18.1 </span><span class="name">Configuring for 3PAR Block Storage Backend</span></a></span></dt><dt><span class="section"><a href="#ironic_oneview_integration"><span class="number">18.2 </span><span class="name">Ironic HPE OneView Integration</span></a></span></dt><dt><span class="section"><a href="#ses.integration"><span class="number">18.3 </span><span class="name">SUSE Enterprise Storage Integration</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#troubleshooting_installation"><span class="number">19 </span><span class="name">Troubleshooting the Installation</span></a></span></dt><dd><dl><dt><span class="section"><a href="#sec.trouble-deployer_setup"><span class="number">19.1 </span><span class="name">Issues during Cloud Lifecycle Manager Setup</span></a></span></dt><dt><span class="section"><a href="#sec.trouble-config_processor"><span class="number">19.2 </span><span class="name">Issues while Updating Configuration Files</span></a></span></dt><dt><span class="section"><a href="#sec.trouble-deploy_cloud"><span class="number">19.3 </span><span class="name">Issues while Deploying the Cloud</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#esx_troubleshooting_installation"><span class="number">20 </span><span class="name">Troubleshooting the ESX</span></a></span></dt><dd><dl><dt><span class="section"><a href="#idm139651559680896"><span class="number">20.1 </span><span class="name">Issue: Ardana-ux-services.service is not running</span></a></span></dt><dt><span class="section"><a href="#idm139651559674672"><span class="number">20.2 </span><span class="name">Issue: ESX Cluster shows UNKNOWN in Operations Console</span></a></span></dt><dt><span class="section"><a href="#idm139651559669456"><span class="number">20.3 </span><span class="name">Issue: Unable to view the VM console in Horizon UI</span></a></span></dt></dl></dd></dl></dd><dt><span class="part"><a href="#post_install"><span class="number">III </span><span class="name">Post-Installation</span></a></span></dt><dd><dl><dt><span class="chapter"><a href="#post_install_overview"><span class="number">21 </span><span class="name">Overview</span></a></span></dt><dt><span class="chapter"><a href="#cloud_verification"><span class="number">22 </span><span class="name">Cloud Verification</span></a></span></dt><dd><dl><dt><span class="section"><a href="#api_verification"><span class="number">22.1 </span><span class="name">API Verification</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#ui_verification"><span class="number">23 </span><span class="name">UI Verification</span></a></span></dt><dd><dl><dt><span class="section"><a href="#sec.verify_block_storage.volume"><span class="number">23.1 </span><span class="name">Verifying Your Block Storage Backend</span></a></span></dt><dt><span class="section"><a href="#sec.verify_block_storage.swift"><span class="number">23.2 </span><span class="name">Verify the Object Storage (Swift) Operations</span></a></span></dt><dt><span class="section"><a href="#upload_image"><span class="number">23.3 </span><span class="name">Uploading an Image for Use</span></a></span></dt><dt><span class="section"><a href="#create_extnet"><span class="number">23.4 </span><span class="name">Creating an External Network</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#install_openstack_clients"><span class="number">24 </span><span class="name">Installing OpenStack Clients</span></a></span></dt><dt><span class="chapter"><a href="#tls30"><span class="number">25 </span><span class="name">Configuring Transport Layer Security (TLS)</span></a></span></dt><dd><dl><dt><span class="section"><a href="#idm139651559329872"><span class="number">25.1 </span><span class="name">Configuring TLS in the input model</span></a></span></dt><dt><span class="section"><a href="#idm139651559305808"><span class="number">25.2 </span><span class="name">User-provided certificates and trust chains</span></a></span></dt><dt><span class="section"><a href="#idm139651559292624"><span class="number">25.3 </span><span class="name">Edit the input model to include your certificate files</span></a></span></dt><dt><span class="section"><a href="#sec.generate-certificate"><span class="number">25.4 </span><span class="name">Generate a self-signed CA</span></a></span></dt><dt><span class="section"><a href="#idm139651559263120"><span class="number">25.5 </span><span class="name">Generate a certificate signing request</span></a></span></dt><dt><span class="section"><a href="#idm139651559257920"><span class="number">25.6 </span><span class="name">Generate a server certificate</span></a></span></dt><dt><span class="section"><a href="#sec.upload-toclm"><span class="number">25.7 </span><span class="name">Upload to the Cloud Lifecycle Manager</span></a></span></dt><dt><span class="section"><a href="#idm139651559221264"><span class="number">25.8 </span><span class="name">Configuring the cipher suite</span></a></span></dt><dt><span class="section"><a href="#idm139651559216528"><span class="number">25.9 </span><span class="name">Testing</span></a></span></dt><dt><span class="section"><a href="#idm139651559211632"><span class="number">25.10 </span><span class="name">Verifying that the trust chain is correctly deployed</span></a></span></dt><dt><span class="section"><a href="#idm139651559208208"><span class="number">25.11 </span><span class="name">Turning TLS on or off</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#config_availability_zones"><span class="number">26 </span><span class="name">Configuring Availability Zones</span></a></span></dt><dt><span class="chapter"><a href="#OctaviaInstall"><span class="number">27 </span><span class="name">Configuring Load Balancer as a Service</span></a></span></dt><dd><dl><dt><span class="section"><a href="#idm139651559174752"><span class="number">27.1 </span><span class="name">Prerequisites</span></a></span></dt><dt><span class="section"><a href="#idm139651559171152"><span class="number">27.2 </span><span class="name">Octavia Load Balancing Provider</span></a></span></dt><dt><span class="section"><a href="#idm139651559154416"><span class="number">27.3 </span><span class="name">Setup of prerequisites</span></a></span></dt><dt><span class="section"><a href="#idm139651559055216"><span class="number">27.4 </span><span class="name">Create Load Balancers</span></a></span></dt><dt><span class="section"><a href="#idm139651559026112"><span class="number">27.5 </span><span class="name">Create Floating IPs for Load Balancer</span></a></span></dt><dt><span class="section"><a href="#idm139651559017792"><span class="number">27.6 </span><span class="name">Testing the Octavia Load Balancer</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#postinstall_checklist"><span class="number">28 </span><span class="name">Other Common Post-Installation Tasks</span></a></span></dt><dd><dl><dt><span class="section"><a href="#idm139651558997520"><span class="number">28.1 </span><span class="name">Determining Your User Credentials</span></a></span></dt><dt><span class="section"><a href="#idm139651558990160"><span class="number">28.2 </span><span class="name">Configure your Cloud Lifecycle Manager to use the command-line tools</span></a></span></dt><dt><span class="section"><a href="#idm139651558985584"><span class="number">28.3 </span><span class="name">Protect home directory</span></a></span></dt><dt><span class="section"><a href="#idm139651558981248"><span class="number">28.4 </span><span class="name">Back up Your SSH Keys</span></a></span></dt><dt><span class="section"><a href="#idm139651558978752"><span class="number">28.5 </span><span class="name">Retrieving Service Endpoints</span></a></span></dt><dt><span class="section"><a href="#idm139651558971152"><span class="number">28.6 </span><span class="name">Other Common Post-Installation Tasks</span></a></span></dt></dl></dd></dl></dd><dt><span class="chapter"><a href="#cha.inst.trouble"><span class="number">29 </span><span class="name">Support</span></a></span></dt><dd><dl><dt><span class="sect1"><a href="#sec.depl.trouble.faq"><span class="number">29.1 </span><span class="name">FAQ</span></a></span></dt><dt><span class="sect1"><a href="#sec.installation.trouble.support"><span class="number">29.2 </span><span class="name">Support</span></a></span></dt></dl></dd></dl></div><div class="list-of-figures"><div class="toc-title">List of Figures</div><dl><dt><span class="figure"><a href="#idm139651570571888"><span class="number">7.1 </span><span class="name">L2 Gateway and 5930 Switch</span></a></span></dt><dt><span class="figure"><a href="#magnum_service_arch_diagram"><span class="number">11.1 </span><span class="name">Service Architecture Diagram for Kubernetes</span></a></span></dt><dt><span class="figure"><a href="#idm139651560879664"><span class="number">14.1 </span><span class="name">Architecture of Multiple Control Plane with Ironic</span></a></span></dt></dl></div><div class="list-of-tables"><div class="toc-title">List of Tables</div><dl><dt><span class="table"><a href="#cp_1"><span class="number">2.1 </span><span class="name">Control Plane 1</span></a></span></dt><dt><span class="table"><a href="#cp_2"><span class="number">2.2 </span><span class="name">Control Plane 2</span></a></span></dt><dt><span class="table"><a href="#cp_3"><span class="number">2.3 </span><span class="name">Control Plane 3</span></a></span></dt><dt><span class="table"><a href="#idm139651565329888"><span class="number">5.1 </span><span class="name">Local Product Repositories for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span></a></span></dt><dt><span class="table"><a href="#tab.smt.repos_local"><span class="number">5.2 </span><span class="name">SMT Repositories Hosted on the Administration Server</span></a></span></dt><dt><span class="table"><a href="#tab.depl.adm_conf.susemgr-repos"><span class="number">5.3 </span><span class="name">SUSE Manager Repositories (Channels)</span></a></span></dt><dt><span class="table"><a href="#tab.depl.adm_conf.local-repos"><span class="number">5.4 </span><span class="name">Repository Locations on the Cloud Lifecycle Manager server</span></a></span></dt><dt><span class="table"><a href="#DNSBackendTable"><span class="number">10.1 </span><span class="name">DNS Backends</span></a></span></dt><dt><span class="table"><a href="#table_ebc_x5v_jz"><span class="number">11.1 </span><span class="name">Data</span></a></span></dt><dt><span class="table"><a href="#table_fst_gxv_jz"><span class="number">11.2 </span><span class="name">Interfaces</span></a></span></dt><dt><span class="table"><a href="#security_groups_table"><span class="number">11.3 </span><span class="name">Security Groups</span></a></span></dt><dt><span class="table"><a href="#network_ports_table"><span class="number">11.4 </span><span class="name">Network Ports</span></a></span></dt></dl></div><div class="list-of-examples"><div class="toc-title">List of Examples</div><dl><dt><span class="example"><a href="#idm139651559764048"><span class="number">18.1 </span><span class="name">ses_config.yml Example</span></a></span></dt><dt><span class="example"><a href="#sec.tls.private_metadata"><span class="number">25.1 </span><span class="name">Certificate request file</span></a></span></dt><dt><span class="example"><a href="#idm139651558915616"><span class="number">29.1 </span><span class="name">Testing a Fix for Nova</span></a></span></dt></dl></div><div class="preface " id="install_overview"><div class="titlepage"><div><div><h1 class="title"><span class="number"> </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installation Overview</span> <a title="Permalink" class="permalink" href="#install_overview">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#sec.install.more"><span class="name">For More Information</span></a></span></dt><dt><span class="section"><a href="#lcm.ardana.note"><span class="name">Note on Cloud Lifecycle Manager and Ardana</span></a></span></dt></dl></div></div><p>
  Before jumping into your installation, we recommend taking the time to read
  through our documentation to get an overview of the sample configurations
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> offers. We have highly tuned example configurations for each
  of these Cloud models:
 </p><div class="informaltable"><table border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Name</th><th>Location</th></tr></thead><tbody><tr><td>
      <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”, Section 10.3 “KVM Examples”, Section 10.3.1 “Entry-Scale Cloud”</span>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-kvm</code>
     </td></tr><tr><td>
      <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”, Section 10.3 “KVM Examples”, Section 10.3.2 “Entry Scale Cloud with Metering and Monitoring Services”</span>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-kvm-mml</code>
     </td></tr><tr><td><span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”, Section 10.4 “ESX Examples”, Section 10.4.1 “Single-Region Entry-Scale Cloud with a Mix of KVM and ESX Hypervisors”</span>
     </td><td><code class="filename">~/openstack/examples/entry-scale-esx-kvm</code>
     </td></tr><tr><td>
      <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”, Section 10.4 “ESX Examples”, Section 10.4.2 “Single-Region Entry-Scale Cloud with Metering and Monitoring Services, and a Mix of KVM and ESX Hypervisors”</span>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-esx-kvm-mml</code>
     </td></tr><tr><td>
      <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”, Section 10.5 “Swift Examples”, Section 10.5.1 “Entry-scale Swift Model”</span>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-swift</code>
     </td></tr><tr><td>
      <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”, Section 10.6 “Ironic Examples”, Section 10.6.1 “Entry-Scale Cloud with Ironic Flat Network”</span>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-ironic-flat-network</code>
     </td></tr><tr><td>
      <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”, Section 10.6 “Ironic Examples”, Section 10.6.2 “Entry-Scale Cloud with Ironic Multi-Tenancy”</span>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-ironic-multi-tenancy</code>
     </td></tr><tr><td><span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”, Section 10.3 “KVM Examples”, Section 10.3.3 “Single-Region Mid-Size Model”</span>
     </td><td>
      <code class="filename">~/openstack/examples/mid-scale-kvm</code>
     </td></tr></tbody></table></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”, Section 10.3 “KVM Examples”, Section 10.3.1 “Entry-Scale Cloud”</span>
   </p></li><li class="listitem "><p>
    <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”, Section 10.4 “ESX Examples”</span>
   </p></li><li class="listitem "><p>
    <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”, Section 10.5 “Swift Examples”, Section 10.5.1 “Entry-scale Swift Model”</span>
   </p></li><li class="listitem "><p>
    <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”, Section 10.3 “KVM Examples”, Section 10.3.3 “Single-Region Mid-Size Model”</span>
   </p></li></ul></div><p>
  <span class="bold"><strong>Using the Command-line</strong></span>
 </p><p>
  You should use the command-line if:
 </p><div class="itemizedlist " id="idg-installation-installation-installation_overview-xml-7"><ul class="itemizedlist"><li class="listitem "><p>
    You are installing a more complex or large-scale cloud.
   </p></li><li class="listitem "><p>
    You need to use availability zones or the server groups functionality of
    the cloud model. Also see <a class="xref" href="#sec.install.more" title="1. For More Information">Section 1, “For More Information”</a>.
   </p></li><li class="listitem "><p>
    You want to customize the cloud configuration beyond the tuned defaults
    that <span class="phrase"><span class="phrase">SUSE</span></span> provides out of the box.
   </p></li><li class="listitem "><p>
    You need deeper customizations than are possible to express using the GUI.
   </p></li></ul></div><p>
  Instructions for installing via the command-line are here:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="#install_kvm" title="Chapter 13. Installing Mid-scale and Entry-scale KVM">Chapter 13, <em>Installing Mid-scale and Entry-scale KVM</em></a>
   </p></li></ul></div><div class="sect1" id="sec.install.more"><div class="titlepage"><div><div><h2 class="title" id="sec.install.more"><span class="number">1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For More Information</span> <a title="Permalink" class="permalink" href="#sec.install.more">#</a></h2></div></div></div><p>
   Other useful documents that will help you understand, plan, configure, and
   install your cloud are listed below.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <a class="xref" href="#using_git" title="Chapter 12. Using Git for Configuration Management">Chapter 12, <em>Using Git for Configuration Management</em></a>
    </p></li><li class="listitem "><p>
     <a class="xref" href="#troubleshooting_installation" title="Chapter 19. Troubleshooting the Installation">Chapter 19, <em>Troubleshooting the Installation</em></a>
    </p></li><li class="listitem "><p>
     <a class="xref" href="#cloud_verification" title="Chapter 22. Cloud Verification">Chapter 22, <em>Cloud Verification</em></a>
    </p></li><li class="listitem "><p>
     <a class="xref" href="#postinstall_checklist" title="Chapter 28. Other Common Post-Installation Tasks">Chapter 28, <em>Other Common Post-Installation Tasks</em></a>
    </p></li></ul></div></div><div class="sect1" id="lcm.ardana.note"><div class="titlepage"><div><div><h2 class="title" id="lcm.ardana.note"><span class="number">2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Note on Cloud Lifecycle Manager and Ardana</span> <a title="Permalink" class="permalink" href="#lcm.ardana.note">#</a></h2></div></div></div><p>
  Ardana is an open-source project and a generalized lifecycle framework.
  Cloud Lifecycle Manager is based on Ardana, but delivers the lifecycle management
  functionality required by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>. Due to this
  relationship, some Cloud Lifecycle Manager commands
  contain references to Ardana.
 </p></div></div><div class="part" id="preinstall"><div class="titlepage"><div><div><h1 class="title"><span class="number">Part I </span><span class="name">Pre-Installation </span><a title="Permalink" class="permalink" href="#preinstall">#</a></h1></div></div></div><div class="toc"><dl><dt><span class="chapter"><a href="#preinstall_overview"><span class="number">1 </span><span class="name">Overview</span></a></span></dt><dd class="toc-abstract"><p>
   To ensure that your environment meets the requirements of the cloud model you
   choose, see the check list in <a class="xref" href="#preinstall_checklist" title="Chapter 2. Pre-Installation Checklist">Chapter 2, <em>Pre-Installation Checklist</em></a>.
  </p></dd><dt><span class="chapter"><a href="#preinstall_checklist"><span class="number">2 </span><span class="name">Pre-Installation Checklist</span></a></span></dt><dd class="toc-abstract"><p>
  This checklist is focused on the Entry-scale KVM model but you can alter it
  to fit the example configuration you choose for your cloud.
 </p></dd><dt><span class="chapter"><a href="#cha.depl.dep_inst"><span class="number">3 </span><span class="name">Installing the Cloud Lifecycle Manager server</span></a></span></dt><dd class="toc-abstract"><p>
    In this chapter you will learn how to install the Cloud Lifecycle Manager from
    scratch. It will run on SUSE Linux Enterprise Server 12 SP3 and include the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
    extension and, optionally, the Subscription Management Tool (SMT) server.
   </p></dd><dt><span class="chapter"><a href="#app.deploy.smt_lcm"><span class="number">4 </span><span class="name">Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)</span></a></span></dt><dd class="toc-abstract"><p>
    One way to provide the repositories needed to set up the nodes in
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is to install a Subscription Management Tool (SMT) server on the Cloud Lifecycle Manager server, and then mirror all
    repositories from SUSE Customer Center via this server. Installing an SMT server
    on the Cloud Lifecycle Manager server is optional. If your organization already provides an
    SMT server or a SUSE Manager server that can be accessed from the
    Cloud Lifecycle Manager server, skip this step.
   </p></dd><dt><span class="chapter"><a href="#cha.depl.repo_conf_lcm"><span class="number">5 </span><span class="name">Software Repository Setup</span></a></span></dt><dd class="toc-abstract"><p>Software repositories containing products, extensions, and the respective updates for all software need to be available to all nodes in SUSE OpenStack Cloud in order to complete the deployment. These can be managed manually, or they can be hosted on the Cloud Lifecycle Manager server. In this config…</p></dd><dt><span class="chapter"><a href="#multipath_boot_from_san"><span class="number">6 </span><span class="name">Boot from SAN and Multipath Configuration</span></a></span></dt><dt><span class="chapter"><a href="#cha.install.l2gw5930"><span class="number">7 </span><span class="name">Installing the L2 Gateway Agent for the Networking Service</span></a></span></dt><dd class="toc-abstract"><p>The L2 gateway is a service plug-in to the Neutron networking service that allows two L2 networks to be seamlessly connected to create a single L2 broadcast domain. The initial implementation provides for the ability to connect a virtual Neutron VxLAN network to a physical VLAN using a VTEP-capable …</p></dd></dl></div><div class="chapter " id="preinstall_overview"><div class="titlepage"><div><div><h2 class="title"><span class="number">1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Overview</span> <a title="Permalink" class="permalink" href="#preinstall_overview">#</a></h2></div></div></div><div class="line"></div><p>
   To ensure that your environment meets the requirements of the cloud model you
   choose, see the check list in <a class="xref" href="#preinstall_checklist" title="Chapter 2. Pre-Installation Checklist">Chapter 2, <em>Pre-Installation Checklist</em></a>.
  </p><p>
   When you have decided on a configuration to choose for your cloud
   and you have gone through the pre-installation steps, you have two options
   for installation:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     You can use a GUI that runs in your Web browser.
    </p></li><li class="listitem "><p>
     You can install via the command-line that exposes the full power and
     flexibility of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>.
    </p></li></ul></div><p>
   <span class="bold"><strong>Using the GUI</strong></span>
  </p><p>
   You should use the GUI if:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     You are not planning to deploy availability zones or use L3 segmentation
     in your initial deployment.
    </p></li><li class="listitem "><p>
     You are satisfied with the tuned <span class="phrase"><span class="phrase">SUSE</span></span>-default <span class="productname">OpenStack</span> configuration.
    </p></li></ul></div><p>
   Instructions for installing via the GUI are here.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <a class="xref" href="#install_gui" title="Chapter 9. Installing with the Install UI">Chapter 9, <em>Installing with the Install UI</em></a>
    </p></li></ul></div><p>
   Note that reconfiguring your cloud, at the moment, can only be done via
   the command-line. The GUI installer is for initial installation only.
  </p></div><div class="chapter " id="preinstall_checklist"><div class="titlepage"><div><div><h2 class="title"><span class="number">2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Pre-Installation Checklist</span> <a title="Permalink" class="permalink" href="#preinstall_checklist">#</a></h2></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#idm139651569880736"><span class="number">2.1 </span><span class="name">BIOS and IPMI Settings</span></a></span></dt><dt><span class="section"><a href="#idm139651570216864"><span class="number">2.2 </span><span class="name">Network Setup and Configuration</span></a></span></dt><dt><span class="section"><a href="#idm139651565635984"><span class="number">2.3 </span><span class="name">Cloud Lifecycle Manager</span></a></span></dt><dt><span class="section"><a href="#idm139651569667600"><span class="number">2.4 </span><span class="name">Information for the <code class="filename">nic_mappings.yml</code> Input File</span></a></span></dt><dt><span class="section"><a href="#idm139651570176416"><span class="number">2.5 </span><span class="name">Control Plane</span></a></span></dt><dt><span class="section"><a href="#idm139651565282832"><span class="number">2.6 </span><span class="name">Compute Hosts</span></a></span></dt><dt><span class="section"><a href="#idm139651567795680"><span class="number">2.7 </span><span class="name">Storage Hosts</span></a></span></dt><dt><span class="section"><a href="#idm139651565548736"><span class="number">2.8 </span><span class="name">Additional Comments</span></a></span></dt></dl></div></div><div id="idm139651567757616" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   The formatting of this page facilitates printing it out and using it to
   record details of your setup.
  </p></div><p>
  This checklist is focused on the Entry-scale KVM model but you can alter it
  to fit the example configuration you choose for your cloud.
 </p><div class="sect1" id="idm139651569880736"><div class="titlepage"><div><div><h2 class="title" id="idm139651569880736"><span class="number">2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">BIOS and IPMI Settings</span> <a title="Permalink" class="permalink" href="#idm139651569880736">#</a></h2></div></div></div><p>
   Ensure that the following BIOS and IPMI settings are applied to each
   bare-metal server:
  </p><div class="informaltable"><table border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>☐</th><th>Item</th></tr></thead><tbody><tr><td> </td><td class="auto-generated"> </td></tr><tr><td> </td><td>
       Choose either UEFI or Legacy BIOS in the BIOS settings
      </td></tr><tr><td> </td><td>
       <p>
        Verify the Date and Time settings in the BIOS.
       </p>
       <div id="idm139651565421376" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
         <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installs and runs with UTC, not local time.
        </p></div>
      </td></tr><tr><td> </td><td>Ensure that Wake-on-LAN is disabled in the BIOS</td></tr><tr><td> </td><td>
       Ensure that the NIC port to be used for PXE installation has PXE
       enabled in the BIOS
      </td></tr><tr><td> </td><td>Ensure that all other NIC ports have PXE disabled in the BIOS</td></tr><tr><td> </td><td>
       Ensure all hardware in the server not directly used by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is
       disabled
      </td></tr></tbody></table></div></div><div class="sect1" id="idm139651570216864"><div class="titlepage"><div><div><h2 class="title" id="idm139651570216864"><span class="number">2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Setup and Configuration</span> <a title="Permalink" class="permalink" href="#idm139651570216864">#</a></h2></div></div></div><p>
   Before installing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, the following networks must be provisioned and
   tested. The networks are not installed or managed by the Cloud. You must
   install and manage the networks as documented in
   <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”</span>.
  </p><p>
   Note that if you want a pluggable IPAM driver, it must be specified at
   install time. Only with a clean install of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> can you specify a
   different IPAM driver. If upgrading, you must use the default driver.
   More information can be found in
   <span class="intraxref">Book “Operations Guide”, Chapter 9 “Managing Networking”, Section 9.3 “Networking Service Overview”, Section 9.3.7 “Using IPAM Drivers in the Networking Service”</span>.
  </p><p>
   Use these checklists to confirm and record your network configuration
   information.
  </p><p>
   <span class="bold"><strong>Router</strong></span>
  </p><p>
   The IP router used with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> must support the updated of its ARP table
   through gratuitous ARP packets.
  </p><p>
   <span class="bold"><strong>PXE Installation Network</strong></span>
  </p><p>
   When provisioning the IP range, allocate sufficient IP addresses to cover
   both the current number of servers and any planned expansion. Use the
   following table to help calculate the requirements:
  </p><div class="informaltable"><table border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Instance</th><th>Description</th><th>IPs</th></tr></thead><tbody><tr><td>Deployer O/S</td><td> </td><td>1</td></tr><tr><td>Controller server O/S (x3)</td><td> </td><td>3</td></tr><tr><td>Compute servers (2nd thru 100th)</td><td>single IP per server</td><td> </td></tr><tr><td>block storage host servers</td><td>single IP per server</td><td> </td></tr></tbody></table></div><div class="informaltable"><table border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>☐</th><th>Item</th><th>Value</th></tr></thead><tbody><tr><td> </td><td>Network is untagged</td><td> </td></tr><tr><td> </td><td>No DHCP servers other than <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> are on the network</td><td> </td></tr><tr><td> </td><td>Switch PVID used to map any "internal" VLANs to untagged</td><td> </td></tr><tr><td> </td><td>Routable to the IPMI network</td><td> </td></tr><tr><td> </td><td>IP CIDR</td><td> </td></tr><tr><td> </td><td>IP Range (Usable IPs)</td><td>
       <p>
        begin:
       </p>
       <p>
        end:
       </p>
      </td></tr><tr><td> </td><td>Default IP Gateway</td><td> </td></tr></tbody></table></div><p>
   <span class="bold"><strong>Management Network</strong></span>
  </p><p>
   The management network is the backbone used for the majority of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   management communications. Control messages are exchanged between the
   Controllers, Compute hosts, and Cinder backends through this
   network. In addition to the control flows, the management network is also
   used to transport Swift and iSCSI based Cinder block storage
   traffic between servers.
  </p><p>
   When provisioning the IP Range, allocate sufficient IP addresses to cover
   both the current number of servers and any planned expansion. Use the
   following table to help calculate the requirements:
  </p><div class="informaltable"><table border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Instance</th><th>Description</th><th>IPs</th></tr></thead><tbody><tr><td>Controller server O/S (x3)</td><td> </td><td>3</td></tr><tr><td>Controller VIP</td><td> </td><td>1</td></tr><tr><td>Compute servers (2nd through 100th)</td><td>single IP per server</td><td> </td></tr><tr><td>VM servers</td><td>single IP per server</td><td> </td></tr><tr><td>VIP per cluster</td><td> </td><td> </td></tr></tbody></table></div><div class="informaltable"><table border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>☐</th><th>Item</th><th>Value</th></tr></thead><tbody><tr><td> </td><td>Network is untagged</td><td> </td></tr><tr><td> </td><td>No DHCP servers other than <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> are on the network</td><td> </td></tr><tr><td> </td><td>Switch PVID used to map any "internal" VLANs to untagged</td><td> </td></tr><tr><td> </td><td>IP CIDR</td><td> </td></tr><tr><td> </td><td>IP Range (Usable IPs)</td><td>
       <p>
        begin:
       </p>
       <p>
        end:
       </p>
      </td></tr><tr><td> </td><td>Default IP Gateway</td><td> </td></tr><tr><td> </td><td>VLAN ID</td><td> </td></tr></tbody></table></div><p>
   <span class="bold"><strong>IPMI Network</strong></span>
  </p><p>
   The IPMI network is used to connect the IPMI interfaces on the servers that
   are assigned for use with implementing the cloud.  This network is used by
   Cobbler to control the state of the servers
   during baremetal deployments.
  </p><div class="informaltable"><table border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>☐</th><th>Item</th><th>Value</th></tr></thead><tbody><tr><td> </td><td>Network is untagged</td><td> </td></tr><tr><td> </td><td>Routable to the Management Network</td><td> </td></tr><tr><td> </td><td>IP Subnet</td><td> </td></tr><tr><td> </td><td>Default IP Gateway</td><td> </td></tr></tbody></table></div><p>
   <span class="bold"><strong>External API Network</strong></span>
  </p><p>
   The External network is used to connect <span class="productname">OpenStack</span> endpoints to an external
   public network such as a company’s intranet or the public internet in the
   case of a public cloud provider.
  </p><p>
   When provisioning the IP Range, allocate sufficient IP addresses to cover
   both the current number of servers and any planned expansion. Use the
   following table to help calculate the requirements.
  </p><div class="informaltable"><table border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Instance</th><th>Description</th><th>IPs</th></tr></thead><tbody><tr><td>Controller server O/S (x3)</td><td> </td><td>3</td></tr><tr><td>Controller VIP</td><td> </td><td>1</td></tr></tbody></table></div><div class="informaltable"><table border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>☐</th><th>Item</th><th>Value</th></tr></thead><tbody><tr><td> </td><td>VLAN Tag assigned:</td><td> </td></tr><tr><td> </td><td>IP CIDR</td><td> </td></tr><tr><td> </td><td>IP Range (Usable IPs)</td><td>
       <p>
        begin:
       </p>
       <p>
        end:
       </p>
      </td></tr><tr><td> </td><td>Default IP Gateway</td><td> </td></tr><tr><td> </td><td>VLAN ID</td><td> </td></tr></tbody></table></div><p>
   <span class="bold"><strong>External VM Network</strong></span>
  </p><p>
   The External VM network is used to connect cloud instances to an external
   public network such as a company’s intranet or the public internet in the
   case of a public cloud provider. The external network has a predefined range
   of Floating IPs which are assigned to individual instances to enable
   communications to and from the instance to the assigned corporate
   intranet/internet. There should be a route between the External VM and
   External API networks so that instances provisioned in the cloud, may access
   the Cloud API endpoints, using the instance floating IPs.
  </p><div class="informaltable"><table border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>☐</th><th>Item</th><th>Value</th></tr></thead><tbody><tr><td> </td><td>VLAN Tag assigned:</td><td> </td></tr><tr><td> </td><td>IP CIDR</td><td> </td></tr><tr><td> </td><td>IP Range (Usable IPs)</td><td>
       <p>
        begin:
       </p>
       <p>
        end:
       </p>
      </td></tr><tr><td> </td><td>Default IP Gateway</td><td> </td></tr><tr><td> </td><td>VLAN ID</td><td> </td></tr></tbody></table></div></div><div class="sect1" id="idm139651565635984"><div class="titlepage"><div><div><h2 class="title" id="idm139651565635984"><span class="number">2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cloud Lifecycle Manager</span> <a title="Permalink" class="permalink" href="#idm139651565635984">#</a></h2></div></div></div><p>
   This server contains the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installer, which is based on Git, Ansible,
   and Cobbler.
  </p><div class="informaltable"><table border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>☐</th><th>Item</th><th>Value</th></tr></thead><tbody><tr><td> </td><td>
       Disk Requirement: Single 8GB disk needed per the
       <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 2 “Hardware and Software Support Matrix”</span>
      </td><td> </td></tr><tr><td> </td><td>
       <a class="xref" href="#sec.depl.adm_inst.add_on" title="3.3. Installing the SUSE OpenStack Cloud Extension">Section 3.3, “Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Extension”</a>
      </td><td> </td></tr><tr><td> </td><td>
       Ensure your local DNS nameserver is placed into your
       <code class="filename">/etc/resolv.conf</code> file
      </td><td> </td></tr><tr><td> </td><td>Install and configure NTP for your environment</td><td> </td></tr><tr><td> </td><td>
       Ensure your NTP server(s) is placed into your
       <code class="filename">/etc/ntp.conf</code> file
      </td><td> </td></tr><tr><td> </td><td>NTP time source:</td><td> </td></tr><tr><td> </td><td> </td><td> </td></tr></tbody></table></div></div><div class="sect1" id="idm139651569667600"><div class="titlepage"><div><div><h2 class="title" id="idm139651569667600"><span class="number">2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Information for the <code class="filename">nic_mappings.yml</code> Input File</span> <a title="Permalink" class="permalink" href="#idm139651569667600">#</a></h2></div></div></div><p>
   Log on to each type of physical server you have and issue platform-appropriate
   commands to identify the <code class="literal">bus-address</code> and
   <code class="literal">port-num</code> values that may be required. For example, run the
   following command:
  </p><div class="verbatim-wrap"><pre class="screen">sudo lspci -D | grep -i net</pre></div><p>
   and enter this information in the space below. Use this information for the
   <code class="literal">bus-address</code> value in your
   <code class="literal">nic_mappings.yml</code> file.
  </p><div class="informaltable"><table border="1"><colgroup><col class="c1" /></colgroup><thead><tr><th>NIC Adapter PCI Bus Address Output</th></tr></thead><tbody><tr><td>
<div class="verbatim-wrap"><pre class="screen">











</pre></div>
      </td></tr></tbody></table></div><p>
   To find the <code class="literal">port-num</code> use:
  </p><div class="verbatim-wrap"><pre class="screen">cat /sys/class/net/&lt;device name&gt;/dev_port</pre></div><p>
   where the 'device-name' is the name of the device <span class="bold"><strong>currently mapped</strong></span> to this address, not necessarily the
   name of the device <span class="bold"><strong>to be mapped</strong></span>. Enter the
   information for your system in the space below.
  </p><div class="informaltable"><table border="1"><colgroup><col class="c1" /></colgroup><thead><tr><th>Network Device Port Number Output</th></tr></thead><tbody><tr><td>
<div class="verbatim-wrap"><pre class="screen">











</pre></div>
      </td></tr></tbody></table></div></div><div class="sect1" id="idm139651570176416"><div class="titlepage"><div><div><h2 class="title" id="idm139651570176416"><span class="number">2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Control Plane</span> <a title="Permalink" class="permalink" href="#idm139651570176416">#</a></h2></div></div></div><p>
   The Control Plane consists of at least three servers in a highly available
   cluster that host the core <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> services including Nova, Keystone,
   Glance, Cinder, Heat, Neutron, Swift, Ceilometer, and
   Horizon. Additional services include mariadb, ip-cluster, apache2,
   rabbitmq, memcached, zookeeper, kafka, storm, monasca, logging, and cmc.
  </p><div id="idm139651565900928" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    To mitigate the <span class="quote">“<span class="quote">split-brain</span>”</span> situation described in
    <span class="intraxref">Book “Operations Guide”, Chapter 15 “Troubleshooting Issues”, Section 15.4 “Network Service Troubleshooting”</span> it is recommended
    that you have HA network configuration with Multi-Chassis Link Aggregation
    (MLAG) and NIC bonding configured for all the controllers to deliver
    system-level redundancy as well network-level resiliency. Also reducing
    the ARP timeout on the TOR switches will help.
   </p></div><div class="table" id="cp_1"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 2.1: </span><span class="name">Control Plane 1 </span><a title="Permalink" class="permalink" href="#cp_1">#</a></h6></div><div class="table-contents"><table summary="Control Plane 1" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>☐</th><th>Item</th><th>Value</th></tr></thead><tbody><tr><td> </td><td>Disk Requirement: 3x 512 GB disks (or enough space to create three
logical drives with that amount of space)
      </td><td> </td></tr><tr><td> </td><td>Ensure the disks are wiped</td><td> </td></tr><tr><td> </td><td>MAC address of first NIC</td><td> </td></tr><tr><td> </td><td>A second NIC, or a set of bonded NICs are required</td><td> </td></tr><tr><td> </td><td>IPMI IP address</td><td> </td></tr><tr><td> </td><td>IPMI Username/Password</td><td> </td></tr></tbody></table></div></div><div class="table" id="cp_2"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 2.2: </span><span class="name">Control Plane 2 </span><a title="Permalink" class="permalink" href="#cp_2">#</a></h6></div><div class="table-contents"><table summary="Control Plane 2" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>☐</th><th>Item</th><th>Value</th></tr></thead><tbody><tr><td> </td><td>
       Disk Requirement: 3x 512 GB disks (or enough space to create three
       logical drives with that amount of space)
      </td><td> </td></tr><tr><td> </td><td>Ensure the disks are wiped</td><td> </td></tr><tr><td> </td><td>MAC address of first NIC</td><td> </td></tr><tr><td> </td><td>A second NIC, or a set of bonded NICs are required</td><td> </td></tr><tr><td> </td><td>IPMI IP address</td><td> </td></tr><tr><td> </td><td>IPMI Username/Password</td><td> </td></tr></tbody></table></div></div><div class="table" id="cp_3"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 2.3: </span><span class="name">Control Plane 3 </span><a title="Permalink" class="permalink" href="#cp_3">#</a></h6></div><div class="table-contents"><table summary="Control Plane 3" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>☐</th><th>Item</th><th>Value</th></tr></thead><tbody><tr><td> </td><td>
       Disk Requirement: 3x 512 GB disks (or enough space to create three
       logical drives with that amount of space)
      </td><td> </td></tr><tr><td> </td><td>Ensure the disks are wiped</td><td> </td></tr><tr><td> </td><td>MAC address of first NIC</td><td> </td></tr><tr><td> </td><td>A second NIC, or a set of bonded NICs are required</td><td> </td></tr><tr><td> </td><td>IPMI IP address</td><td> </td></tr><tr><td> </td><td>IPMI Username/Password</td><td> </td></tr></tbody></table></div></div></div><div class="sect1" id="idm139651565282832"><div class="titlepage"><div><div><h2 class="title" id="idm139651565282832"><span class="number">2.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Compute Hosts</span> <a title="Permalink" class="permalink" href="#idm139651565282832">#</a></h2></div></div></div><p>
   One or more KVM Compute servers will be used as the compute host targets for
   instances.
  </p><div class="informaltable"><table border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>☐</th><th>Item</th></tr></thead><tbody><tr><td> </td><td>
       Disk Requirement: 2x 512 GB disks (or enough space to create three
       logical drives with that amount of space)
      </td></tr><tr><td> </td><td>
       A NIC for PXE boot and a second NIC, or a NIC for PXE and a set of
       bonded NICs are required
      </td></tr><tr><td> </td><td>Ensure the disks are wiped</td></tr></tbody></table></div><p>
   Table to record your Compute host details:
  </p><div class="informaltable"><table border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /></colgroup><thead><tr><th>ID</th><th>NIC MAC Address</th><th>IPMI Username/Password</th><th>IMPI IP Address</th><th>CPU/Mem/Disk</th></tr></thead><tbody><tr><td> </td><td> </td><td> </td><td> </td><td> </td></tr><tr><td> </td><td> </td><td> </td><td> </td><td> </td></tr><tr><td> </td><td> </td><td> </td><td> </td><td> </td></tr></tbody></table></div></div><div class="sect1" id="idm139651567795680"><div class="titlepage"><div><div><h2 class="title" id="idm139651567795680"><span class="number">2.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Storage Hosts</span> <a title="Permalink" class="permalink" href="#idm139651567795680">#</a></h2></div></div></div><p>
   Three or more servers with local disk volumes to provide Cinder
   block storage resources.
  </p><div id="idm139651567794416" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    The cluster created from block storage nodes must allow for quorum. In
    other words, the node count of the cluster must be 3, 5, 7, or another odd
    number.
   </p></div><div class="informaltable"><table border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>☐</th><th>Item</th></tr></thead><tbody><tr><td> </td><td>
       <p>
        Disk Requirement: 3x 512 GB disks (or enough space to create three
        logical drives with that amount of space)
       </p>
       <p>
        The block storage appliance deployed on a host is expected to consume
        ~40 GB of disk space from the host root disk for ephemeral storage to
        run the block storage virtual machine.
       </p>
      </td></tr><tr><td> </td><td>
       A NIC for PXE boot and a second NIC, or a NIC for PXE and a set of
       bonded NICs are required
      </td></tr><tr><td> </td><td>Ensure the disks are wiped</td></tr></tbody></table></div><p>
   Table to record your block storage host details:
  </p><div class="informaltable"><table border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /><col class="c6" /></colgroup><thead><tr><th>ID</th><th>NIC MAC Address</th><th>IPMI Username/Password</th><th>IPMI IP Address</th><th>CPU/Mem/Disk</th><th>Data Volume</th></tr></thead><tbody><tr><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td></tr><tr><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td></tr><tr><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td></tr></tbody></table></div></div><div class="sect1" id="idm139651565548736"><div class="titlepage"><div><div><h2 class="title" id="idm139651565548736"><span class="number">2.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Additional Comments</span> <a title="Permalink" class="permalink" href="#idm139651565548736">#</a></h2></div></div></div><p>
   This section is for any additional information that you deem necessary.
  </p><div class="verbatim-wrap"><pre class="screen">













</pre></div></div></div><div class="chapter " id="cha.depl.dep_inst"><div class="titlepage"><div><div><h2 class="title"><span class="number">3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing the Cloud Lifecycle Manager server</span> <a title="Permalink" class="permalink" href="#cha.depl.dep_inst">#</a></h2></div><div><div class="abstract"><div class="abstract-title-wrap"><h6 class="abstract-title">Abstract<a title="Permalink" class="permalink" href="#idm139651565543856">#</a></h6></div><p>
    In this chapter you will learn how to install the Cloud Lifecycle Manager from
    scratch. It will run on SUSE Linux Enterprise Server 12 SP3 and include the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
    extension and, optionally, the Subscription Management Tool (SMT) server.
   </p></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="#sec.depl.adm_inst.os"><span class="number">3.1 </span><span class="name">Starting the Operating System Installation</span></a></span></dt><dt><span class="sect1"><a href="#sec.depl.adm_inst.online_update"><span class="number">3.2 </span><span class="name">Registration and Online Updates</span></a></span></dt><dt><span class="sect1"><a href="#sec.depl.adm_inst.add_on"><span class="number">3.3 </span><span class="name">Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Extension</span></a></span></dt><dt><span class="sect1"><a href="#sec.depl.adm_inst.partitioning"><span class="number">3.4 </span><span class="name">Partitioning</span></a></span></dt><dt><span class="sect1"><a href="#sec.depl.adm_inst.user"><span class="number">3.5 </span><span class="name">Creating a User</span></a></span></dt><dt><span class="sect1"><a href="#sec.depl.adm_inst.settings"><span class="number">3.6 </span><span class="name">Installation Settings</span></a></span></dt></dl></div></div><div class="sect1 " id="sec.depl.adm_inst.os"><div class="titlepage"><div><div><h2 class="title" id="sec.depl.adm_inst.os"><span class="number">3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Starting the Operating System Installation</span> <a title="Permalink" class="permalink" href="#sec.depl.adm_inst.os">#</a></h2></div></div></div><p>
   Start the installation by booting into the SUSE Linux Enterprise Server 12 SP3 installation system.
   For an overview of a default SUSE Linux Enterprise Server installation, refer to the <a class="link" href="http://www.suse.com/documentation/sles-12/book_quickstarts/data/art_sle_installquick.html" target="_blank">SUSE Linux Enterprise Server
   <em class="citetitle ">Installation Quick Start</em></a>. <a class="link" href="http://www.suse.com/documentation/sles-12/book_sle_deployment/data/cha_inst.html" target="_blank">Detailed
   installation instructions</a> are available in the SUSE Linux Enterprise Server
   <em class="citetitle ">Deployment Guide</em>. Both documents are available at
   <a class="link" href="http://www.suse.com/documentation/sles-12/" target="_blank">http://www.suse.com/documentation/sles-12/</a>.
  </p><p>
   The following sections will only cover the differences from the default
   installation process.
  </p></div><div class="sect1 " id="sec.depl.adm_inst.online_update"><div class="titlepage"><div><div><h2 class="title" id="sec.depl.adm_inst.online_update"><span class="number">3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Registration and Online Updates</span> <a title="Permalink" class="permalink" href="#sec.depl.adm_inst.online_update">#</a></h2></div></div></div><p>
   Registering SUSE Linux Enterprise Server 12 SP3 during the installation process is required for
   getting product updates and for installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   extension. Refer to the <a class="link" href="http://www.suse.com/documentation/sles-12/book_sle_deployment/data/sec_i_yast2_conf_manual_cc.html" target="_blank">SUSE
   Customer Center Registration</a> section of the SUSE Linux Enterprise Server 12 SP3 <em class="citetitle ">Deployment
   Guide</em> for further instructions.
  </p><p>
   After a successful registration you will be asked whether
   to add the update repositories. If you agree, the latest updates will
   automatically be installed, ensuring that your system is on the latest
   patch level after the initial installation. We strongly recommend adding the
   update repositories immediately. If you choose to skip this step you need to
   perform an online update later, before starting the installation.
  </p><div id="idm139651567723632" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: SUSE Login Required</h6><p>
    To register a product, you need to have a SUSE login.
    If you do not have such a login, create it at
    <a class="link" href="http://www.suse.com/login" target="_blank">http://www.suse.com/login</a>.
   </p></div></div><div class="sect1 " id="sec.depl.adm_inst.add_on"><div class="titlepage"><div><div><h2 class="title" id="sec.depl.adm_inst.add_on"><span class="number">3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Extension</span> <a title="Permalink" class="permalink" href="#sec.depl.adm_inst.add_on">#</a></h2></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is an extension to SUSE Linux Enterprise Server. Installing it during the SUSE Linux Enterprise Server
   installation is the easiest and recommended way to set up the Cloud Lifecycle Manager. To get
   access to the extension selection dialog, you need to register SUSE Linux Enterprise Server 12 SP3
   during the installation. After a successful registration, the
   SUSE Linux Enterprise Server 12 SP3 installation continues with the <span class="guimenu">Extension &amp; Module
   Selection</span>. Choose <span class="guimenu"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span></span>
   and provide the registration key you obtained by purchasing
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. The registration and the extension installation require an
   Internet connection.
  </p><p>
   Alternatively, install the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> after the
   SUSE Linux Enterprise Server 12 SP3 installation via <span class="guimenu">YaST</span> › <span class="guimenu">Software</span> › <span class="guimenu">Add-On Products</span>.
   For details, refer to the section <a class="link" href="https://www.suse.com/documentation/sles-12/book_sle_deployment/data/sec_add-ons_extensions.html" target="_blank">Installing
   Modules and Extensions from Online Channels</a> of the SUSE Linux Enterprise Server 12 SP3
   <em class="citetitle ">Deployment Guide</em>.
  </p></div><div class="sect1 " id="sec.depl.adm_inst.partitioning"><div class="titlepage"><div><div><h2 class="title" id="sec.depl.adm_inst.partitioning"><span class="number">3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Partitioning</span> <a title="Permalink" class="permalink" href="#sec.depl.adm_inst.partitioning">#</a></h2></div></div></div><p>
   Create a custom partition setup using the <span class="guimenu">Expert
   Partitioner</span>. The following setup is required:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     An LVM setup with no encryption, containing:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       a volume group named <code class="literal">ardana-vg</code> on the first disk
       (<code class="systemitem">/dev/sda</code>)
      </p></li><li class="listitem "><p>
       a volume named <code class="literal">root</code> with a size of 50GB and an ext4 filesystem
      </p></li></ul></div></li><li class="listitem "><p>
     no separate mount point for <code class="filename">/home</code>
    </p></li><li class="listitem "><p>
     no swap partition or file
    </p></li></ul></div></div><div class="sect1 " id="sec.depl.adm_inst.user"><div class="titlepage"><div><div><h2 class="title" id="sec.depl.adm_inst.user"><span class="number">3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a User</span> <a title="Permalink" class="permalink" href="#sec.depl.adm_inst.user">#</a></h2></div></div></div><p>
   Setting up Cloud Lifecycle Manager requires a regular user which you can set up during the
   installation. You are free to choose any available user name except for
   <code class="systemitem">ardana</code>, since the <code class="systemitem">ardana</code> user is reserved by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
  </p></div><div class="sect1 " id="sec.depl.adm_inst.settings"><div class="titlepage"><div><div><h2 class="title" id="sec.depl.adm_inst.settings"><span class="number">3.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installation Settings</span> <a title="Permalink" class="permalink" href="#sec.depl.adm_inst.settings">#</a></h2></div></div></div><p>
   In the final installation step, <span class="guimenu">Installation Settings</span>, you need to adjust
   the software selection for your Cloud Lifecycle Manager setup. For more information
   refer to the <a class="link" href="http://www.suse.com/documentation/sles-12/book_sle_deployment/data/sec_i_yast2_proposal.html" target="_blank">Installation
   Settings</a> section of the SUSE Linux Enterprise Server 12 SP3 <em class="citetitle ">Deployment
   Guide</em>.
  </p><div id="idm139651570998416" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Disable Default Firewall</h6><p>
    The default firewall must be disabled, as <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> enables its own
    firewall during deployment. You can disable the default firewall in the <span class="guimenu">Installation
    Settings</span> screen of the installation wizard.
   </p></div><div class="sect2 " id="sec.depl.adm_inst.settings.software"><div class="titlepage"><div><div><h3 class="title" id="sec.depl.adm_inst.settings.software"><span class="number">3.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Software Selection</span> <a title="Permalink" class="permalink" href="#sec.depl.adm_inst.settings.software">#</a></h3></div></div></div><p>
    Installing a minimal base system is sufficient to set up the
    Administration Server. The following patterns are the minimum required:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      <span class="guimenu">Base System</span>
     </p></li><li class="listitem "><p>
      <span class="guimenu">Minimal System (Appliances)</span>
     </p></li><li class="listitem "><p>
      <span class="guimenu">Meta Package for Pattern cloud-ardana</span> (in case you have
      chosen to install the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Extension)
     </p></li><li class="listitem "><p>
      <span class="guimenu">Subscription Management Tool</span> (optional, also see <a class="xref" href="#tip.depl.adm_inst.settings.smt" title="Tip: Installing a Local SMT Server (Optional)">Tip: Installing a Local SMT Server (Optional)</a>)
     </p></li><li class="listitem "><p>
      <span class="guimenu">YaST2 configuration packages</span>
     </p></li></ul></div><div id="tip.depl.adm_inst.settings.smt" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip: Installing a Local SMT Server (Optional)</h6><p>
     If you do not have a SUSE Manager or SMT server in your
     organization, or are planning to manually update the repositories required for deployment of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> nodes, you need to set up an SMT server on
     the Administration Server. Choose the pattern <span class="guimenu">Subscription Management
     Tool</span> in addition to the patterns listed above to install the
     SMT server software.
    </p></div></div></div></div><div class="chapter " id="app.deploy.smt_lcm"><div class="titlepage"><div><div><h2 class="title"><span class="number">4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)</span> <a title="Permalink" class="permalink" href="#app.deploy.smt_lcm">#</a></h2></div><div><div class="abstract"><div class="abstract-title-wrap"><h6 class="abstract-title">Abstract<a title="Permalink" class="permalink" href="#idm139651565575200">#</a></h6></div><p>
    One way to provide the repositories needed to set up the nodes in
    <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> is to install a Subscription Management Tool (SMT) server on the Cloud Lifecycle Manager server, and then mirror all
    repositories from SUSE Customer Center via this server. Installing an SMT server
    on the Cloud Lifecycle Manager server is optional. If your organization already provides an
    SMT server or a SUSE Manager server that can be accessed from the
    Cloud Lifecycle Manager server, skip this step.
   </p></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="#app.deploy.smt.install"><span class="number">4.1 </span><span class="name">SMT Installation</span></a></span></dt><dt><span class="sect1"><a href="#app.deploy.smt.config"><span class="number">4.2 </span><span class="name">SMT Configuration</span></a></span></dt><dt><span class="sect1"><a href="#app.deploy.smt.repos"><span class="number">4.3 </span><span class="name">Setting up Repository Mirroring on the SMT Server</span></a></span></dt><dt><span class="sect1"><a href="#app.deploy.smt.info"><span class="number">4.4 </span><span class="name">For More Information</span></a></span></dt></dl></div></div><div id="idm139651565572192" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important: Use of SMT Server and Ports</h6><p>
   When installing an SMT server on the Cloud Lifecycle Manager server, use it exclusively
   for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. To use the SMT server for other
   products, run it outside of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. Make sure it can be accessed
   from the Cloud Lifecycle Manager for mirroring the repositories needed for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
  </p><p>
   When the SMT server is installed on the Cloud Lifecycle Manager server, Cloud Lifecycle Manager
   provides the mirrored repositories on port <code class="literal">79</code>.
  </p></div><div class="sect1 " id="app.deploy.smt.install"><div class="titlepage"><div><div><h2 class="title" id="app.deploy.smt.install"><span class="number">4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SMT Installation</span> <a title="Permalink" class="permalink" href="#app.deploy.smt.install">#</a></h2></div></div></div><p>
   If you have not installed the SMT server during the initial Cloud Lifecycle Manager server
   installation as suggested in <a class="xref" href="#sec.depl.adm_inst.settings.software" title="3.6.1. Software Selection">Section 3.6.1, “Software Selection”</a>, run the following command
   to install it:
  </p><div class="verbatim-wrap"><pre class="screen">sudo zypper in -t pattern smt</pre></div></div><div class="sect1 " id="app.deploy.smt.config"><div class="titlepage"><div><div><h2 class="title" id="app.deploy.smt.config"><span class="number">4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SMT Configuration</span> <a title="Permalink" class="permalink" href="#app.deploy.smt.config">#</a></h2></div></div></div><p>
   No matter whether the SMT server was installed during the initial
   installation or in the running system, it needs to be configured with the
   following steps.
  </p><div id="idm139651570124496" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note: Prerequisites</h6><p>
    To configure the SMT server, a SUSE account is required. If you do not
    have such an account, register at <a class="link" href="http://www.suse.com/login" target="_blank">http://www.suse.com/login</a>. All products and
    extensions for which you want to mirror updates with the SMT
    server should be registered at the SUSE Customer Center (<a class="link" href="http://scc.suse.com/" target="_blank">http://scc.suse.com/</a>).
   </p></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Configuring the SMT server requires you to have your mirroring
     credentials (user name and password) and your registration e-mail
     address at hand. To access them, proceed as follows:
    </p><ol type="a" class="substeps "><li class="step "><p>
       Open a Web browser and log in to the SUSE Customer Center at
       <a class="link" href="http://scc.suse.com/" target="_blank">http://scc.suse.com/</a>.
      </p></li><li class="step "><p>
       Click your name to see the e-mail address which you have registered.
      </p></li><li class="step "><p>
       Click <span class="guimenu">Organization</span> › <span class="guimenu">Organization Credentials</span> to obtain
       your mirroring credentials (user name and password).
      </p></li></ol></li><li class="step "><p>
     Start <span class="guimenu">YaST</span> › <span class="guimenu">Network
     Services</span> › <span class="guimenu">SMT Configuration
     Wizard</span>.
    </p></li><li class="step "><p>
     Activate <span class="guimenu">Enable Subscription Management Tool Service
     (SMT)</span>.
    </p></li><li class="step "><p>
     Enter the <span class="guimenu">Customer Center Configuration</span> data as
     follows:
    </p><table border="0" summary="Simple list" class="simplelist "><tr><td><span class="guimenu">Use Custom Server</span>:
     Do <span class="emphasis"><em>not</em></span> activate this option</td></tr><tr><td><span class="guimenu">User</span>: The user name you retrieved from the
     SUSE Customer Center</td></tr><tr><td><span class="guimenu">Password</span>: The password you retrieved from the
     SUSE Customer Center</td></tr></table><p>
     Check your input with <span class="guimenu">Test</span>. If the test does not
     return <code class="literal">success</code>, check the credentials you entered.
    </p></li><li class="step "><p>
     Enter the e-mail address you retrieved from the SUSE Customer Center at
     <span class="guimenu">SCC E-Mail Used for Registration</span>.
    </p></li><li class="step "><p>
     <span class="guimenu">Your SMT Server URL</span> shows the HTTP address of your
     server. Usually it should not be necessary to change it.
    </p></li><li class="step "><p>
     Select <span class="guimenu">Next</span> to proceed to step two of the <span class="guimenu">SMT Configuration Wizard</span>.
    </p></li><li class="step "><p>
     Enter a <span class="guimenu">Database Password for SMT User</span> and confirm
     it by entering it once again.
    </p></li><li class="step "><p>
     Enter one or more e-mail addresses to which SMT status reports are
     sent by selecting <span class="guimenu">Add</span>.
    </p></li><li class="step "><p>
     Select <span class="guimenu">Next</span> to save your SMT configuration. When
     setting up the database you will be prompted for the MariaDB root
     password. If you have not already created one then create it in this step. Note that this is
     the global MariaDB root password, not the database password for the SMT
     user you specified before.
    </p><p>
     The SMT server requires a server certificate at
     <code class="filename">/etc/pki/trust/anchors/YaST-CA.pem</code>. Choose
     <span class="guimenu">Run CA Management</span>, provide a password and choose
     <span class="guimenu">Next</span> to create such a certificate. If your
     organization already provides a CA certificate, <span class="guimenu">Skip</span>
     this step and import the certificate via <span class="guimenu">YaST</span> › <span class="guimenu">Security and Users</span> › <span class="guimenu">CA Management</span> after the SMT
     configuration is done. See
   <a class="link" href="http://www.suse.com/documentation/sles-12/book_security/data/cha_security_yast_ca.html" target="_blank">http://www.suse.com/documentation/sles-12/book_security/data/cha_security_yast_ca.html</a>
   for more information.
    </p><p>
     After you complete your configuration a synchronization check with the SUSE Customer Center will run, which may take several minutes.
    </p></li></ol></div></div></div><div class="sect1 " id="app.deploy.smt.repos"><div class="titlepage"><div><div><h2 class="title" id="app.deploy.smt.repos"><span class="number">4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting up Repository Mirroring on the SMT Server</span> <a title="Permalink" class="permalink" href="#app.deploy.smt.repos">#</a></h2></div></div></div><p>
   The final step in setting up the SMT server is configuring it to
   mirror the repositories needed for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. The SMT server
   mirrors the repositories from the SUSE Customer Center. Make
   sure to have the appropriate subscriptions registered in SUSE Customer Center with the
   same e-mail address you specified when configuring SMT.
  </p><div class="sect2 " id="app.deploy.smt.repos.mandatory"><div class="titlepage"><div><div><h3 class="title" id="app.deploy.smt.repos.mandatory"><span class="number">4.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding Mandatory Repositories</span> <a title="Permalink" class="permalink" href="#app.deploy.smt.repos.mandatory">#</a></h3></div></div></div><p>
    Mirroring the SUSE Linux Enterprise Server 12 SP3 and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>
    repositories is mandatory. Run the following commands as user
    <code class="systemitem">root</code> to add them to the list of mirrored repositories:
   </p><div class="verbatim-wrap"><pre class="screen">for REPO in SLES12-SP3-{Pool,Updates} <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-8</span></span>-{Pool,Updates}; do
  smt-repos $REPO sle-12-x86_64 -e
done</pre></div></div><div class="sect2 " id="app.deploy.smt.repos.mirror"><div class="titlepage"><div><div><h3 class="title" id="app.deploy.smt.repos.mirror"><span class="number">4.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Updating the Repositories</span> <a title="Permalink" class="permalink" href="#app.deploy.smt.repos.mirror">#</a></h3></div></div></div><p>
    New repositories added to SMT must be updated immediately by running the following command as user <code class="systemitem">root</code>:
   </p><div class="verbatim-wrap"><pre class="screen">smt-mirror -L /var/log/smt/smt-mirror.log</pre></div><p>
    This command will download several GB of patches. This process may last
    up to several hours. A log file is written to
    <code class="filename">/var/log/smt/smt-mirror.log</code>. After this first manual update the repositories are updated automatically via cron
    job. A list of all
    repositories and their location in the file system on the Cloud Lifecycle Manager server can be
    found at <a class="xref" href="#tab.smt.repos_local" title="SMT Repositories Hosted on the Administration Server">Table 5.2, “SMT Repositories Hosted on the Administration Server”</a>.
   </p></div></div><div class="sect1 " id="app.deploy.smt.info"><div class="titlepage"><div><div><h2 class="title" id="app.deploy.smt.info"><span class="number">4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">For More Information</span> <a title="Permalink" class="permalink" href="#app.deploy.smt.info">#</a></h2></div></div></div><p>
   For detailed information about SMT refer to the Subscription Management Tool manual at <a class="link" href="http://www.suse.com/documentation/sles-12/book_smt/data/book_smt.html" target="_blank">http://www.suse.com/documentation/sles-12/book_smt/data/book_smt.html</a>.
  </p></div></div><div class="chapter " id="cha.depl.repo_conf_lcm"><div class="titlepage"><div><div><h2 class="title"><span class="number">5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Software Repository Setup</span> <a title="Permalink" class="permalink" href="#cha.depl.repo_conf_lcm">#</a></h2></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="#sec.depl.adm_conf.repos.product"><span class="number">5.1 </span><span class="name">Copying the Product Media Repositories</span></a></span></dt><dt><span class="sect1"><a href="#sec.depl.adm_conf.repos.scc"><span class="number">5.2 </span><span class="name">Update and Pool Repositories</span></a></span></dt><dt><span class="sect1"><a href="#sec.deploy.repo_locations"><span class="number">5.3 </span><span class="name">Repository Locations</span></a></span></dt></dl></div></div><p>
  Software repositories containing products, extensions, and the respective
  updates for all software need to be available to all nodes in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  in order to complete the deployment. These can be managed manually, or they
  can be hosted on the Cloud Lifecycle Manager server. In this configuration step, these
  repositories are made available on the Cloud Lifecycle Manager server. There are two types of
  repositories:
 </p><p>
  <span class="bold"><strong>Product Media Repositories</strong></span>: Product media
  repositories are copies of the installation media. They need to be
  directly copied to the Cloud Lifecycle Manager server, <span class="quote">“<span class="quote">loop-mounted</span>”</span> from an iso
  image, or mounted from a remote server via NFS. Affected are SUSE Linux Enterprise Server 12 SP3 and
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>. These are static repositories; they do not
  change or receive updates. See <a class="xref" href="#sec.depl.adm_conf.repos.product" title="5.1. Copying the Product Media Repositories">Section 5.1, “Copying the Product Media Repositories”</a> for setup instructions.
 </p><p>
  <span class="bold"><strong>Update and Pool Repositories</strong></span>: Update and
  Pool repositories are provided by the SUSE Customer Center. They contain all updates and
  patches for the products and extensions. To make them available for
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> they need to be mirrored from the SUSE Customer Center. Since their content is
  regularly updated, they must be kept in synchronization with SUSE Customer Center. For
  these purposes, SUSE provides either the Subscription Management Tool (SMT) or the
  SUSE Manager. See <a class="xref" href="#sec.depl.adm_conf.repos.scc" title="5.2. Update and Pool Repositories">Section 5.2, “Update and Pool Repositories”</a> for setup instructions.
 </p><div class="sect1 " id="sec.depl.adm_conf.repos.product"><div class="titlepage"><div><div><h2 class="title" id="sec.depl.adm_conf.repos.product"><span class="number">5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Copying the Product Media Repositories</span> <a title="Permalink" class="permalink" href="#sec.depl.adm_conf.repos.product">#</a></h2></div></div></div><p>
   The files in the product repositories for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> do not
   change, therefore they do not need to be synchronized with a remote
   source. If you have installed the product media from a downloaded ISO image,
   the product repositories will automatically be made available to the client
   nodes and these steps can be skipped. These steps can also be skipped if you
   prefer to install from the Pool repositories provided by SUSE Customer Center. Otherwise,
   it is sufficient to either copy the data (from a remote host or the
   installation media), to mount the product repository from a remote server
   via <code class="literal">NFS</code>, or to loop mount a copy of the installation
   images.
   </p><p>
    If you choose to install from the product media rather than from the SUSE Customer Center
    repositories, the following product media needs to reside in the specified
    directories:
   </p><div class="table" id="idm139651565329888"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 5.1: </span><span class="name">Local Product Repositories for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> </span><a title="Permalink" class="permalink" href="#idm139651565329888">#</a></h6></div><div class="table-contents"><table summary="Local Product Repositories for SUSE OpenStack Cloud" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th>
        <p>
         Repository
        </p>
       </th><th>
        <p>
         Directory
        </p>
       </th></tr></thead><tbody><tr><td>
        <p>
         <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> DVD #1
        </p>
       </td><td>
        <p>
         <code class="filename">/srv/www/suse-12.3/x86_64/repos/Cloud</code>
        </p>
       </td></tr></tbody></table></div></div><p>
    The data can be copied by a variety of methods:
   </p><div class="variablelist "><dl class="variablelist"><dt id="idm139651565315568"><span class="term ">Copying from the Installation Media</span></dt><dd><p>
      We recommend using <code class="command">rsync</code> for copying. If the
      installation data is located on a removable device, make sure to mount
      it first (for example, after inserting the DVD1 in the Administration Server and
      waiting for the device to become ready):
     </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="idm139651565313392"><span class="name">
      <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> DVD#1
     </span><a title="Permalink" class="permalink" href="#idm139651565313392">#</a></h5></div><div class="verbatim-wrap"><pre class="screen">mkdir -p /srv/www/suse-12.3/x86_64/repos/Cloud
mount /dev/dvd /mnt
rsync -avP /mnt/ /srv/www/suse-12.3/x86_64/repos/Cloud/
umount /mnt</pre></div></dd><dt id="idm139651565309376"><span class="term ">Copying from a Remote Host</span></dt><dd><p>
       If the data is provided by a remote machine, log in to that machine and
       push the data to the Administration Server (which has the IP address <code class="systemitem">192.168.245.10</code> in the following
       example):
      </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="idm139651565307056"><span class="name">
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> DVD#1
      </span><a title="Permalink" class="permalink" href="#idm139651565307056">#</a></h5></div><div class="verbatim-wrap"><pre class="screen">mkdir -p /srv/www/suse-12.3/x86_64/repos/Cloud
rsync -avPz <em class="replaceable ">/data/SUSE-OPENSTACK-CLOUD//DVD1/</em> <em class="replaceable ">192.168.245.10</em>:/srv/www/suse-12.3/x86_64/repos/Cloud/</pre></div></dd><dt id="idm139651565350768"><span class="term ">Mounting from an NFS Server</span></dt><dd><p>
       If the installation data is provided via NFS by a remote machine, mount
       the respective shares as follows. To automatically mount these
       directories either create entries in <code class="filename">/etc/fstab</code> or
       set up the automounter.
      </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="idm139651565348624"><span class="name">
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> DVD#1
      </span><a title="Permalink" class="permalink" href="#idm139651565348624">#</a></h5></div><div class="verbatim-wrap"><pre class="screen">mkdir -p /srv/www/suse-12.3/x86_64/repos/Cloud/
mount -t nfs <em class="replaceable ">nfs.example.com:/exports/SUSE-OPENSTACK-CLOUD/DVD1/</em> /srv/www/suse-12.3/x86_64/repos/Cloud</pre></div></dd></dl></div></div><div class="sect1 " id="sec.depl.adm_conf.repos.scc"><div class="titlepage"><div><div><h2 class="title" id="sec.depl.adm_conf.repos.scc"><span class="number">5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Update and Pool Repositories</span> <a title="Permalink" class="permalink" href="#sec.depl.adm_conf.repos.scc">#</a></h2></div></div></div><p>
    Update and Pool Repositories are required on the Cloud Lifecycle Manager server to set up and
    maintain the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> nodes. They are provided by SUSE Customer Center and contain all
    software packages needed to install SUSE Linux Enterprise Server 12 SP3 and the extensions (pool
    repositories). In addition, they contain all updates and patches (update
    repositories).
   </p><p>
    The repositories can be made available on the Cloud Lifecycle Manager server using one or more of the
    following methods:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      <a class="xref" href="#sec.depl.adm_conf.repos.scc.local_smt" title="5.2.1.  Repositories Hosted on an SMT Server Installed on the Administration Server">Section 5.2.1, “
     Repositories Hosted on an SMT Server Installed on the Administration Server
    ”</a>
     </p></li><li class="listitem "><p>
      <a class="xref" href="#sec.depl.adm_conf.repos.scc.alternatives" title="5.2.2. Alternative Ways to Make the Repositories Available">Section 5.2.2, “Alternative Ways to Make the Repositories Available”</a>
     </p></li></ul></div><div class="sect2 " id="sec.depl.adm_conf.repos.scc.local_smt"><div class="titlepage"><div><div><h3 class="title" id="sec.depl.adm_conf.repos.scc.local_smt"><span class="number">5.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">
     Repositories Hosted on an SMT Server Installed on the Administration Server
    </span> <a title="Permalink" class="permalink" href="#sec.depl.adm_conf.repos.scc.local_smt">#</a></h3></div></div></div><p>
     When all update and pool repositories are managed by an SMT server
     installed on the Cloud Lifecycle Manager server (see <a class="xref" href="#app.deploy.smt_lcm" title="Chapter 4. Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)">Chapter 4, <em>Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)</em></a>),
     the Cloud Lifecycle Manager automatically detects all available repositories. No further
     action is required.
    </p></div><div class="sect2 " id="sec.depl.adm_conf.repos.scc.alternatives"><div class="titlepage"><div><div><h3 class="title" id="sec.depl.adm_conf.repos.scc.alternatives"><span class="number">5.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Alternative Ways to Make the Repositories Available</span> <a title="Permalink" class="permalink" href="#sec.depl.adm_conf.repos.scc.alternatives">#</a></h3></div></div></div><p>
     If you want to keep your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> network as isolated from the company
     network as possible, or your infrastructure does not allow accessing a
     SUSE Manager or an SMT server, you can alternatively provide access to the
     required repositories by one of the following methods:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Mount the repositories from a remote server.
      </p></li><li class="listitem "><p>
       Synchronize the repositories from a remote server (for example via
       <code class="command">rsync</code> and cron).
      </p></li><li class="listitem "><p>
        Manually synchronize the update repositories from removable media.
      </p></li></ul></div><p>
     The repositories must be made available at the
     default locations on the Cloud Lifecycle Manager server as listed in <a class="xref" href="#tab.depl.adm_conf.local-repos" title="Repository Locations on the Cloud Lifecycle Manager server">Table 5.4, “Repository Locations on the Cloud Lifecycle Manager server”</a>.
    </p></div></div><div class="sect1 " id="sec.deploy.repo_locations"><div class="titlepage"><div><div><h2 class="title" id="sec.deploy.repo_locations"><span class="number">5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Repository Locations</span> <a title="Permalink" class="permalink" href="#sec.deploy.repo_locations">#</a></h2></div></div></div><p>
The following tables show the locations of all repositories that can be used for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
 </p><div class="table" id="tab.smt.repos_local"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 5.2: </span><span class="name">SMT Repositories Hosted on the Administration Server </span><a title="Permalink" class="permalink" href="#tab.smt.repos_local">#</a></h6></div><div class="table-contents"><table summary="SMT Repositories Hosted on the Administration Server" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th>
      <p>
       Repository
      </p>
     </th><th>
      <p>
       Directory
      </p>
     </th></tr></thead><tbody><tr><td colspan="2">
      <p>
       Mandatory Repositories
      </p>
     </td></tr><tr><td>
      <p>
       SLES12-SP3-Pool
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/www/htdocs/repo/SUSE/Products/SLE-SERVER/12-SP3/x86_64/product/</code>
      </p>
     </td></tr><tr><td>
      <p>
       SLES12-SP3-Updates
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/www/htdocs/repo/SUSE/Updates/SLE-SERVER/12-SP3/x86_64/update/</code>
      </p>
     </td></tr><tr><td>
      <p>
       <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-8</span></span>-Pool
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/www/htdocs/repo/SUSE/Products/OpenStack-Cloud/8/x86_64/product/</code>
      </p>
     </td></tr><tr><td>
      <p>
       <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-8</span></span>-Updates
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/www/htdocs/repo/SUSE/Updates/OpenStack-Cloud/8/x86_64/update/</code>
      </p>
     </td></tr></tbody></table></div></div><div class="table" id="tab.depl.adm_conf.susemgr-repos"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 5.3: </span><span class="name">SUSE Manager Repositories (Channels) </span><a title="Permalink" class="permalink" href="#tab.depl.adm_conf.susemgr-repos">#</a></h6></div><div class="table-contents"><table summary="SUSE Manager Repositories (Channels)" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th>
      <p>
       Repository
      </p>
     </th><th>
      <p>
       URL
      </p>
     </th></tr></thead><tbody><tr><td colspan="2">
      <p>
       Mandatory Repositories
      </p>
     </td></tr><tr><td>
      <p>
       SLES12-SP3-Updates
      </p>
     </td><td>
      <p>
       <code class="uri">http://manager.example.com/ks/dist/child/sles12-sp3-updates-x86_64/sles12-sp3-x86_64/</code>
      </p>
     </td></tr><tr><td>
      <p>
       <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-8</span></span>-Pool
      </p>
     </td><td>
      <p>
       <code class="uri">http://manager.example.com/ks/dist/child/suse-openstack-cloud-8-pool-x86_64/sles12-sp3-x86_64/</code>
      </p>
     </td></tr><tr><td>
      <p>
       <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-8</span></span>-Updates
      </p>
     </td><td>
      <p>
       <code class="uri">http://manager.example.com/ks/dist/child/suse-openstack-cloud-8-updates-x86_64/sles12-sp3-x86_64/</code>
      </p>
     </td></tr></tbody></table></div></div><p>
  The following table shows the required repository locations  to use when manually copying, synchronizing, or mounting the
  repositories.
 </p><div class="table" id="tab.depl.adm_conf.local-repos"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 5.4: </span><span class="name">Repository Locations on the Cloud Lifecycle Manager server </span><a title="Permalink" class="permalink" href="#tab.depl.adm_conf.local-repos">#</a></h6></div><div class="table-contents"><table summary="Repository Locations on the Cloud Lifecycle Manager server" border="1"><colgroup><col class="1" /><col class="2" /></colgroup><thead><tr><th>
      <p>
       Channel
      </p>
     </th><th>
      <p>
       Directory on the Administration Server
      </p>
     </th></tr></thead><tbody><tr><td colspan="2">
      <p>
       Mandatory Repositories
      </p>
     </td></tr><tr><td>
      <p>
       SLES12-SP3-Pool
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/www/suse-12.3/x86_64/repos/SLES12-SP3-Pool/</code>
      </p>
     </td></tr><tr><td>
      <p>
       SLES12-SP3-Updates
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/www/suse-12.3/x86_64/repos/SLES12-SP3-Updates/</code>
      </p>
     </td></tr><tr><td>
      <p>
       <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-8</span></span>-Pool
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/www/suse-12.3/x86_64/repos/<span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-8</span></span>-Pool/</code>
      </p>
     </td></tr><tr><td>
      <p>
       <span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-8</span></span>-Updates
      </p>
     </td><td>
      <p>
       <code class="filename">/srv/www/suse-12.3/x86_64/repos/<span class="phrase"><span class="phrase">SUSE-OpenStack-Cloud-8</span></span>-Updates</code>
      </p>
     </td></tr></tbody></table></div></div></div></div><div class="chapter " id="multipath_boot_from_san"><div class="titlepage"><div><div><h2 class="title"><span class="number">6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Boot from SAN and Multipath Configuration</span> <a title="Permalink" class="permalink" href="#multipath_boot_from_san">#</a></h2></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#multipath_overview"><span class="number">6.1 </span><span class="name">Introduction</span></a></span></dt><dt><span class="section"><a href="#idm139651566103968"><span class="number">6.2 </span><span class="name">Install Phase Configuration</span></a></span></dt><dt><span class="section"><a href="#restriction2"><span class="number">6.3 </span><span class="name">QLogic FCoE restrictions and additional configurations</span></a></span></dt><dt><span class="section"><a href="#install_boot_from_san"><span class="number">6.4 </span><span class="name">Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> ISO for nodes that support Boot from SAN</span></a></span></dt></dl></div></div><div class="sect1" id="multipath_overview"><div class="titlepage"><div><div><h2 class="title" id="multipath_overview"><span class="number">6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Introduction</span> <a title="Permalink" class="permalink" href="#multipath_overview">#</a></h2></div></div></div><p>
   For information about supported hardware for multipathing, see
   <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 2 “Hardware and Software Support Matrix”, Section 2.2 “Supported Hardware Configurations”</span>.
  </p><div id="boot-from-san-LUN0" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    When exporting a LUN to a node for boot from SAN, you should ensure that
    <span class="emphasis"><em>LUN 0</em></span> is assigned to the LUN and configure any setup
    dialog that is necessary in the firmware to consume this LUN 0 for OS
    boot.
   </p></div><div id="boot-from-san-host-persona" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    Any hosts that are connected to 3PAR storage must have a <code class="literal">host
    persona</code> of <code class="literal">2-generic-alua</code> set on the 3PAR.
    Refer to the 3PAR documentation for the steps necessary to check this and
    change if necessary.
   </p></div><p>
   iSCSI boot from SAN is not supported. For more information on the use of
   Cinder with multipath, see <a class="xref" href="#sec.3par-multipath" title="18.1.3. Multipath Support">Section 18.1.3, “Multipath Support”</a>.
  </p><p>
   To allow <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> to use volumes from a SAN, you have to specify
   configuration options for both the installation and the OS configuration
   phase. In all cases, the devices that are utilized are
   devices for which multipath is configured.
  </p></div><div class="sect1" id="idm139651566103968"><div class="titlepage"><div><div><h2 class="title" id="idm139651566103968"><span class="number">6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Install Phase Configuration</span> <a title="Permalink" class="permalink" href="#idm139651566103968">#</a></h2></div></div></div><p>
   For FC connected nodes and for FCoE nodes where the network processor used
   is from the Emulex family such as for the 650FLB, the following changes need
   to be made.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     In each stanza of the <code class="filename">servers.yml</code> insert a line
     stating <code class="literal">boot-from-san: true</code>
    </p><div class="verbatim-wrap"><pre class="screen">    - id: controller2
      ip-addr: 192.168.10.4
      role: CONTROLLER-ROLE
      server-group: RACK2
      nic-mapping: HP-DL360-4PORT
      mac-addr: "8a:8e:64:55:43:76"
      ilo-ip: 192.168.9.4
      ilo-password: password
      ilo-user: admin
      <span class="bold"><strong>boot-from-san: true</strong></span></pre></div><p>
     This uses the disk <code class="filename">/dev/mapper/mpatha</code> as the default
     device on which to install the OS.
    </p></li><li class="step "><p>
     In the disk input models, specify the devices that will be
     used via their multipath names (which will be of the form
     <code class="filename">/dev/mapper/mpatha</code>,
     <code class="filename">/dev/mapper/mpathb</code> etc.).
    </p><div class="verbatim-wrap"><pre class="screen">    volume-groups:
      - name: ardana-vg
        physical-volumes:

          # NOTE: 'sda_root' is a templated value. This value is checked in
          # os-config and replaced by the partition actually used on sda
          #for example sda1 or sda5
          - /dev/mapper/mpatha_root

...
      - name: vg-comp
        physical-volumes:
          - /dev/mapper/mpathb</pre></div></li></ol></div></div></div><div class="sect1" id="restriction2"><div class="titlepage"><div><div><h2 class="title" id="restriction2"><span class="number">6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">QLogic FCoE restrictions and additional configurations</span> <a title="Permalink" class="permalink" href="#restriction2">#</a></h2></div></div></div><p>
   If you are using network cards such as Qlogic Flex Fabric 536 and 630 series,
   there are additional OS configuration steps to support the importation of
   LUNs as well as some restrictions on supported configurations.
  </p><p>
   The restrictions are:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Only one network card can be enabled in the system.
    </p></li><li class="listitem "><p>
     The FCoE interfaces on this card are dedicated to FCoE traffic. They
     cannot have IP addresses associated with them.
    </p></li><li class="listitem "><p>
     NIC mapping cannot be used.
    </p></li></ul></div><p>
   In addition to the configuration options above, you also need to specify the
   FCoE interfaces for install and for os configuration. There are 3 places
   where you need to add additional configuration options for fcoe-support:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     In <code class="literal">servers.yml</code>, which is used for configuration of the
     system during OS install, FCoE interfaces need to be specified for each
     server. In particular, the mac addresses of the FCoE interfaces need to be
     given, <span class="emphasis"><em>not</em></span> the symbolic name (for example,
     <code class="literal">eth2</code>).
    </p><div class="verbatim-wrap"><pre class="screen">    - id: compute1
      ip-addr: 10.245.224.201
      role: COMPUTE-ROLE
      server-group: RACK2
      mac-addr: 6c:c2:17:33:4c:a0
      ilo-ip: 10.1.66.26
      ilo-user: linuxbox
      ilo-password: linuxbox123
      boot-from-san: True
      fcoe-interfaces:
         - <span class="bold"><strong>6c:c2:17:33:4c:a1</strong></span>
         - <span class="bold"><strong>6c:c2:17:33:4c:a9</strong></span></pre></div><div id="idm139651566084112" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      NIC mapping cannot be used.
     </p></div></li><li class="listitem "><p>
     For the osconfig phase, you will need to specify the
     <code class="literal">fcoe-interfaces</code> as a peer of
     <code class="literal">network-interfaces</code> in the
     <code class="literal">net_interfaces.yml</code> file:
    </p><div class="verbatim-wrap"><pre class="screen">    - name: CONTROLLER-INTERFACES
      fcoe-interfaces:
        - name: fcoe
          devices:
             - <span class="bold"><strong>eth2</strong></span>
             - <span class="bold"><strong>eth3</strong></span>
      network-interfaces:
        - name: eth0
          device:
              name: eth0
          network-groups:
            - EXTERNAL-API
            - EXTERNAL-VM
            - GUEST
            - MANAGEMENT</pre></div><div id="idm139651566078720" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      The MAC addresses specified in the <code class="literal">fcoe-interfaces</code>
      stanza in <code class="filename">servers.yml</code> must correspond to the symbolic
      names used in the <code class="literal">fcoe-interfaces</code> stanza in
      <code class="filename">net_interfaces.yml</code>.
     </p><p>
      Also, to satisfy the FCoE restriction outlined in
      <a class="xref" href="#restriction2" title="6.3. QLogic FCoE restrictions and additional configurations">Section 6.3, “QLogic FCoE restrictions and additional configurations”</a> above, there
      can be no overlap between the devices in
      <code class="literal">fcoe-interfaces</code> and those in
      <code class="literal">network-interfaces</code> in the
      <code class="filename">net_interfaces.yml</code> file. In the example,
      <code class="literal">eth2</code> and <code class="literal">eth3</code> are
      <code class="literal">fcoe-interfaces</code> while <code class="literal">eth0</code> is in
      <code class="literal">network-interfaces</code>.
     </p></div></li><li class="listitem "><p>
     As part of the initial install from an iso, additional parameters need to
     be supplied on the kernel command line:
    </p><div class="verbatim-wrap"><pre class="screen">multipath=true partman-fcoe/interfaces=&lt;mac address1&gt;,&lt;mac address2&gt; disk-detect/fcoe/enable=true --- quiet</pre></div><p>
     See the sections of installing from an ISO using
     UEFI (<a class="xref" href="#sec.san-uefi_fcoe" title="6.4.2. UEFI Boot Mode with QLogic FCoE">Section 6.4.2, “UEFI Boot Mode with QLogic FCoE”</a>) and
     BIOS (<a class="xref" href="#sec.san-bios_fcoe" title="6.4.4. Legacy BIOS Boot Mode with QLogic FCoE">Section 6.4.4, “Legacy BIOS Boot Mode with QLogic FCoE”</a>).
    </p></li></ul></div><p>
   Since NIC mapping is not used to guarantee order of the networks across the
   system the installer will remap the network interfaces in a deterministic
   fashion as part of the install. As part of the installer dialogue, if DHCP
   is not configured for the interface, it is necessary to confirm that the
   appropriate interface is assigned the ip address. The network interfaces may
   not be at the names expected when installing via an ISO. When you are asked
   to apply an IP address to an interface, press
   <span class="keycap">Alt</span><span class="key-connector">–</span><span class="keycap">F2</span> and
   in the console window, run the command <code class="command">ip a</code> to examine
   the interfaces and their associated MAC addresses. Make a note of the
   interface name with the expected MAC address and use this in the subsequent
   dialog. Press
   <span class="keycap">Alt</span><span class="key-connector">–</span><span class="keycap">F1</span>
   to return to the installation
   screen. You should note that the names of the interfaces may have changed
   after the installation completes. These names are used consistently in any
   subsequent operations.
  </p><p>
   Therefore, even if FCoE is not used for boot from SAN (for example for
   cinder), then it is recommended that <code class="literal">fcoe-interfaces</code> be
   specified as part of install (without the multipath or disk detect options).
   Alternatively, you need to run <code class="filename">osconfig-fcoe-reorder.yml</code>
   before <code class="filename">site.yml</code> or
   <code class="filename">osconfig-run.yml</code> is
   invoked to reorder the networks in a similar manner to the installer. In
   this case, the nodes will need to be manually rebooted for the network
   reorder to take effect. Run
   <code class="filename">osconfig-fcoe-reorder.yml</code> in the following scenarios:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     If you have used a third-party installer to provision your bare-metal nodes
    </p></li><li class="listitem "><p>
     If you are booting from a local disk (that is one that is not presented
     from the SAN) but you want to use FCoE later, for example, for cinder.
    </p></li></ul></div><p>
   To run the command:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts osconfig-fcoe-reorder.yml</pre></div><p>
   If you do not run <code class="filename">osconfig-fcoe-reorder.yml</code>, you will
   encounter a failure in <code class="filename">osconfig-run.yml</code>.
  </p><p>
   If you are booting from a local disk, the LUNs that will be imported over
   FCoE will not be visible before <code class="filename">site.yml</code> or
   <code class="filename">osconfig-run.yml</code> has been run. However, if you need to
   import the LUNs before this, for instance, in scenarios where you need to
   run <code class="filename">wipe_disks.yml</code>, then you can run the
   <code class="literal">fcoe-enable</code> playbook across the nodes in question. This
   will configure FCoE and import the LUNs presented to the nodes.
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/verb_hosts fcoe-enable.yml</pre></div></div><div class="sect1" id="install_boot_from_san"><div class="titlepage"><div><div><h2 class="title" id="install_boot_from_san"><span class="number">6.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> ISO for nodes that support Boot from SAN</span> <a title="Permalink" class="permalink" href="#install_boot_from_san">#</a></h2></div></div></div><p>
   This section describes how to install the ISO on the Cloud Lifecycle Manager to support the
   following configurations:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <a class="xref" href="#sec.san-uefi" title="6.4.1. UEFI Boot Mode">Section 6.4.1, “UEFI Boot Mode”</a>
    </p></li><li class="listitem "><p>
     <a class="xref" href="#sec.san-uefi_fcoe" title="6.4.2. UEFI Boot Mode with QLogic FCoE">Section 6.4.2, “UEFI Boot Mode with QLogic FCoE”</a>
    </p></li><li class="listitem "><p>
     <a class="xref" href="#sec.san-bios" title="6.4.3. Legacy BIOS Boot Mode">Section 6.4.3, “Legacy BIOS Boot Mode”</a>
    </p></li><li class="listitem "><p>
     <a class="xref" href="#sec.san-bios_fcoe" title="6.4.4. Legacy BIOS Boot Mode with QLogic FCoE">Section 6.4.4, “Legacy BIOS Boot Mode with QLogic FCoE”</a>
    </p></li></ul></div><p>
   The Cloud Lifecycle Manager will then automatically handle the installation on nodes that
   support Boot from SAN based on the configuration information in
   <code class="literal">servers.yml</code> and the disk models, as described in the
   preceding section.
  </p><div class="sect2" id="sec.san-uefi"><div class="titlepage"><div><div><h3 class="title" id="sec.san-uefi"><span class="number">6.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">UEFI Boot Mode</span> <a title="Permalink" class="permalink" href="#sec.san-uefi">#</a></h3></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      On the installer boot menu, press <span class="keycap">E</span> to enter the
      <code class="literal">Edit Selection</code> screen.
     </p></li><li class="step "><p>
      On the line that starts with <code class="literal">linux</code>, enter the text
      <code class="literal">multipath=true</code> before <code class="literal">--- quiet</code>.
     </p></li><li class="step "><p>
      Press <span class="keycap">Ctrl</span><span class="key-connector">–</span><span class="keycap">X</span>
      or <span class="keycap">F10</span> to proceed with the installation.
     </p></li></ol></div></div></div><div class="sect2" id="sec.san-uefi_fcoe"><div class="titlepage"><div><div><h3 class="title" id="sec.san-uefi_fcoe"><span class="number">6.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">UEFI Boot Mode with QLogic FCoE</span> <a title="Permalink" class="permalink" href="#sec.san-uefi_fcoe">#</a></h3></div></div></div><p>
    In addition to inserting <code class="literal">multipath=true</code>, it is necessary
    to supply details of the FCoE network interfaces.
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      At the installer boot menu, press <span class="keycap">E</span> to enter the
      <code class="literal">Edit Selection</code> screen.
     </p></li><li class="step "><p>
      On the line that starts with <code class="literal">linux</code>, enter the text
      <code class="literal">partman-fcoe/interfaces=6c:c2:17:33:4c:a1,6c:c2:17:33:4c:a9
      disk-detect/fcoe/enable=true</code> between
      <code class="literal">multipath=true</code> and <code class="literal"> --- quiet</code>.
     </p></li><li class="step "><p>
      Press <span class="keycap">Ctrl</span><span class="key-connector">–</span><span class="keycap">X</span>
      or <span class="keycap">F10</span> to proceed with the installation.
     </p></li></ol></div></div></div><div class="sect2" id="sec.san-bios"><div class="titlepage"><div><div><h3 class="title" id="sec.san-bios"><span class="number">6.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Legacy BIOS Boot Mode</span> <a title="Permalink" class="permalink" href="#sec.san-bios">#</a></h3></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      On the installer boot menu, navigate (using <span class="keycap">↓</span>) to
      the <code class="literal">Advanced options</code> entry and then press
      <span class="keycap">Enter</span>.
     </p></li><li class="step "><p>
      Navigate to the <code class="literal">Multipath install</code> entry and press
      <span class="keycap">Enter</span> to start the installation.
     </p></li></ol></div></div></div><div class="sect2" id="sec.san-bios_fcoe"><div class="titlepage"><div><div><h3 class="title" id="sec.san-bios_fcoe"><span class="number">6.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Legacy BIOS Boot Mode with QLogic FCoE</span> <a title="Permalink" class="permalink" href="#sec.san-bios_fcoe">#</a></h3></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      On the installer boot menu, navigate (using <span class="keycap">↓</span>) to
      the <code class="literal">Advanced options</code> entry and then press
      <span class="keycap">Enter</span>.
     </p></li><li class="step "><p>
      Navigate to the <code class="literal">Multipath install</code> entry and press
      <span class="keycap">→|</span> to edit the entry details. The kernel command
      line is now displayed at the bottom of the screen and
      <code class="literal">multipath=true</code> is already specified.
     </p></li><li class="step "><p>
      Edit the kernel command line to add the FCoE network interfaces before
      <code class="literal"> --- quiet</code>. Add
      <code class="literal">partman-fcoe/interfaces=6c:c2:17:33:4c:a1,6c:c2:17:33:4c:a9
      disk-detect/fcoe/enable=true</code> before
      <code class="literal"> --- quiet</code>.
     </p></li><li class="step "><p>
      Now press <span class="keycap">Enter</span> to start the installation.
     </p></li></ol></div></div></div></div></div><div class="chapter " id="cha.install.l2gw5930"><div class="titlepage"><div><div><h2 class="title"><span class="number">7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing the L2 Gateway Agent for the Networking Service</span> <a title="Permalink" class="permalink" href="#cha.install.l2gw5930">#</a></h2></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#idm139651570580944"><span class="number">7.1 </span><span class="name">Sample network topology (for illustration purposes)</span></a></span></dt><dt><span class="section"><a href="#idm139651570573344"><span class="number">7.2 </span><span class="name">Networks</span></a></span></dt><dt><span class="section"><a href="#idm139651570558528"><span class="number">7.3 </span><span class="name">HPE 5930 switch configuration</span></a></span></dt><dt><span class="section"><a href="#idm139651570538976"><span class="number">7.4 </span><span class="name">Configuring the Provider Data Path Network</span></a></span></dt><dt><span class="section"><a href="#idm139651570511984"><span class="number">7.5 </span><span class="name">Enabling and Configuring the L2 Gateway Agent</span></a></span></dt><dt><span class="section"><a href="#idm139651565876752"><span class="number">7.6 </span><span class="name">Routing Between Software and Hardware - VTEP Networks</span></a></span></dt><dt><span class="section"><a href="#idm139651565860512"><span class="number">7.7 </span><span class="name">Connecting a Bare-Metal Server to the HPE 5930 Switch</span></a></span></dt><dt><span class="section"><a href="#idm139651565855696"><span class="number">7.8 </span><span class="name">Configuration on a Bare-Metal Server</span></a></span></dt><dt><span class="section"><a href="#idm139651565847088"><span class="number">7.9 </span><span class="name">NIC Bonding and IRF Configuration</span></a></span></dt><dt><span class="section"><a href="#idm139651565843536"><span class="number">7.10 </span><span class="name">Scale Numbers Tested</span></a></span></dt><dt><span class="section"><a href="#idm139651565838048"><span class="number">7.11 </span><span class="name">L2 Gateway Commands</span></a></span></dt></dl></div></div><p>
   The L2 gateway is a service plug-in to the Neutron networking service that
   allows two L2 networks to be seamlessly connected to create a single L2
   broadcast domain. The initial implementation provides for the ability to
   connect a virtual Neutron VxLAN network to a physical VLAN using a
   VTEP-capable HPE 5930 switch. The L2 gateway is to be enabled only for VxLAN
   deployments.
  </p><p>
   To begin L2 gateway agent setup, you need to configure your switch. These
   instructions use an
   <a class="link" href="http://www8.hp.com/us/en/products/networking-switches/product-detail.html?oid=6604154#!tab=models" target="_blank">HPE
   FlexFabric 5930 Switch Series</a> switch.
  </p><div class="sect1" id="idm139651570580944"><div class="titlepage"><div><div><h2 class="title" id="idm139651570580944"><span class="number">7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Sample network topology (for illustration purposes)</span> <a title="Permalink" class="permalink" href="#idm139651570580944">#</a></h2></div></div></div><p>
   When viewing the following network diagram, assume that the blue VNET has
   been created by the tenant and has been assigned a segmentation ID of 1000
   (VNI 1000). The Cloud Admin is now connecting physical servers to this VNET.
  </p><p>
   Assume also that the blue L2 Gateway is created and points to the HW VTEPS,
   the physical ports, the VLAN, and if it is an access or trunk port (tagged
   or untagged)
  </p><div id="idm139651570578784" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    This example does not apply to distributed route networks, which use
    Distributed Virtual Routers (DVR).
   </p></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-networking-l2gateway5930-new.png"><img src="images/media-networking-l2gateway5930-new.png" width="" /></a></div></div></div><div class="sect1" id="idm139651570573344"><div class="titlepage"><div><div><h2 class="title" id="idm139651570573344"><span class="number">7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networks</span> <a title="Permalink" class="permalink" href="#idm139651570573344">#</a></h2></div></div></div><p>
   The following diagram illustrates an example network configuration. It does
   not apply to distributed route networks, which use Distributed Virtual
   Routers (DVR).
   </p><div class="figure" id="idm139651570571888"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/media-networking-l2gateway5930-2-new.png"><img src="images/media-networking-l2gateway5930-2-new.png" width="" alt="L2 Gateway and 5930 Switch" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 7.1: </span><span class="name">L2 Gateway and 5930 Switch </span><a title="Permalink" class="permalink" href="#idm139651570571888">#</a></h6></div></div><p>
   An L2 gateway is useful in extending virtual networks (VxLAN) in a cloud
   onto physical VLAN networks. The L2 gateway switch converts VxLAN packets
   into VLAN packets and back again, as shown in the following diagram. This
   topic assumes a VxLAN deployment.
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-networking-l2gateway5930-3-new.png"><img src="images/media-networking-l2gateway5930-3-new.png" width="" /></a></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Management Network: 10.10.85.0/24
    </p></li><li class="listitem "><p>
     Data Network: 10.1.1.0/24
    </p></li><li class="listitem "><p>
     Tenant VM Network: 10.10.10.0/24
    </p></li></ul></div><div id="idm139651570559920" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    These IP ranges are used in the topology shown in the diagram for
    illustration only.
   </p></div></div><div class="sect1" id="idm139651570558528"><div class="titlepage"><div><div><h2 class="title" id="idm139651570558528"><span class="number">7.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HPE 5930 switch configuration</span> <a title="Permalink" class="permalink" href="#idm139651570558528">#</a></h2></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Telnet to the 5930 switch and provide your username and password.
    </p></li><li class="step "><p>
     Go into system view:
    </p><div class="verbatim-wrap"><pre class="screen">system-view</pre></div></li><li class="step "><p>
     Create the required VLANs and VLAN ranges:
    </p><div class="verbatim-wrap"><pre class="screen">vlan 103
vlan 83
vlan 183
vlan 1261 to 1270</pre></div></li><li class="step "><p>
     Assign an IP address to VLAN 103. This is used as a data path network for
     VxLAN traffic.
    </p><div class="verbatim-wrap"><pre class="screen">interface vlan 103
ip address 10.1.1.10 255.255.255.0</pre></div></li><li class="step "><p>
     Assign an IP address to VLAN 83. This is used as a hardware VTEP network.
    </p><div class="verbatim-wrap"><pre class="screen">interface vlan 83
ip address 10.10.83.3 255.255.255.0</pre></div><p>
     The 5930 switch has a fortygigE1/0/5 interface to which a splitter cable
     is connected that splits the network into four tengigEthernet
     (tengigEthernet1/0/5:1 to tengigEthernet1/0/5:4) interfaces:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       tengigEthernet1/0/5:1 and tengigEthernet1/0/5:2 are connected to the
       compute node. This is required just to bring the interface up. In other
       words, in order to have the HPE 5930 switch work as a router, there
       should be at least one interface of that particular VLAN up.
       Alternatively, the interface can be connected to any host or network
       element.
      </p></li><li class="listitem "><p>
       tengigEthernet1/0/5:3 is connected to a baremetal server.
      </p></li><li class="listitem "><p>
       tengigEthernet1/0/5:4 is connected to controller 3, as shown in
       the figure <em class="citetitle ">L2 Gateway and 5930 Switch</em>.
       
      </p></li></ul></div><p>
     The switch’s fortygigE1/0/6 interface to which the splitter cable is
     connected splits it into four tengigEthernet (tengigEthernet1/0/6:1 to
     tengigEthernet1/0/6:4) interfaces:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       tengigEthernet1/0/6:1 is connected to controller 2
      </p></li><li class="listitem "><p>
       tengigEthernet1/0/6:2 is connected to controller 1
      </p><p>
       Note: 6:3 and 6:4 are not used although they are available.
      </p></li></ul></div></li><li class="step "><p>
     Split the fortygigE 1/0/5 interface into tengig interfaces:
    </p><div class="verbatim-wrap"><pre class="screen">interface fortygigE 1/0/5
using tengig
The interface FortyGigE1/0/5 will be deleted. Continue? [Y/N]: y</pre></div></li><li class="step "><p>
     Configure the Ten-GigabitEthernet1/0/5:1 interface:
    </p><div class="verbatim-wrap"><pre class="screen">interface Ten-GigabitEthernet1/0/5:1
port link-type trunk
port trunk permit vlan 83</pre></div></li></ol></div></div></div><div class="sect1" id="idm139651570538976"><div class="titlepage"><div><div><h2 class="title" id="idm139651570538976"><span class="number">7.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Provider Data Path Network</span> <a title="Permalink" class="permalink" href="#idm139651570538976">#</a></h2></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Configure the Ten-GigabitEthernet1/0/5:2 interface:
    </p><div class="verbatim-wrap"><pre class="screen">interface Ten-GigabitEthernet1/0/5:2
port link-type trunk
port trunk permit vlan 103
port trunk permit vlan 1261 to 1270
port trunk pvid vlan 103</pre></div></li><li class="step "><p>
     Configure the Ten-GigabitEthernet1/0/5:4 interface:
    </p><div class="verbatim-wrap"><pre class="screen">interface Ten-GigabitEthernet1/0/5:4
port link-type trunk
port trunk permit vlan 103
port trunk permit vlan 1261 to 1270
port trunk pvid vlan 103</pre></div></li><li class="step "><p>
     Configure the Ten-GigabitEthernet1/0/5:3 interface:
    </p><div class="verbatim-wrap"><pre class="screen">interface Ten-GigabitEthernet1/0/5:3
port link-type trunk
port trunk permit vlan 183
vtep access port</pre></div></li><li class="step "><p>
     Split the fortygigE 1/0/6 interface into tengig interfaces:
    </p><div class="verbatim-wrap"><pre class="screen">interface fortygigE 1/0/6
using tengig
The interface FortyGigE1/0/6 will be deleted. Continue? [Y/N]: y</pre></div></li><li class="step "><p>
     Configure the Ten-GigabitEthernet1/0/6:1 interface:
    </p><div class="verbatim-wrap"><pre class="screen">interface Ten-GigabitEthernet1/0/6:1
port link-type trunk
port trunk permit vlan 103
port trunk permit vlan 1261 to 1270
port trunk pvid vlan 103</pre></div></li><li class="step "><p>
     Configure the Ten-GigabitEthernet1/0/6:2 interface:
    </p><div class="verbatim-wrap"><pre class="screen">interface Ten-GigabitEthernet1/0/6:2
port link-type trunk
port trunk permit vlan 103
port trunk permit vlan 1261 to 1270
port trunk pvid vlan 103</pre></div></li><li class="step "><p>
     Enable l2vpn:
    </p><div class="verbatim-wrap"><pre class="screen">l2vpn enable</pre></div></li><li class="step "><p>
     Configure a passive TCP connection for OVSDB on port 6632:
    </p><div class="verbatim-wrap"><pre class="screen">ovsdb server ptcp port 6632</pre></div></li><li class="step "><p>
     Enable OVSDB server:
    </p><div class="verbatim-wrap"><pre class="screen">ovsdb server enable</pre></div></li><li class="step "><p>
     Enable a VTEP process:
    </p><div class="verbatim-wrap"><pre class="screen">vtep enable</pre></div></li><li class="step "><p>
     Configure 10.10.83.3 as the VTEP source IP. This acts as a hardware VTEP
     IP.
    </p><div class="verbatim-wrap"><pre class="screen">tunnel global source-address 10.10.83.3</pre></div></li><li class="step "><p>
     Configure the VTEP access port:
    </p><div class="verbatim-wrap"><pre class="screen">interface Ten-GigabitEthernet1/0/5:3
vtep access port</pre></div></li><li class="step "><p>
     Disable VxLAN tunnel mac-learning:
    </p><div class="verbatim-wrap"><pre class="screen">vxlan tunnel mac-learning disable</pre></div></li><li class="step "><p>
     Display the current configuration of the 5930 switch and verify the
     configuration:
    </p><div class="verbatim-wrap"><pre class="screen">display current-configuration</pre></div></li></ol></div></div><p>
   After switch configuration is complete, you can dump OVSDB to see the
   entries.
  </p><div class="procedure "><div class="procedure-contents"><ul class="procedure"><li class="step "><p>
     Run the ovsdb-client from any Linux machine reachable by the switch:
    </p><div class="verbatim-wrap"><pre class="screen">ovsdb-client dump --pretty tcp:10.10.85.10:6632

sdn@small-linuxbox:~$ ovsdb-client dump --pretty tcp:10.10.85.10:6632
Arp_Sources_Local table
_uuid locator src_mac
----- ------- -------

Arp_Sources_Remote table
_uuid locator src_mac
----- ------- -------

Global table
_uuid                                managers switches
------------------------------------ -------- --------------------------------------
2c891edc-439b-4144-84d9-fa...bf []       [f5f4b43b-40bc-4640-b580-d4...88]

Logical_Binding_Stats table
_uuid bytes_from_local bytes_to_local packets_from_local packets_to_local
----- ---------------- -------------- ------------------ ----------------

Logical_Router table
_uuid description name static_routes switch_binding
----- ----------- ---- ------------- --------------

Logical_Switch table
_uuid description name tunnel_key
----- ----------- ---- ----------

Manager table
_uuid inactivity_probe is_connected max_backoff other_config status target
----- ---------------- ------------ ----------- ------------ ------ ------

Mcast_Macs_Local table
MAC _uuid ipaddr locator_set logical_switch
--- ----- ------ ----------- --------------

Mcast_Macs_Remote table
MAC _uuid ipaddr locator_set logical_switch
--- ----- ------ ----------- --------------

Physical_Locator table
_uuid dst_ip encapsulation_type
----- ------ ------------------

Physical_Locator_Set table
_uuid locators
----- --------

Physical_Port table
_uuid      description name                         port_flt_status vlan_bindings vlan_stats
---------- ----------- ---------------------------- --------------- ------------- ----------
fda9...07e ""          "Ten-GigabitEthernet1/0/5:3" [UP]            {}            {}

Physical_Switch table
_uuid     desc... mgmnt_ips name       ports       sw_flt_status tunnel_ips     tunnels
--------- ------- --------- ---------- ----------- ------------- -------------- -------
f5f...688 ""      []        "L2GTWY02" [fda...07e] []            ["10.10.83.3"] []

Tunnel table
_uuid bfd_config_local bfd_config_remote bfd_params bfd_status local remote
----- ---------------- ----------------- ---------- ---------- ----- ------

Ucast_Macs_Local table
MAC _uuid ipaddr locator logical_switch
--- ----- ------ ------- --------------

Ucast_Macs_Remote table
MAC _uuid ipaddr locator logical_switch
--- ----- ------ ------- --------------</pre></div></li></ul></div></div></div><div class="sect1" id="idm139651570511984"><div class="titlepage"><div><div><h2 class="title" id="idm139651570511984"><span class="number">7.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling and Configuring the L2 Gateway Agent</span> <a title="Permalink" class="permalink" href="#idm139651570511984">#</a></h2></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Update the input model (in <code class="literal">control_plane.yml</code>) to
     specify where you want to run the neutron-l2gateway-agent. For example,
     see the line in bold in the following yml:
    </p><div class="verbatim-wrap"><pre class="screen">---
 product:
 version: 2
 control-planes:
 - name: cp
   region-name: region1
 failure-zones:
 - AZ1
   common-service-components:
     - logging-producer
     - openstack-monasca-agent
     - freezer-agent
     - stunnel
     - lifecycle-manager-target
 clusters:
 - name: cluster1
   cluster-prefix: c1
   server-role: ROLE-CONTROLLER
   member-count: 2
   allocation-policy: strict
 service-components:

 ...
 <span class="bold"><strong>- neutron-l2gateway-agent</strong></span>
 ... )</pre></div></li><li class="step "><p>
     Update <code class="literal">l2gateway_agent.ini.j2</code>. For example, here the IP
     address (10.10.85.10) must be the management IP address of your 5930
     switch. Open the file in vi:
    </p><div class="verbatim-wrap"><pre class="screen">$ vi /home/stack/my_cloud/config/neutron/l2gateway_agent.ini.j2</pre></div></li><li class="step "><p>
     Then make the changes:
    </p><div class="verbatim-wrap"><pre class="screen">[ovsdb]
# (StrOpt) OVSDB server tuples in the format
# &lt;ovsdb_name&gt;:&lt;ip address&gt;:&lt;port&gt;[,&lt;ovsdb_name&gt;:&lt;ip address&gt;:&lt;port&gt;]
# - ovsdb_name: symbolic name that helps identifies keys and certificate files
# - ip address: the address or dns name for the ovsdb server
# - port: the port (ssl is supported)
ovsdb_hosts = hardware_vtep:10.10.85.10:6632</pre></div></li><li class="step "><p>
     By default, the L2 gateway agent initiates a connection to OVSDB servers
     running on the L2 gateway switches. Set the attribute
     <code class="literal">enable_manager</code> to <code class="literal">True </code>if you want
     to change this behavior (to make L2 gateway switches initiate a connection
     to the L2 gateway agent). In this case, it is assumed that the Manager
     table in the OVSDB hardware_vtep schema on the L2 gateway switch has been
     populated with the management IP address of the L2 gateway agent and the
     port.
    </p><div class="verbatim-wrap"><pre class="screen">#enable_manager = False
#connection can be initiated by the ovsdb server.
#By default 'enable_manager' value is False, turn on the variable to True
#to initiate the connection from ovsdb server to l2gw agent.</pre></div></li><li class="step "><p>
     If the port that is configured with <code class="literal">enable_manager =
     True</code> is any port other than 6632, update the
     <code class="literal">2.0/services/neutron/l2gateway-agent.yml</code> input model
     file with that port number:
    </p><div class="verbatim-wrap"><pre class="screen">endpoints:
    - port: '6632'
      roles:
      - ovsdb-server</pre></div></li><li class="step "><p>
     Note: The following command can be used to set the Manager table on the
     switch from a remote system:
    </p><div class="verbatim-wrap"><pre class="screen">sudo vtep-ctl --db=tcp:10.10.85.10:6632 set-manager tcp:10.10.85.130:6632</pre></div></li><li class="step "><p>
     For SSL communication, the command is:
    </p><div class="verbatim-wrap"><pre class="screen">sudo vtep-ctl --db=tcp:10.10.85.10:6632 set-manager ssl:10.10.85.130:6632</pre></div><p>
     where <span class="bold"><strong>10.10.85.10</strong></span> is the management IP
     address of the L2 gateway switch and
     <span class="bold"><strong>10.10.85.130</strong></span> is the management IP of the
     host on which the L2 gateway agent runs.
    </p><p>
     Therefore, in the above topology, this command has to be repeated for
     <span class="bold"><strong>10.10.85.131</strong></span> and
     <span class="bold"><strong>10.10.85.132</strong></span>.
    </p></li><li class="step "><p>
     If you are not using SSL, comment out the following:
    </p><div class="verbatim-wrap"><pre class="screen">#l2_gw_agent_priv_key_base_path={{ neutron_l2gateway_agent_creds_dir }}/keys
#l2_gw_agent_cert_base_path={{ neutron_l2gateway_agent_creds_dir }}/certs
#l2_gw_agent_ca_cert_base_path={{ neutron_l2gateway_agent_creds_dir }}/ca_certs</pre></div></li><li class="step "><p>
     If you are using SSL, then rather than commenting out the attributes,
     specify the directory path of the private key, the certificate, and the CA
     cert that the agent should use to communicate with the L2 gateway switch
     which has the OVSDB server enabled for SSL communication.
    </p><p>
     Make sure that the directory path of the files is given permissions 755,
     and the files’ owner is root and the group is root with 644 permissions.
    </p><p>
     <span class="bold"><strong>Private key:</strong></span> The name should be the same
     as the symbolic name used above in ovsdb_hosts attribute. The extension of
     the file should be ".key". With respect to the above example, the filename
     will be hardware_vtep.key
    </p><p>
     <span class="bold"><strong>Certificate</strong></span> The name should be the same
     as the symbolic name used above in ovsdb_hosts attribute. The extension of
     the file should be “.cert”. With respect to the above example, the
     filename will be hardware_vtep.cert
    </p><p>
     <span class="bold"><strong>CA certificate</strong></span> The name should be the
     same as the symbolic name used above in ovsdb_hosts attribute. The
     extension of the file should be “.ca_cert”. With respect to the above
     example, the filename will be hardware_vtep.ca_cert
    </p></li><li class="step "><p>
     To enable the HPE 5930 switch for SSL communication, execute the following
     commands:
    </p><div class="verbatim-wrap"><pre class="screen">undo ovsdb server ptcp
undo ovsdb server enable
ovsdb server ca-certificate flash:/cacert.pem bootstrap
ovsdb server certificate flash:/sc-cert.pem
ovsdb server private-key flash:/sc-privkey.pem
ovsdb server pssl port 6632
ovsdb server enable</pre></div></li><li class="step "><p>
     Data from the OVSDB sever with SSL can be viewed using the following
     command:
    </p><div class="verbatim-wrap"><pre class="screen">ovsdb-client -C &lt;ca-cert.pem&gt; -p &lt;client-private-key.pem&gt; -c &lt;client-cert.pem&gt; \
dump ssl:10.10.85.10:6632</pre></div></li></ol></div></div></div><div class="sect1" id="idm139651565876752"><div class="titlepage"><div><div><h2 class="title" id="idm139651565876752"><span class="number">7.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Routing Between Software and Hardware - VTEP Networks</span> <a title="Permalink" class="permalink" href="#idm139651565876752">#</a></h2></div></div></div><p>
   In order to allow L2 gateway switches to send VxLAN packets over the correct
   tunnels destined for the compute node and controller node VTEPs, you must
   ensure that the cloud VTEP (compute and controller) IP addresses are in
   different network/subnet from that of the L2 gateway switches. You must also
   create a route between these two networks. This is explained below.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     In the following example of the input model file
     <code class="literal">networks.yml</code>, the GUEST-NET represents the cloud data
     VxLAN network. REMOTE-NET is the network that represents the hardware VTEP
     network.
    </p><div class="verbatim-wrap"><pre class="screen"># networks.yml
 networks:
    - name: GUEST-NET
      vlanid: 103
      tagged-vlan: false
      cidr: 10.1.1.0/24
      gateway-ip: 10.1.1.10
      network-group: GUEST

    - name: REMOTE-NET
      vlanid: 183
      tagged-vlan: false
      cidr: 10.10.83.0/24
      gateway-ip: 10.10.83.3
      network-group: REMOTE</pre></div></li><li class="step "><p>
     The route must be configured between the two networks in the
     <code class="literal">network-groups.yml</code> input model file:
    </p><div class="verbatim-wrap"><pre class="screen"># network_groups.yml
 network-groups:
    - name: REMOTE
      routes:
        - GUEST

    - name: GUEST
      hostname-suffix: guest
      tags:
        - neutron.networks.vxlan:
            tenant-vxlan-id-range: "1:5000"
      routes:
        - REMOTE</pre></div><div id="idm139651565870656" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      Note that the IP route is configured on the compute
      node. Per this route, the HPE 5930 acts as a gateway that routes between
      the two networks.
     </p></div></li><li class="step "><p>
     On the compute node, it looks like this:
    </p><div class="verbatim-wrap"><pre class="screen">stack@padawan-cp1-comp0001-mgmt:~$ sudo ip route
10.10.83.0/24 via 10.1.1.10 dev eth4</pre></div></li><li class="step "><p>
     Run the following Ansible playbooks to apply the changes.
    </p><p>
     config-processor-run.yml:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
     ready-deployment.yml:
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div><p>
     ardana-reconfigure.yml:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible/
ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml</pre></div></li></ol></div></div><p>
   Notes:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Make sure that the controller cluster is able to reach the management IP
     address of the L2 gateway switch. Otherwise, the L2 gateway agents running
     on the controllers will not be able to reach the gateway switches.
    </p></li><li class="listitem "><p>
     Make sure that the interface on the baremetal server connected to the 5930
     switch is tagged (this is explained shortly).
    </p></li></ul></div></div><div class="sect1" id="idm139651565860512"><div class="titlepage"><div><div><h2 class="title" id="idm139651565860512"><span class="number">7.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Connecting a Bare-Metal Server to the HPE 5930 Switch</span> <a title="Permalink" class="permalink" href="#idm139651565860512">#</a></h2></div></div></div><p>
   As SUSE Linux Enterprise Server (bare-metal server) is not aware of tagged packets, it is not
   possible for a virtual machine to communicate with a baremetal box.
  </p><p>
   As the administrator, you must manually perform configuration changes to the
   interface in the HPE 5930 switch to which the baremetal server is connected
   so that the switch can send untagged packets. Either one of the following
   command sets can be used to do so:
  </p><div class="verbatim-wrap"><pre class="screen">Interface &lt;interface number&gt;
service-instance &lt;service-instance id&gt;
encapsulation untagged
xconnect vsi &lt;vsi-name&gt;</pre></div><p>
   Or:
  </p><div class="verbatim-wrap"><pre class="screen">Interface &lt;interface number&gt;
service-instance &lt;service-instance id&gt;
encapsulation s-vid &lt;vlan-id&gt;
xconnect vsi &lt;vsi-name&gt; access-mode ethernet</pre></div><p>
   There are two ways of configuring the baremetal server to communicate with
   virtual machines. If the switch sends tagged traffic, then the baremetal
   server should be able to receive the tagged traffic.
  </p></div><div class="sect1" id="idm139651565855696"><div class="titlepage"><div><div><h2 class="title" id="idm139651565855696"><span class="number">7.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuration on a Bare-Metal Server</span> <a title="Permalink" class="permalink" href="#idm139651565855696">#</a></h2></div></div></div><p>
   If the configuration changes mentioned previously are not made on the
   switch to send untagged traffic to the bare-metal server, to make the
   bare-metal server receive tagged traffic from the switch, perform the
   following on the bare-metal server:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Bare-metal management IP is 10.10.85.129 on interface em1
    </p></li><li class="listitem "><p>
     Switch 5930 must be connected to baremetal on eth1
    </p></li><li class="listitem "><p>
     IP address must be set into tagged interface of eth1
    </p></li></ul></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Create a tagged (VLAN 183) interface
    </p><div class="verbatim-wrap"><pre class="screen">vconfig add eth1 183</pre></div></li><li class="step "><p>
     
     Assign the IP address (10.10.10.129) to eth1.183 tagged interface (IP
     from the subnet 10.10.10.0/24 as VM (10.10.10.4) spawned in Compute node
     belongs to this subnet).
    </p><div class="verbatim-wrap"><pre class="screen">ifconfig eth1.183 10.10.10.129/24</pre></div></li></ol></div></div></div><div class="sect1" id="idm139651565847088"><div class="titlepage"><div><div><h2 class="title" id="idm139651565847088"><span class="number">7.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">NIC Bonding and IRF Configuration</span> <a title="Permalink" class="permalink" href="#idm139651565847088">#</a></h2></div></div></div><p>
   With L2 gateway in actiondeployment, NIC bonding can be enabled on compute
   nodes. For more details on NIC bonding, please refer to
   <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 7 “Configuration Objects”, Section 7.11 “Interface Models”</span>.
   In order to achieve high availability, HPE 5930 switches can be
   configured to form a cluster using Intelligent Resilient Framework (IRF).
   Please refer to the
   <a class="link" href="http://h20564.www2.hpe.com/hpsc/doc/public/display?docId=c04567141" target="_blank">HPE
   FlexFabric 5930 Switch Series configuration guide</a> for details.
  </p></div><div class="sect1" id="idm139651565843536"><div class="titlepage"><div><div><h2 class="title" id="idm139651565843536"><span class="number">7.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Scale Numbers Tested</span> <a title="Permalink" class="permalink" href="#idm139651565843536">#</a></h2></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Number of neutron port MACs tested on a single switch: 4000
    </p></li><li class="listitem "><p>
     Number of HPE 5930 switches tested: 2
    </p></li><li class="listitem "><p>
     Number of baremetal connected to a single HPE 5930 switch: 100
    </p></li><li class="listitem "><p>
     Number of L2 gateway connections to different networks: 800
    </p></li></ul></div></div><div class="sect1" id="idm139651565838048"><div class="titlepage"><div><div><h2 class="title" id="idm139651565838048"><span class="number">7.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">L2 Gateway Commands</span> <a title="Permalink" class="permalink" href="#idm139651565838048">#</a></h2></div></div></div><p>
   These commands are not part of the L2 gateway deployment. They are to be
   executed after L2 gateway is deployed.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Create Network
    </p><div class="verbatim-wrap"><pre class="screen">neutron net-create net1</pre></div></li><li class="step "><p>
     Create Subnet
    </p><div class="verbatim-wrap"><pre class="screen">neutron subnet-create net1 10.10.10.0/24</pre></div></li><li class="step "><p>
     Boot a tenant VM (nova boot –image &lt;Image-id&gt; --flavor 2 –nic
     net-id=&lt;net_id&gt; &lt;VM name&gt;)
    </p><div class="verbatim-wrap"><pre class="screen">nova boot --image 1f3cd49d-9239-49cf-8736-76bac5360489 --flavor 2 \
  --nic net-id=4f6c58b6-0acc-4e93-bb4c-439b38c27a23 VM</pre></div><p>
     Assume the VM was assigned the IP address 10.10.10.4
    </p></li><li class="step "><p>
     Create the L2 gateway filling in your information here:
    </p><div class="verbatim-wrap"><pre class="screen">neutron l2-gateway-create \
  --device name="<em class="replaceable ">SWITCH_NAME</em>",interface_names="<em class="replaceable ">INTERFACE_NAME</em>" \
  <em class="replaceable ">GATEWAY-NAME</em></pre></div><p>
     For this example:
    </p><div class="verbatim-wrap"><pre class="screen">neutron l2-gateway-create \
  --device name="L2GTWY02",interface_names="Ten-GigabitEthernet1/0/5:3" \
  gw1</pre></div><p>
     Ping from the VM (10.10.10.4) to baremetal server and from baremetal
     server (10.10.10.129) to the VM Ping should not work as there is no
     gateway connection created yet.
    </p></li><li class="step "><p>
     Create l2 gateway Connection
    </p><div class="verbatim-wrap"><pre class="screen">neutron l2-gateway-connection-create gw1 net1 --segmentation-id 183</pre></div></li><li class="step "><p>
     Ping from VM (10.10.10.4) to baremetal and from baremetal (10.10.10.129)
     to VM Ping should work.
    </p></li><li class="step "><p>
     Delete l2 gateway Connection
    </p><div class="verbatim-wrap"><pre class="screen">neutron l2-gateway-connection-delete &lt;gateway id/gateway_name&gt;</pre></div></li><li class="step "><p>
     Ping from the VM (10.10.10.4) to baremetal and from baremetal
     (10.10.10.129) to the VM. Ping should not work as l2 gateway connection
     was deleted.
    </p></li></ol></div></div></div></div></div><div class="part" id="cloudinstallation"><div class="titlepage"><div><div><h1 class="title"><span class="number">Part II </span><span class="name">Cloud Installation </span><a title="Permalink" class="permalink" href="#cloudinstallation">#</a></h1></div></div></div><div class="toc"><dl><dt><span class="chapter"><a href="#cloudinstallation_overview"><span class="number">8 </span><span class="name">Overview</span></a></span></dt><dd class="toc-abstract"><p>
   Before starting the installation, review the sample configurations offered
   by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> in <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”</span>.
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> ships with highly tuned example configurations for each of
   these cloud models:
  </p></dd><dt><span class="chapter"><a href="#install_gui"><span class="number">9 </span><span class="name">Installing with the Install UI</span></a></span></dt><dd class="toc-abstract"><p>
 </p></dd><dt><span class="chapter"><a href="#DesignateInstallOverview"><span class="number">10 </span><span class="name">DNS Service Installation Overview</span></a></span></dt><dd class="toc-abstract"><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> DNS Service supports several different backends for domain name
  service. The choice of backend must be included in the deployment model
  before the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> install is completed.
 </p></dd><dt><span class="chapter"><a href="#MagnumOverview"><span class="number">11 </span><span class="name">Magnum Overview</span></a></span></dt><dd class="toc-abstract"><p>The SUSE OpenStack Cloud Magnum Service provides container orchestration engines such as Docker Swarm, Kubernetes, and Apache Mesos available as first class resources. SUSE OpenStack Cloud Magnum uses Heat to orchestrate an OS image which contains Docker and Kubernetes and runs that image in either …</p></dd><dt><span class="chapter"><a href="#using_git"><span class="number">12 </span><span class="name">Using Git for Configuration Management</span></a></span></dt><dd class="toc-abstract"><p>In SUSE OpenStack Cloud 8, a local git repository is used to track configuration changes; the Configuration Processor (CP) uses this repository. Use of a git workflow means that your configuration history is maintained, making rollbacks easier and keeping a record of previous configuration settings.…</p></dd><dt><span class="chapter"><a href="#install_kvm"><span class="number">13 </span><span class="name">Installing Mid-scale and Entry-scale KVM</span></a></span></dt><dt><span class="chapter"><a href="#install_ironic_overview"><span class="number">14 </span><span class="name">Installing Baremetal (Ironic)</span></a></span></dt><dd class="toc-abstract"><p>
  Bare Metal as a Service is enabled in this release for deployment of Nova
  instances on bare metal nodes using flat networking.
  
 </p></dd><dt><span class="chapter"><a href="#install_swift"><span class="number">15 </span><span class="name">Installation for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale Cloud with Swift Only</span></a></span></dt><dd class="toc-abstract"><p>
  This page describes the installation step requirements for the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  Entry-scale Cloud with Swift Only model.
 </p></dd><dt><span class="chapter"><a href="#install_sles_compute"><span class="number">16 </span><span class="name">Installing SLES Compute</span></a></span></dt><dt><span class="chapter"><a href="#install_heat_templates"><span class="number">17 </span><span class="name">Installation of SUSE CaaS Platform Heat Templates</span></a></span></dt><dd class="toc-abstract"><p>
  This chapter describes how to install SUSE CaaS Platform Heat template on
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
 </p></dd><dt><span class="chapter"><a href="#integrations"><span class="number">18 </span><span class="name">Integrations</span></a></span></dt><dd class="toc-abstract"><p>
  Once you have completed your cloud installation, these are some of the common
  integrations you may want to perform.
 </p></dd><dt><span class="chapter"><a href="#troubleshooting_installation"><span class="number">19 </span><span class="name">Troubleshooting the Installation</span></a></span></dt><dd class="toc-abstract"><p>
  We have gathered some of the common issues that occur during installation and
  organized them by when they occur during the installation. These sections
  will coincide with the steps labeled in the installation instructions.
 </p></dd><dt><span class="chapter"><a href="#esx_troubleshooting_installation"><span class="number">20 </span><span class="name">Troubleshooting the ESX</span></a></span></dt><dd class="toc-abstract"><p>
  This section contains troubleshooting tasks for your <span class="phrase"><span class="phrase">SUSE® <span class="productname">OpenStack</span> Cloud</span></span>
  <span class="phrase"><span class="phrase">8</span></span> for ESX.
 </p></dd></dl></div><div class="chapter " id="cloudinstallation_overview"><div class="titlepage"><div><div><h2 class="title"><span class="number">8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Overview</span> <a title="Permalink" class="permalink" href="#cloudinstallation_overview">#</a></h2></div></div></div><div class="line"></div><p>
   Before starting the installation, review the sample configurations offered
   by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> in <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”</span>.
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> ships with highly tuned example configurations for each of
   these cloud models:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”, Section 10.3 “KVM Examples”, Section 10.3.1 “Entry-Scale Cloud”</span>
    </p></li><li class="listitem "><p>
     <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”, Section 10.4 “ESX Examples”, Section 10.4.1 “Single-Region Entry-Scale Cloud with a Mix of KVM and ESX Hypervisors”</span>
    </p></li><li class="listitem "><p>
     <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”, Section 10.5 “Swift Examples”, Section 10.5.1 “Entry-scale Swift Model”</span>
    </p></li><li class="listitem "><p>
     <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”, Section 10.3 “KVM Examples”, Section 10.3.3 “Single-Region Mid-Size Model”</span>
    </p></li></ul></div><div class="informaltable"><table border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Name</th><th>Location</th></tr></thead><tbody><tr><td>
      <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”, Section 10.3 “KVM Examples”, Section 10.3.1 “Entry-Scale Cloud”</span>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-kvm</code>
     </td></tr><tr><td>
      <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”, Section 10.3 “KVM Examples”, Section 10.3.2 “Entry Scale Cloud with Metering and Monitoring Services”</span>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-kvm-mml</code>
     </td></tr><tr><td><span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”, Section 10.4 “ESX Examples”, Section 10.4.1 “Single-Region Entry-Scale Cloud with a Mix of KVM and ESX Hypervisors”</span>
     </td><td><code class="filename">~/openstack/examples/entry-scale-esx-kvm</code>
     </td></tr><tr><td>
      <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”, Section 10.4 “ESX Examples”, Section 10.4.2 “Single-Region Entry-Scale Cloud with Metering and Monitoring Services, and a Mix of KVM and ESX Hypervisors”</span>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-esx-kvm-mml</code>
     </td></tr><tr><td>
      <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”, Section 10.5 “Swift Examples”, Section 10.5.1 “Entry-scale Swift Model”</span>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-swift</code>
     </td></tr><tr><td>
      <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”, Section 10.6 “Ironic Examples”, Section 10.6.1 “Entry-Scale Cloud with Ironic Flat Network”</span>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-ironic-flat-network</code>
     </td></tr><tr><td>
      <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”, Section 10.6 “Ironic Examples”, Section 10.6.2 “Entry-Scale Cloud with Ironic Multi-Tenancy”</span>
     </td><td>
      <code class="filename">~/openstack/examples/entry-scale-ironic-multi-tenancy</code>
     </td></tr><tr><td><span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”, Section 10.3 “KVM Examples”, Section 10.3.3 “Single-Region Mid-Size Model”</span>
     </td><td>
      <code class="filename">~/openstack/examples/mid-scale-kvm</code>
     </td></tr></tbody></table></div><p>
   <span class="bold"><strong>Using the Command-line</strong></span>
  </p><p>
   You should use the command-line if:
  </p><div class="itemizedlist " id="idg-installation-installation-cloudinstallation_overview-xml-7"><ul class="itemizedlist"><li class="listitem "><p>
     You are installing a more complex or large-scale cloud.
    </p></li><li class="listitem "><p>
     You need to use availability zones or the server groups functionality of
     the cloud model. For more information, see the
     <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 6 “Input Model”</span>.
    </p></li><li class="listitem "><p>
     You want to customize the cloud configuration beyond the tuned defaults
     that <span class="phrase"><span class="phrase">SUSE</span></span> provides out of the box.
    </p></li><li class="listitem "><p>
     You need deeper customizations than are possible to express using the GUI.
    </p></li></ul></div><p>
   Instructions for installing via the command-line are here:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <a class="xref" href="#install_kvm" title="Chapter 13. Installing Mid-scale and Entry-scale KVM">Chapter 13, <em>Installing Mid-scale and Entry-scale KVM</em></a>
    </p></li></ul></div></div><div class="chapter " id="install_gui"><div class="titlepage"><div><div><h2 class="title"><span class="number">9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing with the Install UI</span> <a title="Permalink" class="permalink" href="#install_gui">#</a></h2></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#idm139651565764176"><span class="number">9.1 </span><span class="name">Before You Start</span></a></span></dt><dt><span class="section"><a href="#idm139651565727344"><span class="number">9.2 </span><span class="name">Preparing to Run the Install UI</span></a></span></dt><dt><span class="section"><a href="#create_csv_file"><span class="number">9.3 </span><span class="name">Optional: Creating a CSV File to Import Server Data</span></a></span></dt><dt><span class="section"><a href="#discover_servers"><span class="number">9.4 </span><span class="name">Optional: Importing Certificates for SUSE Manager and HPE OneView</span></a></span></dt><dt><span class="section"><a href="#idm139651565208272"><span class="number">9.5 </span><span class="name">Running the Install UI</span></a></span></dt></dl></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> comes with a GUI-based installation wizard for first-time cloud
  installations. It will guide you through the configuration process and deploy
  your cloud based on the custom configuration you provide.  The Install UI
  will start with a set of example cloud configurations for you to choose
  from. Based on your cloud choice, you can refine your configuration to match
  your needs using Install UI widgets. You can also directly edit your model
  configuration files.
 </p><p>
  When you are satisfied with your configuration and the Install UI has
  validated your configuration successfully, you can then deploy the cloud into
  your environment. Deploying the cloud will version-control your configuration
  into a git repository and provide you with live progress of your deployment.
 </p><p>
  With the Install UI, you have the option of provisioning SLES12-SP3 to
  IPMI-capable machines described in your configuration files. Provisioning
  machines with the Install UI will also properly configure them for Ansible
  access.
 </p><p>
  The Install UI is designed to make the initial installation process
  simpler, more accurate, and faster than manual installation.
 </p><div class="sect1" id="idm139651565764176"><div class="titlepage"><div><div><h2 class="title" id="idm139651565764176"><span class="number">9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Before You Start</span> <a title="Permalink" class="permalink" href="#idm139651565764176">#</a></h2></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Review the <a class="xref" href="#preinstall_checklist" title="Chapter 2. Pre-Installation Checklist">Chapter 2, <em>Pre-Installation Checklist</em></a> about recommended
     pre-installation tasks.
    </p></li><li class="step "><p>
     Prepare the Cloud Lifecycle Manager node. The Cloud Lifecycle Manager must be accessible either directly or via
     <code class="literal">ssh</code>, and have SUSE Linux Enterprise Server 12 SP3 installed. All nodes must
     be accessible to the Cloud Lifecycle Manager. If the nodes do not have direct access to
     online Cloud subscription channels, the Cloud Lifecycle Manager node will need to host the
     Cloud repositories.
    </p><ol type="a" class="substeps "><li class="step "><p>
       If you followed the installation instructions for Cloud Lifecycle Manager (see <a class="xref" href="#cha.depl.dep_inst" title="Chapter 3. Installing the Cloud Lifecycle Manager server">Chapter 3, <em>Installing the Cloud Lifecycle Manager server</em></a> <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> should already be
       installed. Double-check whether SUSE Linux Enterprise and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> are properly
       registered at the SUSE Customer Center by starting YaST and running <span class="guimenu">Software</span> › <span class="guimenu">Product
       Registration</span>.
      </p><p>
       If you have not yet installed <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, do so by starting YaST and
       running <span class="guimenu">Software</span> › <span class="guimenu">Product
       Registration</span> › <span class="guimenu">Select
       Extensions</span>. Choose <span class="guimenu"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> 
       and follow the on-screen instructions. Make sure to register
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> during the installation process and to install the software
       pattern <code class="literal">patterns-cloud-ardana</code>.
      </p></li><li class="step "><p>
       Ensure the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> media repositories and updates repositories are
       made available to all nodes in your deployment. This can be accomplished
       either by configuring the Cloud Lifecycle Manager server as an SMT mirror as described in
       <a class="xref" href="#app.deploy.smt_lcm" title="Chapter 4. Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)">Chapter 4, <em>Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)</em></a> or by syncing or mounting the
       Cloud and updates repositories to the Cloud Lifecycle Manager server as described in <a class="xref" href="#cha.depl.repo_conf_lcm" title="Chapter 5. Software Repository Setup">Chapter 5, <em>Software Repository Setup</em></a>.
      </p></li><li class="step "><p>
       Configure passwordless <code class="command">sudo</code> for the user created when
       setting up the node (as described in <a class="xref" href="#sec.depl.adm_inst.user" title="3.5. Creating a User">Section 3.5, “Creating a User”</a>). Note that this is
       <span class="emphasis"><em>not</em></span> the user <code class="systemitem">ardana</code> that will be used later in this
       procedure. In the following we assume you named the user <code class="systemitem">cloud</code>. Run the command
       <code class="command">visudo</code> as user <code class="systemitem">root</code> and add the following line
       to the end of the file:
      </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable ">cloud</em> ALL = (root) NOPASSWD:ALL</pre></div><p>
       Make sure to replace <em class="replaceable ">cloud</em> with your user name
       choice.
      </p></li><li class="step "><p>
       Set the password for the user
       <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen">sudo passwd ardana</pre></div></li><li class="step "><p>
       Become the user <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen">su - ardana</pre></div></li><li class="step "><p>
       Place a copy of the SUSE Linux Enterprise Server 12 SP3 <code class="filename">.iso</code> in the
       <code class="systemitem">ardana</code> home directory,
       <code class="filename">var/lib/ardana</code>, and rename it to
       <code class="filename">sles12sp3.iso</code>.
      </p></li></ol></li></ol></div></div></div><div class="sect1" id="idm139651565727344"><div class="titlepage"><div><div><h2 class="title" id="idm139651565727344"><span class="number">9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Preparing to Run the Install UI</span> <a title="Permalink" class="permalink" href="#idm139651565727344">#</a></h2></div></div></div><p>
   Before you launch the Install UI to install your cloud, do the following:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Gather the following details from the servers that will make up your
     cloud:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Server names
      </p></li><li class="listitem "><p>
       IP addresses
      </p></li><li class="listitem "><p>
       Server roles
      </p></li><li class="listitem "><p>
       PXE MAC addresses
      </p></li><li class="listitem "><p>
       PXE IP addresses
      </p></li><li class="listitem "><p>
       PXE interfaces
      </p></li><li class="listitem "><p>
       IPMI IP address, username, password
      </p></li></ul></div></li><li class="step "><p>
     Choose an input model from <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”</span>. No
     action other than an understanding of your needs is necessary at this
     point. In the Install UI you will indicate which input model you wish to
     deploy.
    </p></li><li class="step "><p>
     Before you use the Install UI to install your cloud, you may install the
     operating system, SLES, on your nodes (servers) if you choose.
     Otherwise, the Install UI will install it for you.
    </p><p> If you are installing the operating system on all nodes yourself,
    you must do so using the SLES image included in the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>
    package.
    </p></li></ol></div></div><p>
  In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, a local git repository is used to track configuration
  changes; the Configuration Processor (CP) uses this repository. Use of a git
  workflow means that your configuration history is maintained, making
  rollbacks easier and keeping a record of previous configuration settings. The
  git repository also provides a way for you to merge changes that you pull down as
  <span class="quote">“<span class="quote">upstream</span>”</span> updates (that is, updates from <span class="phrase"><span class="phrase">SUSE</span></span>). It also
  allows you to manage your own configuration changes.
 </p><p>
  The git repository is installed by the Cloud Lifecycle Manager on the Cloud Lifecycle Manager node.
 </p><p>
  Using the Install UI does not require the use of the git repository. After
  the installation, it may be useful to know more about
  <a class="xref" href="#using_git" title="Chapter 12. Using Git for Configuration Management">Chapter 12, <em>Using Git for Configuration Management</em></a>.
 </p></div><div class="sect1" id="create_csv_file"><div class="titlepage"><div><div><h2 class="title" id="create_csv_file"><span class="number">9.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Optional: Creating a CSV File to Import Server Data</span> <a title="Permalink" class="permalink" href="#create_csv_file">#</a></h2></div></div></div><p>
   Before beginning the installation, you can create a CSV file with your
   server information to import directly into the Install UI to avoid
   entering it manually on the <span class="guimenu">Assign Servers</span> page.
  </p><p>
   The following table shows the fields needed for your CSV file.
  </p><div class="informaltable"><table border="1"><colgroup><col class="col1" /><col align="center" class="col2" /><col align="center" class="col3" /><col class="col4" /></colgroup><tbody><tr><td><span class="bold"><strong>Field</strong></span>
      </td><td align="center"><span class="bold"><strong>Required</strong></span>
      </td><td align="center"><span class="bold"><strong>Required for OS Provisioning</strong></span>
      </td><td><span class="bold"><strong>Aliases</strong></span>
      </td></tr><tr><td>Server ID</td><td align="center">Yes</td><td align="center">Yes</td><td>server_id, id</td></tr><tr><td>IP Address</td><td align="center">Yes</td><td align="center">Yes</td><td>ip, ip_address, ip_addr</td></tr><tr><td>MAC Address</td><td align="center">Yes</td><td align="center">Yes</td><td>mac, mac_address, mac_addr</td></tr><tr><td>IPMI IP Address</td><td align="center">No</td><td align="center">Yes</td><td>ipmi_ip, ipmi_ip_address</td></tr><tr><td>IPMI User</td><td align="center">No</td><td align="center">Yes</td><td>ipmi_user, user</td></tr><tr><td>IPMI Password</td><td align="center">No</td><td align="center">Yes</td><td>ipmi_password, password</td></tr><tr><td>Server Role</td><td align="center">No</td><td align="center">No</td><td>server_role, role</td></tr><tr><td>Server Group</td><td align="center">No</td><td align="center">No</td><td>server_group, group</td></tr><tr><td>NIC Mapping</td><td align="center">No</td><td align="center">No</td><td>server_nic_map, nic_map, nic_mapping</td></tr></tbody></table></div><p>
   The aliases are all the valid names that can be used in the CSV file for the
   column header for a given field. Field names are not case sensitive. You can
   use either <code class="literal"> </code> (space) or <code class="literal">-</code> (hyphen) in
   place of underscore for a field name.
  </p><p>
   An example CSV file could be:
  </p><div class="verbatim-wrap"><pre class="screen">id,ip-addr,mac-address,server-group,nic-mapping,server-role,ipmi-ip,ipmi-user
controller1,192.168.110.3,b2:72:8d:ac:7c:6f,RACK1,HP-DL360-4PORT,CONTROLLER-ROLE,192.168.109.3,admin
myserver4,10.2.10.24,00:14:22:01:23:44,AZ1,,,,</pre></div></div><div class="sect1" id="discover_servers"><div class="titlepage"><div><div><h2 class="title" id="discover_servers"><span class="number">9.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Optional: Importing Certificates for SUSE Manager and HPE OneView</span> <a title="Permalink" class="permalink" href="#discover_servers">#</a></h2></div></div></div><p>
   If you intend to use SUSE Manager or HPE OneView to add servers, certificates for
   those services must be accessible to the Install UI.
  </p><p>
   Use the following steps to import a SUSE Manager certificate.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Retrieve the <code class="filename">.pem</code> file from the SUSE Manager.
    </p><div class="verbatim-wrap"><pre class="screen">curl -k https://<em class="replaceable ">SUSE_MANAGER_IP</em>:<em class="replaceable ">PORT</em>/pub/RHN-ORG-TRUSTED-SSL-CERT &gt; <em class="replaceable ">PEM_NAME</em>.pem</pre></div></li><li class="step "><p>
     Copy the <code class="filename">.pem</code> file to the proper location on the
     Cloud Lifecycle Manager.
    </p><div class="verbatim-wrap"><pre class="screen">cd /etc/pki/trust/anchors
sudo cp ~/<em class="replaceable ">PEM_NAME</em>.pem .</pre></div></li><li class="step "><p>
     Install the certificate.
    </p><div class="verbatim-wrap"><pre class="screen">sudo update-ca-certificates</pre></div></li><li class="step "><p>
     Add <em class="replaceable ">SUSE Manager host IP address</em> if
     <em class="replaceable ">SUSE Manager.test.domain</em> is not reachable by DNS.
    </p><div class="verbatim-wrap"><pre class="screen">sudo vi /etc/hosts</pre></div><p>
     Add <em class="replaceable ">SUSE Manager host IP address</em>
     <em class="replaceable ">SUSE Manager.test.domain</em>. For example:
    </p><div class="verbatim-wrap"><pre class="screen">10.10.10.10 SUSE Manager.test.domain</pre></div></li></ol></div></div><p>
   Use the following steps to import an HPE OneView certificate.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step " id="st.oneview.retrieve-id"><p>
     Retrieve the <code class="literal">sessionID</code>.
    </p><div class="verbatim-wrap"><pre class="screen">curl -k -H "X-Api-Version:500" -H "Content-Type: application/json" \
-d '{"userName":<em class="replaceable ">ONEVIEW_USER</em>, "password":<em class="replaceable ">ONEVIEW_PASSWORD</em>, \
"loginMsgAck":"true"}' https://<em class="replaceable ">ONEVIEW_MANAGER_URL</em>:<em class="replaceable ">PORT</em>/rest/login-sessions</pre></div><p>
     The response will be similar to:
    </p><div class="verbatim-wrap"><pre class="screen">{"partnerData":{},"sessionID":"LTYxNjA1O1NjkxMHcI1b2ypaGPscErUOHrl7At3-odHPmR"}</pre></div></li><li class="step "><p>
     Retrieve a Certificate Signing Request (CSR) using the
     <em class="replaceable ">sessionID</em> from
     <a class="xref" href="#st.oneview.retrieve-id" title="Step 1">Step 1</a>.
    </p><div class="verbatim-wrap"><pre class="screen">curl -k -i -H "X-Api-Version:500" -H <em class="replaceable ">sessionID</em> \
<em class="replaceable ">ONEVIEW_MANAGER_URL</em>/rest/certificates/ca \
&gt; <em class="replaceable ">CA_NAME</em>.csr</pre></div></li><li class="step "><p>
     Follow instructions in the HPE OneView User Guide to validate the CSR and
     obtain a signed certificate
     (<em class="replaceable ">CA_NAME</em><code class="filename">.crt</code>).
    </p></li><li class="step "><p>
     Copy the <code class="filename">.crt</code> file to the proper location on the
     Cloud Lifecycle Manager.
    </p><div class="verbatim-wrap"><pre class="screen">cd /etc/pki/trust/anchors
sudo cp ~/data/<em class="replaceable ">CA_NAME</em>.crt .</pre></div></li><li class="step "><p>
     Install the certificate.
    </p><div class="verbatim-wrap"><pre class="screen">sudo update-ca-certificates</pre></div></li><li class="step "><p>
     Follow instructions in your HPE OneView User Guide to import the
     <em class="replaceable ">CA_NAME</em>.crt certificate into HPE OneView.
    </p></li><li class="step "><p>
     Add <em class="replaceable ">HPE OneView host IP address</em> if
     <em class="replaceable ">HPE OneView.test.domain</em> is not reachable by DNS.
    </p><div class="verbatim-wrap"><pre class="screen">sudo vi /etc/hosts</pre></div><p>
     Add <em class="replaceable ">HPE OneView host IP address</em>
     <em class="replaceable ">HPE OneView.test.domain</em> For example:
    </p><div class="verbatim-wrap"><pre class="screen">10.84.84.84  HPE OneView.test.domain</pre></div></li></ol></div></div></div><div class="sect1" id="idm139651565208272"><div class="titlepage"><div><div><h2 class="title" id="idm139651565208272"><span class="number">9.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Running the Install UI</span> <a title="Permalink" class="permalink" href="#idm139651565208272">#</a></h2></div></div></div><div id="idm139651565207536" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    The Install UI must run continuously without stopping for authentication
    at any step. When using the Install UI it is required to launch the Cloud Lifecycle Manager
    with the following command:
   </p><div class="verbatim-wrap"><pre class="screen">ARDANA_INIT_AUTO=1 /usr/bin/ardana-init</pre></div></div><p>
   Deploying the cloud to your servers will reconfigure networking and firewall
   rules on your cloud servers. To avoid problems with these networking changes
   when using the Install UI, we recommend you run a browser directly on your
   Cloud Lifecycle Manager node and point it to <code class="uri">http://localhost:3000</code>.
  </p><p>
   If you cannot run a browser on the Cloud Lifecycle Manager node to perform the install, you
   can run a browser from a Linux-based computer in your
   <span class="bold"><strong>MANAGEMENT</strong></span> network. However, firewall rules
   applied during cloud deployment will block access to the Install UI. To
   avoid blocking the connection, you can use the Install UI via an SSH
   tunnel to the Cloud Lifecycle Manager server. This will allow SSH connections through the
   <span class="bold"><strong>MANAGEMENT</strong></span> network when you reach the
   "Review Configuration Files" step of the install process.
  </p><p>
   To open an SSH tunnel from your Linux-based computer in your
   <span class="bold"><strong>MANAGEMENT</strong></span> network to the Cloud Lifecycle Manager:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Open a new terminal and enter the following command:
    </p><div class="verbatim-wrap"><pre class="screen">ssh -N -L 8080:localhost:3000 ardana@<em class="replaceable ">MANAGEMENT IP address of Cloud Lifecycle Manager</em></pre></div><p>
     The user name and password should be what was set in
     <a class="xref" href="#sec.depl.adm_inst.add_on" title="3.3. Installing the SUSE OpenStack Cloud Extension">Section 3.3, “Installing the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Extension”</a>. There will be no prompt after
     you have logged in.
    </p></li><li class="step "><p>
     Leave this terminal session open to keep the SSH tunnel open and running.
     This SSH tunnel will forward connections from your Linux-based computer
     directly to the Cloud Lifecycle Manager, bypassing firewall restrictions.
    </p></li><li class="step "><p>
     On your local computer (the one you are tunneling from), point your
     browser to <code class="uri">http://localhost:8080</code>.
    </p></li><li class="step "><p>
     If the connection is interrupted, refresh your browser.
    </p></li></ol></div></div><div id="idm139651565194256" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    If you use an SSH tunnel to connect to the Install UI, there is an
    important note in the "Review Configuration Files" step about modifying
    <code class="filename">firewall_rules.yml</code> to allow SSH connections on the
    <span class="bold"><strong>MANAGEMENT</strong></span> network.
   </p></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="idm139651565192032"><span class="name">Overview</span><a title="Permalink" class="permalink" href="#idm139651565192032">#</a></h5></div><p>
   The first page of the Install UI shows the general installation process
   and a reminder to gather some information before beginning. Clicking the
   <span class="guimenu">Next</span> button brings up the <span class="guimenu">Model
   Selection</span> page.
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_intro.png"><img src="images/installer_ui_intro.png" width="" /></a></div></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="idm139651565185552"><span class="name">Choose an <span class="productname">OpenStack</span> Cloud Model</span><a title="Permalink" class="permalink" href="#idm139651565185552">#</a></h5></div><p>
   The input model choices are displayed on this page. Details of each model
   can be seen on the right by clicking the model name on the left. If you have
   already decided some aspects of your cloud environment, models can be
   filtered using the dropdown selections. Narrowing a parameter affects the
   range of choices of models and changes other dropdown choices to only those
   that are compatible.
  </p><p>
   Selecting a model will determine the base template from which the cloud will
   be deployed. Models can be adjusted later in the process, though selecting
   the closest match to your requirements reduces the effort required to deploy
   your cloud.
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_select_model.png"><img src="images/installer_ui_select_model.png" width="" /></a></div></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="idm139651565178816"><span class="name">Cloud Model to Deploy</span><a title="Permalink" class="permalink" href="#idm139651565178816">#</a></h5></div><p>
   Based on the cloud example selected on the previous page, more detail is
   shown about that cloud configuration and the components that will be
   deployed. If you go back and select a different model, the deployment
   process restarts from the beginning. Any configuration changes you have made
   will be deleted.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="emphasis"><em>Mandatory components</em></span> have assigned quantities. We
     strongly suggest not changing those quantities to avoid potential problems
     later in the installation process.
    </p></li><li class="listitem "><p>
     <span class="emphasis"><em>Additional components</em></span> can be adjusted within the
     parameters shown.
    </p></li></ul></div><p>
   The number of nodes (servers) dedicated to each server role can be adjusted.
   Most input models are designed to support High Availability and to
   distribute <span class="productname">OpenStack</span> services appropriately.
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_model_details.png"><img src="images/installer_ui_model_details.png" width="" /></a></div></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="idm139651565168672"><span class="name">Adding Servers and Assigning Server Roles</span><a title="Permalink" class="permalink" href="#idm139651565168672">#</a></h5></div><p>
   This page provides more detail about the number and assignment of each type
   of node based on the information from the previous page (any changes must be
   made there).
  </p><p>
   Components that do not meet the required parameters will be shown in red in
   the accordion bar. Missing required fields and duplicate server names will
   also be red, as will the accordion bar. The <span class="guimenu">Next</span> button
   will be disabled.
  </p><p>
   Servers may be discovered using SUSE Manager, HPE OneView, or both. Ensure that
   the certificates are accessible, as described in
   <a class="xref" href="#discover_servers" title="9.4. Optional: Importing Certificates for SUSE Manager and HPE OneView">Section 9.4, “Optional: Importing Certificates for SUSE Manager and HPE OneView”</a>. Clicking the <span class="guimenu">Discover</span>
   button will prompt for access credentials to the system management software
   to be used for discovery. Certificates can be verified by checking
   <span class="guimenu">Verify SSL certificate</span>. After validating credentials,
   Discovery will retrieve a list of known servers from SUSE Manager and/or
   HPE OneView and allow access to server details on those management platforms.
  </p><p>
   You can drag and drop to move servers from the left to the right in order to
   assign server roles, from right to left, or up and down in the accordion
   bar.
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_assign_servers.png"><img src="images/installer_ui_assign_servers.png" width="" /></a></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_discovery.png"><img src="images/installer_ui_discovery.png" width="" /></a></div></div><p>
   Server information may also be entered manually or imported via CSV in the
   <span class="guimenu">Manual Entry</span> tab. The format for CSV entry is described
   in <a class="xref" href="#create_csv_file" title="9.3. Optional: Creating a CSV File to Import Server Data">Section 9.3, “Optional: Creating a CSV File to Import Server Data”</a>. The server assignment list includes
   placeholder server details that can be edited to reflect real hardware, or
   can be removed and replaced with discovered or manually added systems.
  </p><p>
   For more information about server roles, see
   <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 6 “Input Model”, Section 6.2 “Concepts”, Section 6.2.4 “Server Roles”</span>.
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_add_servers_manually.png"><img src="images/installer_ui_add_servers_manually.png" width="" /></a></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_add_server_manually.png"><img src="images/installer_ui_add_server_manually.png" width="" /></a></div></div><p>
   Subnet and netmask values should be set on this page as they may impact the
   IP addresses being assigned to various servers.
  </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="idm139651565144896"><span class="name">Choose servers on which SLES will be installed</span><a title="Permalink" class="permalink" href="#idm139651565144896">#</a></h5></div><p>
   If an OS has not previously been installed on the servers that make up the
   cloud configuration, the OS installation page allows for Cobbler to deploy
   SLES on servers in the cloud configuration. Enter password, select
   servers and click <span class="guimenu">Install</span> to deploy SLES to these
   servers. An installation log and progress indicators will be displayed.
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_install_os.png"><img src="images/installer_ui_install_os.png" width="" /></a></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_install_os_inprogress.png"><img src="images/installer_ui_install_os_inprogress.png" width="" /></a></div></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="idm139651565134864"><span class="name">Server and Role Summary</span><a title="Permalink" class="permalink" href="#idm139651565134864">#</a></h5></div><p>
   When the OS installation is complete, a Server and Server Role Summary page
   is displayed. It shows which servers have been assigned to each role, and
   provides an opportunity to edit the server configurations. Various cloud
   components can be configured by clicking on the <span class="guimenu">Manage Cloud
   Settings</span> button. Incorrect information will be shown in red.
  </p><p>
   Below is the list of what can be changed within the Install UI, followed
   by a list of customizations that can only be changed by directly editing the
   files on the <span class="guimenu">Review Configuration Files</span> page. Anything
   changed directly in the files themselves during the Install UI process
   will be overwritten by values you have entered with the Install UI.
  </p><p>
   Changes to the following items can be made:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     servers (including SLES installation configuration)
    </p></li><li class="listitem "><p>
     networks
    </p></li><li class="listitem "><p>
     disk models
    </p></li><li class="listitem "><p>
     interface models
    </p></li><li class="listitem "><p>
     NIC mappings
    </p></li><li class="listitem "><p>
     NTP servers
    </p></li><li class="listitem "><p>
     name servers
    </p></li><li class="listitem "><p>
     tags in network groups
    </p></li></ul></div><p>
   Changes to the following items can only be made by manually editing the
   associated <code class="filename">.yml</code> files on the <span class="guimenu">Review
   Configuration</span> page of the Install UI:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     server groups
    </p></li><li class="listitem "><p>
     server roles
    </p></li><li class="listitem "><p>
     network groups
    </p></li><li class="listitem "><p>
     firewall rules
    </p></li><li class="listitem "><p>
     DNS, SMTP, firewall settings (<code class="filename">cloudConfig.yml</code>)
    </p></li><li class="listitem "><p>
     control planes
    </p></li></ul></div><div id="idm139651565115744" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    Directly changing files may cause the configuration to fail validation.
    During the process of installing with the Install UI, any changes should
    be made with the tools provided within the Install UI.
   </p></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_server_summary.png"><img src="images/installer_ui_server_summary.png" width="" /></a></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_edit_server.png"><img src="images/installer_ui_edit_server.png" width="" /></a></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_manage_cloud_settings.png"><img src="images/installer_ui_manage_cloud_settings.png" width="" /></a></div></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="idm139651565102816"><span class="name">Review Configuration Files</span><a title="Permalink" class="permalink" href="#idm139651565102816">#</a></h5></div><p>
   Advanced editing of the cloud configuration can be done on the
   <code class="literal">Review Configuration Files</code> page. Individual
   <code class="filename">.yml</code> and <code class="filename">.j2</code> files can be edited
   directly with the embedded editor in the <span class="guimenu">Model</span> and
   <span class="guimenu">Templates and Services</span> tabs. The
   <span class="guimenu">Deployment</span> tab contains the items <span class="guimenu">Wipe Data
   Disks</span>, <span class="guimenu">Encryption Key</span> and <span class="guimenu">Verbosity
   Level</span>.
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_review_configuration.png"><img src="images/installer_ui_review_configuration.png" width="" /></a></div></div><div id="idm139651565093440" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    If you are using an SSH tunnel to connect to the Install UI, you will
    need to make an extra modification here to allow SSH connections through
    the firewall:
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      While on the Review Configuration Files page, click on the
      <span class="guimenu">Model</span> tab.
     </p></li><li class="step "><p>
      Click on <span class="guimenu">Firewall Rules</span>.
     </p></li><li class="step "><p>
      Uncomment the <code class="literal">SSH</code> section (remove the
      <code class="literal">#</code> at the beginning of the line for the <code class="literal">-
      name: SSH</code> section).
     </p></li><li class="step "><p>
      If you do not have such a <code class="literal">- name: SSH</code> section,
      manually add the following under the <code class="literal">firewall-rules:</code>
      section:
     </p><div class="verbatim-wrap"><pre class="screen">name: SSH
network-groups:
- MANAGEMENT
rules:
- type: allow
  remote-ip-prefix: 0.0.0.0/0
  port-range-min: 22
  port-range-max: 22
  protocol: tcp</pre></div></li></ol></div></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_review_configuration_edit_yml.png"><img src="images/installer_ui_review_configuration_edit_yml.png" width="" /></a></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_review_configuration_edit_services.png"><img src="images/installer_ui_review_configuration_edit_services.png" width="" /></a></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_review_configuration_deployment.png"><img src="images/installer_ui_review_configuration_deployment.png" width="" /></a></div></div><div id="idm139651565072304" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   Manual edits to your configuration files outside of the Install UI may not
   be reflected in the Install UI. If you make changes to any files directly,
   refresh the browser to make sure changes are seen by the Install UI.
  </p></div><p>
   Before performing the deployment, the configuration must be validated by
   clicking the <span class="guimenu">Validate</span> button below the list of
   configuration files on the <span class="guimenu">Model</span> tab. This ensures the
   configuration will be successful <span class="bold"><strong>before</strong></span> the
   actual configuration process runs and possibly fails. The
   <span class="guimenu">Validate</span> button also commits any changes. If there are
   issues with the validation, the configuration processor will provide
   detailed information about the causes. When validation completes
   successfully, a message will be displayed that the model is valid. If either
   validation or commit fail, the <span class="guimenu">Next</span> button is disabled.
  </p><p>
   Clicking the <span class="guimenu">Deploy</span> button starts the actual deployment
   process.
  </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="idm139651565066800"><span class="name">Cloud Deployment in Progress</span><a title="Permalink" class="permalink" href="#idm139651565066800">#</a></h5></div><p>
   General progress steps are shown on the left. Detailed activity is shown on
   the right.
  </p><p>
   To start the deployment process, the Install UI runs scripts and playbooks
   based on the actual final configuration. Completed operations are green,
   black means in process, gray items are not started yet.
  </p><p>
   The log stream on the right shows finished states. If there are any
   failures, the log stream will show the errors and the
   <span class="guimenu">Next</span> button will be disabled. The <span class="guimenu">Back</span>
   and <span class="guimenu">Next</span> buttons are disabled during the deployment
   process.
  </p><p>
   The log files in <code class="filename">~/ardana/.ansible/ansible.log</code> and
   <code class="filename">/var/cache/ardana_installer/</code> have debugging
   information.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <code class="filename">/var/cache/ardana_installer/log/ardana-service/ardana-service.log</code>
     is created and used during the deployment step.
    </p></li><li class="listitem "><p>
     Each of the time-stamped files in
     <code class="filename">/var/cache/ardana_installer/log/ardana-service/logs/*.log</code>
     shows the output of a single Ansible playbook run invoked during the UI
     installation process and the log output for each of those runs.
    </p></li><li class="listitem "><p>
     The <code class="filename">~/ardana/.ansible/ansible.log</code> file is the output
     of all Ansible playbook runs. This includes the logs from
     <code class="filename">/var/cache/ardana_installer/log/ardana-service/logs/*.log</code>.
    </p></li></ul></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_deploy_inprogress.png"><img src="images/installer_ui_deploy_inprogress.png" width="" /></a></div></div><p>
   When the deployment process is complete, all items on the left will be
   green. Some deployments will not include all steps shown if they don't apply
   to the selected input model. In such a situation, those unneeded steps will
   remain gray.
  </p><p>
   The <span class="guimenu">Next</span> button will be enabled when deployment is
   successful.
  </p><p>
   Clicking <span class="guimenu">Next</span> will display the <code class="literal">Cloud Deployment
   Successful</code> page with information about the deployment, including
   the chosen input model and links to cloud management tools.
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/installer_ui_deploy_successful.png"><img src="images/installer_ui_deploy_successful.png" width="" /></a></div></div><p>
   After installation is complete, shutdown the Install UI by logging into
   the Cloud Lifecycle Manager and running the following commands:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost installui-stop.yml</pre></div><p>
   After deployment, continue to <a class="xref" href="#cloud_verification" title="Chapter 22. Cloud Verification">Chapter 22, <em>Cloud Verification</em></a> and
   <a class="xref" href="#postinstall_checklist" title="Chapter 28. Other Common Post-Installation Tasks">Chapter 28, <em>Other Common Post-Installation Tasks</em></a>.
  </p><p>
   To understand cloud configuration more thoroughly and to learn how to make
   changes later, see:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 6 “Input Model”</span>
    </p></li><li class="listitem "><p>
     <a class="xref" href="#using_git" title="Chapter 12. Using Git for Configuration Management">Chapter 12, <em>Using Git for Configuration Management</em></a>
    </p></li></ul></div></div></div><div class="chapter " id="DesignateInstallOverview"><div class="titlepage"><div><div><h2 class="title"><span class="number">10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DNS Service Installation Overview</span> <a title="Permalink" class="permalink" href="#DesignateInstallOverview">#</a></h2></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#DesignateBIND"><span class="number">10.1 </span><span class="name">Installing the DNS Service with BIND</span></a></span></dt><dt><span class="section"><a href="#DesignatePowerDNS"><span class="number">10.2 </span><span class="name">Install the DNS Service with PowerDNS</span></a></span></dt><dt><span class="section"><a href="#DesignateInfoBlox"><span class="number">10.3 </span><span class="name">Installing the DNS Service with InfoBlox</span></a></span></dt><dt><span class="section"><a href="#DNS_NS"><span class="number">10.4 </span><span class="name">Configure DNS Domain and NS Records</span></a></span></dt></dl></div></div><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> DNS Service supports several different backends for domain name
  service. The choice of backend must be included in the deployment model
  before the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> install is completed.
 </p><div id="idm139651565031856" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
   By default any user in the project is allowed to manage a DNS domain. This
   can be changed by updating the Policy.json file for Designate.
  </p></div><p>
  The backends that are available within the DNS Service are separated into two
  categories, self-contained and external.
 </p><div class="table" id="DNSBackendTable"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 10.1: </span><span class="name">DNS Backends </span><a title="Permalink" class="permalink" href="#DNSBackendTable">#</a></h6></div><div class="table-contents"><table summary="DNS Backends" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /></colgroup><thead><tr><th>Category</th><th>Backend</th><th>Description</th><th>Recommended For</th></tr></thead><tbody><tr><td>Self-contained</td><td>PowerDNS 3.4.1, BIND 9.9.5</td><td>
      All components necessary will be installed and configured as part of
      the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> install.
     </td><td>
      POCs and customers who wish to keep cloud and traditional DNS separated.
     </td></tr><tr><td>External</td><td>InfoBlox</td><td>
      The authoritative DNS server itself is external to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. Management
      and configuration is out of scope for the Cloud Lifecycle Manager but remains
      the responsibility of the customer.
     </td><td>
      Customers who wish to integrate with their existing DNS infrastructure.
     </td></tr></tbody></table></div></div><div class="sect1" id="DesignateBIND"><div class="titlepage"><div><div><h2 class="title" id="DesignateBIND"><span class="number">10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing the DNS Service with BIND</span> <a title="Permalink" class="permalink" href="#DesignateBIND">#</a></h2></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> DNS Service defaults to the BIND back-end if another back-end is
   not configured for domain name service. BIND will be deployed to one or
   more control planes clusters. The following configuration example shows how
   the BIND service is installed.
 </p><div class="sect2" id="sec.bind.configure-back-end"><div class="titlepage"><div><div><h3 class="title" id="sec.bind.configure-back-end"><span class="number">10.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Back-end</span> <a title="Permalink" class="permalink" href="#sec.bind.configure-back-end">#</a></h3></div></div></div><p>
   Ensure the DNS Service components and the BIND component have been placed
   on a cluster. BIND can be placed on a cluster separate from the other DNS
   service components.
  </p><div class="verbatim-wrap"><pre class="screen">control-planes:
          - name: control-plane-1
          region-name: region1

          clusters:
          - name: cluster1
          service-components:
          - lifecycle-manager
          - mariadb
          - ip-cluster
          - apache2
          - ...
          - designate-api
          - designate-central
          - designate-pool-manager
          - designate-zone-manager
          - designate-mdns
          - designate-client
          - bind</pre></div><p>
   <span class="bold"><strong>Updating the Input Model</strong></span>
  </p><p>
   When the back-end is configured, add <code class="literal">bind-ext</code> to the file
   <code class="filename">network_groups.yml</code>.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Edit
     <code class="filename">~/openstack/my_cloud/definition/data/network_groups.yml</code>
     to add <code class="literal">bind-ext</code> to component-endpoints.
    </p><div class="verbatim-wrap"><pre class="screen">name: EXTERNAL-API
hostname-suffix: extapi
component-endpoints:
- bind-ext</pre></div></li><li class="step "><p>
     Save the file.
    </p></li></ol></div></div></div></div><div class="sect1" id="DesignatePowerDNS"><div class="titlepage"><div><div><h2 class="title" id="DesignatePowerDNS"><span class="number">10.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Install the DNS Service with PowerDNS</span> <a title="Permalink" class="permalink" href="#DesignatePowerDNS">#</a></h2></div></div></div><div class="sect2" id="idm139651564996496"><div class="titlepage"><div><div><h3 class="title" id="idm139651564996496"><span class="number">10.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing DNS Service with PowerDNS</span> <a title="Permalink" class="permalink" href="#idm139651564996496">#</a></h3></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> DNS Service and <span class="bold"><strong>PowerDNS</strong></span> can be
   installed together instead of the default
   <span class="bold"><strong>BIND</strong></span> backend. PowerDNS will be deployed to
   one or more control planes clusters. The following configuration example
   shows how the PowerDNS service is installed.
  </p></div><div class="sect2" id="idm139651564991936"><div class="titlepage"><div><div><h3 class="title" id="idm139651564991936"><span class="number">10.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure the Backend</span> <a title="Permalink" class="permalink" href="#idm139651564991936">#</a></h3></div></div></div><p>
   To configure the backend for PowerDNS, follow these steps.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Ensure the DNS Service components and the PowerDNS component have been
     placed on a cluster. PowerDNS may be placed on a separate cluster to the
     other DNS Service components. Ensure the default
     <span class="bold"><strong>bind</strong></span> component has been removed.
    </p><div class="verbatim-wrap"><pre class="screen">control-planes:
          - name: control-plane-1
          region-name: region1

          clusters:
          - name: cluster1
          service-components:
          - lifecycle-manager
          - mariadb
          - ip-cluster
          - apache2
          - ...
          - designate-api
          - designate-central
          - designate-pool-manager
          - designate-zone-manager
          - designate-mdns
          - designate-client
          - powerdns</pre></div></li><li class="step "><p>
     Edit the
     <code class="filename">~/openstack/my_cloud/definitions/data/network_groups.yml</code>
     file to include the powerdns-ext.
    </p><div class="verbatim-wrap"><pre class="screen">- name: EXTERNAL-API
hostname-suffix: extapi
component-endpoints:
 - powerdns-ext
load-balancers:
 - provider: ip-cluster</pre></div></li><li class="step "><p>
     Edit the
     <code class="filename">~/openstack/my_cloud/definitions/data/firewall_rules.yml</code>
     to allow UDP/TCP access.
    </p><div class="verbatim-wrap"><pre class="screen">	    - name: DNSudp
      # network-groups lists the network group names that the rules apply to
      network-groups:
      - EXTERNAL-API
      rules:
      - type: allow
        # range of remote addresses in CIDR format that this rule applies to
        remote-ip-prefix:  0.0.0.0/0
        port-range-min: 53
        port-range-max: 53
        protocol: udp

    - name: DNStcp
      # network-groups lists the network group names that the rules apply to
      network-groups:
      - EXTERNAL-API
      rules:
      - type: allow
        # range of remote addresses in CIDR format that this rule applies to
        remote-ip-prefix:  0.0.0.0/0
        port-range-min: 53
        port-range-max: 53
        protocol: tcp</pre></div></li></ol></div></div><p>
   Please see <span class="intraxref">Book “Operations Guide”, Chapter 9 “Managing Networking”, Section 9.2 “DNS Service Overview”, Section 9.2.2 “Designate Initial Configuration”</span> for post-installation
   DNS Service configuration.
  </p></div></div><div class="sect1" id="DesignateInfoBlox"><div class="titlepage"><div><div><h2 class="title" id="DesignateInfoBlox"><span class="number">10.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing the DNS Service with InfoBlox</span> <a title="Permalink" class="permalink" href="#DesignateInfoBlox">#</a></h2></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> DNS service can be installed with the InfoBlox back-end instead of
  the default PowerDNS back-end. To use InfoBlox as the back-end, all
  prerequisites must be satisfied and the configuration of
  InfoBlox must be complete.
 </p><div id="idm139651564976704" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   No DNS server will be deployed onto the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> nodes. Instead, zones will
   be hosted on the InfoBlox servers.
  </p></div><div id="idm139651564974160" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   Before you enable and use InfoBlox in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, review the
   <em class="citetitle ">InfoBlox OpenStack Drive Deployment Guide</em> at
   <a class="link" href="https://www.infoblox.com/wp-content/uploads/infoblox-deployment-guide-infoblox-openstack-driver.pdf" target="_blank">https://www.infoblox.com/wp-content/uploads/infoblox-deployment-guide-infoblox-openstack-driver.pdf</a>.
   Address any questions with your InfoBlox support contact.
  </p></div><div class="sect2" id="sec.infoblox.prerequisite"><div class="titlepage"><div><div><h3 class="title" id="sec.infoblox.prerequisite"><span class="number">10.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#sec.infoblox.prerequisite">#</a></h3></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     An existing InfoBlox system deployed within your organization.
    </p></li><li class="listitem "><p>
     IPs and other credentials required to access the InfoBlox WS API.
    </p></li><li class="listitem "><p>
     Network connectivity to and from the cluster containing Designate and the
     InfoBlox WS API.
    </p></li></ol></div><p>
   This information is available from your InfoBlox system administrator.
  </p></div><div class="sect2" id="sec.infoblox.configure-back-end"><div class="titlepage"><div><div><h3 class="title" id="sec.infoblox.configure-back-end"><span class="number">10.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the Back-end</span> <a title="Permalink" class="permalink" href="#sec.infoblox.configure-back-end">#</a></h3></div></div></div><p>
   If not already present, DNS service components must be placed on a cluster.
   Additionally, ensure the default <code class="literal">bind</code>
   component has been removed, and <code class="literal">powerdns</code> has
   not been added. Replace the <code class="literal">designate-mdns</code> component with
   the <code class="literal">designate-mdns-external</code> component.
  </p><div class="verbatim-wrap"><pre class="screen">control-planes:
          - name: control-plane-1
            region-name: region1
              - lifecycle-manager-target

          clusters:
            - name: cluster1
              service-components:
              - lifecycle-manager
              - mariadb
              - ip-cluster
              - apache2
              - ...
              - designate-api
              - designate-central
              - designate-pool-manager
              - designate-zone-manager
              - designate-mdns-external
              - designate-client</pre></div><p>
   You will need to provide DNS service information details on your InfoBlox
   deployment. Open the Designate pool-manager configuration template:
  </p><div class="verbatim-wrap"><pre class="screen">$ cd ~/openstack/my_cloud
$ nano config/designate/pool-manager.conf.j2</pre></div><p>
   In the <code class="filename">config/designate/pool-manager.conf.j2</code>, find the
   following code block:
  </p><div class="verbatim-wrap"><pre class="screen">  nameservers:
    - host: &lt;infoblox-server-ip&gt;
      port: &lt;infoblox-port-number&gt;
   ...
     also_notifies:
    - host: &lt;infoblox-server-ip&gt;
      port: &lt;infoblox-port-number&gt;</pre></div><p>
   Make the following changes:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Uncomment the block for <code class="literal">infoblox</code> and
     <code class="literal">also_notifies</code>. In Jinja2, this means
     replacing the <code class="literal">{#</code> and <code class="literal">#}</code> markers with
     <code class="literal">{{</code> and <code class="literal">}}</code>.
    </p></li><li class="step "><p>
     Fill in the API URL, user name, password, and all remaining fields.
    </p></li><li class="step "><p>
     Save the file.
    </p></li></ol></div></div><p>
   Once complete, the block should look like this:
  </p><div class="verbatim-wrap"><pre class="screen">    - type: infoblox
      description: infoblox Cluster

      masters:
{% if DES_PMG.consumes_DES_MDN_EXT is defined %}
{% for mdn_member in DES_PMG.consumes_DES_MDN_EXT.members.private %}
        - host: {{ mdn_member.ip_address }}
          port: {{ mdn_member.port }}
{% endfor %}
{% endif %}

      options:
        wapi_url: https://&lt;infoblox-server-ip&gt;/wapi/v2.2.2/
        username: ardana-designate
        password: MySecretPassword
        ns_group: designate
        sslverify: False
        dns_view: default
        network_view: default</pre></div><p>
   You will need to inspect and commit the changes before proceeding with the
   deployment:
  </p><div class="verbatim-wrap"><pre class="screen">$git diff ardana/ansible/roles/designate-pool-manager/templates/pools.yaml.j2
index 291c6c9..b7fb39c 100644
--- a/ardana/ansible/roles/designate-pool-manager/templates/pools.yaml.j2
+++ b/ardana/ansible/roles/designate-pool-manager/templates/pools.yaml.j2
@@ -28,6 +28,8 @@
       priority: 2

   nameservers:
+    - host: &lt;infoblox-server-ip&gt;
+      port: &lt;infoblox-port-number&gt;
 {% if DES_PMG.consumes_FND_PDN is defined %}
 {% for pdn_member in DES_PMG.consumes_FND_PDN.members.private %}
     - host: {{ pdn_member.ip_address }}
@@ -40,7 +42,9 @@
       port: {{ bnd_member.port }}
 {% endfor %}
 {% endif %}
-#  also_notifies:
+  also_notifies:
+    - host: &lt;infoblox-server-ip&gt;
+      port: &lt;infoblox-port-number&gt;
   targets:
 {% if DES_PMG.consumes_FND_PDN is defined %}
     - type: powerdns
@@ -89,27 +93,27 @@
 {% endfor %}
 {% endif %}

-#
-#    - type: infoblox
-#      description: infoblox Cluster
-#
-#      masters:
+
+    - type: infoblox
+      description: infoblox Cluster
+
+      masters:
 {% if DES_PMG.consumes_DES_MDN_EXT is defined %}
 {% for mdn_member in DES_PMG.consumes_DES_MDN_EXT.members.private %}
-#        - host: {{ mdn_member.ip_address }}
-#          port: {{ mdn_member.port }}
+        - host: {{ mdn_member.ip_address }}
+          port: {{ mdn_member.port }}
 {% endfor %}
 {% endif %}
-#
-#      options:
-#        wapi_url: https://127.0.0.1/wapi/v2.2.2/
-#        username: admin
-#        password: infoblox
-#        ns_group: designate
-#        sslverify: False
-#        dns_view: default
-#        network_view: default
-#
+
+      options:
+        wapi_url: https://127.0.0.1/wapi/v2.2.2/
+        username: admin
+        password: infoblox
+        ns_group: designate
+        sslverify: False
+        dns_view: default</pre></div><p>
   <span class="bold"><strong>SSL and CA Certificate</strong></span>
  </p><p>
   To enable SSL Verify, edit the following file:
  </p><div class="verbatim-wrap"><pre class="screen">$ cd ~/openstack/my_cloud
$ nano config/designate/pools.yaml.j2</pre></div><p>
   In the <code class="literal">infoblox</code> section, set
   <code class="literal">sslverify</code> to <code class="literal">True</code>:
  </p><div class="verbatim-wrap"><pre class="screen">sslverify: True</pre></div><p>
   To generate a CA certificate for InfoBlox, follow these steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the InfoBlox user interface.
    </p></li><li class="step "><p>
     Generate a self-signed certificate by selecting:
     <span class="guimenu">System</span> › <span class="guimenu">Certificate</span> › <span class="guimenu">HTTP Cert</span> › <span class="guimenu">Generate Self Signed Certificate</span>
    </p></li><li class="step "><p>
     Provide the host name as the InfoBlox server IP.
    </p></li><li class="step "><p>
     Reload the InfoBlox user interface. The certificate will be loaded and can
     be verified through the browser.
    </p></li><li class="step "><p>
     To download the certificate, select:
     <span class="guimenu">System</span> › <span class="guimenu">Certificate</span> › <span class="guimenu">HTTP Cert</span> › <span class="guimenu">Download the Certificate</span>.
    </p></li><li class="step "><p>
     Copy the certificate file to
     <code class="filename">~/openstack/my_cloud/config/tls/cacerts/</code>.
    </p></li></ol></div></div><p>
   Commit the changes to Git:
  </p><div class="verbatim-wrap"><pre class="screen">$ git commit -a -m "Configure Designate InfoBlox back-end"</pre></div></div></div><div class="sect1" id="DNS_NS"><div class="titlepage"><div><div><h2 class="title" id="DNS_NS"><span class="number">10.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure DNS Domain and NS Records</span> <a title="Permalink" class="permalink" href="#DNS_NS">#</a></h2></div></div></div><p>
  To configure the default DNS domain and Name Server records for the default
  pool, follow these steps.
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Ensure that <code class="literal">designate_config.yml</code> file is present in the
    <code class="literal">~/openstack/my_cloud/definition/data/designate</code> folder. If
    the file or folder is not present, create the folder and copy
    <code class="literal">designate_config.yml</code> file from one of the example input
    models (for example,
    <code class="filename">~/openstack/examples/entry-scale-kvm/data/designate/designate_config.yml</code>).
   </p></li><li class="step "><p>
    Modify the <span class="bold"><strong>dns_domain</strong></span> and/or
    <span class="bold"><strong>ns_records</strong></span> entries in the
    <code class="filename">designate_config.yml</code> file.
   </p><div class="verbatim-wrap"><pre class="screen">data:
dns_domain: example.org.
ns_records:
    hostname: ns1.example.org.
    priority: 1
    hostname: ns2.example.org.
    priority: 2</pre></div></li><li class="step "><p>
    Edit your input model's <code class="filename">control_plane.yml</code> file to
    include <span class="bold"><strong>DESIGNATE-CONFIG-CP1</strong></span> in
    <span class="bold"><strong>configuration-data</strong></span> section.
   </p><div class="verbatim-wrap"><pre class="screen">control-planes:
   - name: control-plane-1
     region-name: region1
     lifecycle-manager-target
     configuration-data:
        - DESIGNATE-CONFIG-CP1
        - NEUTRON-CONFIG-CP1</pre></div></li><li class="step "><p>
    Continue your cloud deployment by reviewing and committing your changes.
   </p><div class="verbatim-wrap"><pre class="screen">$ git add ~/openstack/my_cloud/definition/data/designate/designate_config.yml
$ git commit -m "Adding DNS Domain and NS Records"</pre></div></li></ol></div></div><div id="idm139651564916640" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   In an entry-scale model (<span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”, Section 10.3 “KVM Examples”, Section 10.3.1 “Entry-Scale Cloud”</span>),
   you will have 3 ns_records since the DNS service runs on all three
   control planes.
  </p><p>
   In a mid-scale model (<span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”, Section 10.3 “KVM Examples”, Section 10.3.3 “Single-Region Mid-Size Model”</span>) or
   dedicated metering, monitoring and logging model
   (<span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”, Section 10.3 “KVM Examples”, Section 10.3.2 “Entry Scale Cloud with Metering and Monitoring Services”</span>), the above example would be
   correct since there are only two controller nodes.
  </p></div></div></div><div class="chapter " id="MagnumOverview"><div class="titlepage"><div><div><h2 class="title"><span class="number">11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Magnum Overview</span> <a title="Permalink" class="permalink" href="#MagnumOverview">#</a></h2></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#MagnumArchitecture"><span class="number">11.1 </span><span class="name">Magnum Architecture</span></a></span></dt><dt><span class="section"><a href="#MagnumInstall"><span class="number">11.2 </span><span class="name">Install the Magnum Service</span></a></span></dt><dt><span class="section"><a href="#MagnumIntegrateDNS"><span class="number">11.3 </span><span class="name">Integrate Magnum with the DNS Service</span></a></span></dt></dl></div></div><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Magnum Service provides container orchestration engines such as
  Docker Swarm, Kubernetes, and Apache Mesos available as first class
  resources. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Magnum uses Heat to orchestrate an OS image which
  contains Docker and Kubernetes and runs that image in either virtual machines
  or bare metal in a cluster configuration.
 </p><div class="sect1" id="MagnumArchitecture"><div class="titlepage"><div><div><h2 class="title" id="MagnumArchitecture"><span class="number">11.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Magnum Architecture</span> <a title="Permalink" class="permalink" href="#MagnumArchitecture">#</a></h2></div></div></div><p>
  As an OpenStack API service, Magnum provides Container as a Service (CaaS)
  functionality. Magnum is capable of working with container orchestration
  engines (COE) such as Kubernetes, Docker Swarm, and Apache Mesos. Some
  operations work with a User CRUD (Create, Read, Update, Delete) filter.
 </p><p>
  <span class="bold"><strong>Components</strong></span>
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <span class="bold"><strong>Magnum API</strong></span>: RESTful API for cluster and
    cluster template operations.
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>Magnum Conductor</strong></span>: Performs operations on
    clusters requested by Magnum API in an asynchronous manner.
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>Magnum CLI</strong></span>: Command-line interface to the
    Magnum API.
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>Etcd (planned, currently using public
    service)</strong></span>: Remote key/value storage for distributed cluster
    bootstrap and discovery.
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>Kubemaster (in case of Kubernetes COE)</strong></span>:
    One or more VM(s) or baremetal server(s), representing a control plane for
    Kubernetes cluster.
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>Kubeminion (in case of Kubernetes COE)</strong></span>:
    One or more VM(s) or baremetal server(s), representing a workload node for
    Kubernetes cluster.
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>Octavia VM aka Amphora (in case of Kubernetes COE
    with enabled load balancer functionality)</strong></span>: One or more VM(s),
    created by LBaaS v2, performing request load balancing for Kubemasters.
   </p></li></ul></div><div class="table" id="table_ebc_x5v_jz"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 11.1: </span><span class="name">Data </span><a title="Permalink" class="permalink" href="#table_ebc_x5v_jz">#</a></h6></div><div class="table-contents"><table summary="Data" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /><col class="c6" /></colgroup><thead><tr><th>Data Name</th><th>Confidentiality</th><th>Integrity</th><th>Availability</th><th>Backup?</th><th>Description</th></tr></thead><tbody><tr><td>Session Tokens</td><td>Confidential</td><td>High</td><td>Medium</td><td>No</td><td>Session tokens not stored.</td></tr><tr><td>System Request</td><td>Confidential</td><td>High</td><td>Medium</td><td>No</td><td>Data in motion or in MQ not stored.</td></tr><tr><td>MariaDB Database "Magnum"</td><td>Confidential</td><td>High</td><td>High</td><td>Yes</td><td>Contains user preferences. Backed up to Swift daily.</td></tr><tr><td>etcd data</td><td>Confidential</td><td>High</td><td>Low</td><td>No</td><td>Kubemaster IPs and cluster info. Only used during cluster bootstrap.</td></tr></tbody></table></div></div><div class="figure" id="magnum_service_arch_diagram"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/media-magnum-magnum_service_arch_diagram.png"><img src="images/media-magnum-magnum_service_arch_diagram.png" width="" alt="Service Architecture Diagram for Kubernetes" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 11.1: </span><span class="name">Service Architecture Diagram for Kubernetes </span><a title="Permalink" class="permalink" href="#magnum_service_arch_diagram">#</a></h6></div></div><div class="table" id="table_fst_gxv_jz"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 11.2: </span><span class="name">Interfaces </span><a title="Permalink" class="permalink" href="#table_fst_gxv_jz">#</a></h6></div><div class="table-contents"><table summary="Interfaces" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /><col class="c5" /></colgroup><thead><tr><th>Interface</th><th>Network</th><th>Request</th><th>Response</th><th>Operation Description</th></tr></thead><tbody><tr><td>1</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> External-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Manage clusters
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> User
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Keystone token
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Manage objects that
       belong to current project
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Magnum API
      </p>
     </td><td>
      <p>
       Operation status with or without data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       CRUD operations on cluster templates and clusters
      </p>
     </td></tr><tr><td>2a</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> Internal-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> AMQP over HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Enqueue messages
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Magnum API
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> RabbitMQ username,
       password
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> RabbitMQ queue
       read/write operations
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> RabbitMQ
      </p>
     </td><td>
      <p>
       Operation status
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       Notifications issued when cluster CRUD operations requested
      </p>
     </td></tr><tr><td>2b</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> Internal-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> AMQP over HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Read queued messages
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Magnum Conductor
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> RabbitMQ username,
       password
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> RabbitMQ queue
       read/write operations
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> RabbitMQ
      </p>
     </td><td>
      <p>
       Operation status
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       Notifications issued when cluster CRUD operations requested
      </p>
     </td></tr><tr><td>3</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> Internal-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> MariaDB over HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Persist data in MariaDB
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Magnum Conductor
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> MariaDB username,
       password
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Magnum database
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> MariaDB
      </p>
     </td><td>
      <p>
       Operation status with or without data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       Persist cluster/cluster template data, read persisted data
      </p>
     </td></tr><tr><td>4</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> Internal-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Create per-cluster user in
       dedicated domain, no role assignments initially
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Magnum Conductor
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Trustee domain admin
       username, password
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Manage users in
       dedicated Magnum domain
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Keystone
      </p>
     </td><td>
      <p>
       Operation status with or without data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       Magnum generates user record in a dedicated Keystone domain for each
       cluster
      </p>
     </td></tr><tr><td>5</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> Internal-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Create per-cluster user stack
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Magnum Conductor
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Keystone token
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Limited to scope of
       authorized user
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Heat
      </p>
     </td><td>
      <p>
       Operation status with or without data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       Magnum creates Heat stack for each cluster
      </p>
     </td></tr><tr><td>6</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> External Network
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Bootstrap a cluster in public
       discovery <a class="link" href="https://discovery.etcd.io/" target="_blank">https://discovery.etcd.io/</a>
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Magnum Conductor
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Unguessable URL over
       HTTPS. URL is only available to software processes needing it.
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Read and update
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Public discovery service
      </p>
     </td><td>
      <p>
       Cluster discovery URL
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       Create key/value registry of specified size in public storage. This is
       used to stand up a cluster of kubernetes master nodes (refer to
       interface call #12).
      </p>
     </td></tr><tr><td>7</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> Internal-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Create Cinder volumes
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Heat Engine
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Keystone token
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Limited to scope of
       authorized user
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Cinder API
      </p>
     </td><td>
      <p>
       Operation status with or without data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       Heat creates Cinder volumes as part of stack.
      </p>
     </td></tr><tr><td>8</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> Internal-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Create networks, routers, load
       balancers
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Heat Engine
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Keystone token
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Limited to scope of
       authorized user
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Neutron API
      </p>
     </td><td>
      <p>
       Operation status with or without data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       Heat creates networks, routers, load balancers as part of the stack.
      </p>
     </td></tr><tr><td>9</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> Internal-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Create Nova VMs, attach
       volumes
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Heat Engine
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Keystone token
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Limited to scope of
       authorized user
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Nova API
      </p>
     </td><td>
      <p>
       Operation status with or without data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       Heat creates Nova VMs as part of the stack.
      </p>
     </td></tr><tr><td>10</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> Internal-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Read pre-configured Glance
       image
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Nova
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Keystone token
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Limited to scope of
       authorized user
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Glance API
      </p>
     </td><td>
      <p>
       Operation status with or without data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       Nova uses pre-configured image in Glance to bootstrap VMs.
      </p>
     </td></tr><tr><td>11a</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> External-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Heat notification
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Cluster member (VM or Ironic
       node)
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Keystone token
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Limited to scope of
       authorized user
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Heat API
      </p>
     </td><td>
      <p>
       Operation status with or without data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       Heat uses OS::Heat::WaitCondition resource. VM is expected to call Heat
       notification URL upon completion of certain bootstrap operation.
      </p>
     </td></tr><tr><td>11b</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> External-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Heat notification
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Cluster member (VM or Ironic
       node)
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Keystone token
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Limited to scope of
       authorized user
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Heat API
      </p>
     </td><td>
      <p>
       Operation status with or without data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       Heat uses OS::Heat::WaitCondition resource. VM is expected to call Heat
       notification URL upon completion of certain bootstrap operation.
      </p>
     </td></tr><tr><td>12</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> External-API
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Update cluster member state in
       a public registry at https://discovery.etcd.io
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Cluster member (VM or Ironic
       node)
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Unguessable URL over HTTPS
       only available to software processes needing it.
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Read and update
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Public discovery service
      </p>
     </td><td>
      <p>
       Operation status
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       Update key/value pair in a registry created by interface call #6.
      </p>
     </td></tr><tr><td>13a</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> VxLAN encapsulated private
       network on the Guest network
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Various communications inside
       Kubernetes cluster
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Cluster member (VM or Ironic
       node)
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Cluster member (VM or Ironic
       node)
      </p>
     </td><td>
      <p>
       Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       Various calls performed to build Kubernetes clusters, deploy
       applications and put workload
      </p>
     </td></tr><tr><td>13b</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> VxLAN encapsulated private
       network on the Guest network
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Various communications inside
       Kubernetes cluster
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Cluster member (VM or Ironic
       node)
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Cluster member (VM or Ironic
       node)
      </p>
     </td><td>
      <p>
       Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       Various calls performed to build Kubernetes clusters, deploy
       applications and put workload
      </p>
     </td></tr><tr><td>14</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> Guest/External
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Download container images
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Cluster member (VM or Ironic
       node)
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> None
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> None
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> External
      </p>
     </td><td>
      <p>
       Container image data
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> TLS certificate
      </p>
     </td><td>
      <p>
       Kubernetes makes calls to external repositories to download pre-packed
       container images
      </p>
     </td></tr><tr><td>15a</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> External/EXT_VM (Floating IP)
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Octavia load balancer
      </p>
     </td><td>
      <p>
       Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Tenant specific
      </p>
     </td><td>
      <p>
       External workload handled by container applications
      </p>
     </td></tr><tr><td>15b</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> Guest
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Cluster member (VM or Ironic
       node)
      </p>
     </td><td>
      <p>
       Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Tenant specific
      </p>
     </td><td>
      <p>
       External workload handled by container applications
      </p>
     </td></tr><tr><td>15c</td><td>
      <p>
       <span class="bold"><strong>Name:</strong></span> External/EXT_VM (Floating IP)
      </p>
      <p>
       <span class="bold"><strong>Protocol:</strong></span> HTTPS
      </p>
     </td><td>
      <p>
       <span class="bold"><strong>Request:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Requester:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Authorization:</strong></span> Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Listener:</strong></span> Cluster member (VM or Ironic
       node)
      </p>
     </td><td>
      <p>
       Tenant specific
      </p>
      <p>
       <span class="bold"><strong>Credentials:</strong></span> Tenant specific
      </p>
     </td><td>
      <p>
       External workload handled by container applications
      </p>
     </td></tr></tbody></table></div></div><p>
  <span class="bold"><strong>Dependencies</strong></span>
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Keystone
   </p></li><li class="listitem "><p>
    RabbitMQ
   </p></li><li class="listitem "><p>
    MariaDB
   </p></li><li class="listitem "><p>
    Heat
   </p></li><li class="listitem "><p>
    Glance
   </p></li><li class="listitem "><p>
    Nova
   </p></li><li class="listitem "><p>
    Cinder
   </p></li><li class="listitem "><p>
    Neutron
   </p></li><li class="listitem "><p>
    Barbican
   </p></li><li class="listitem "><p>
    Swift
   </p></li></ul></div><p>
  <span class="bold"><strong>Implementation</strong></span>
 </p><p>
  Magnum API and Magnum Conductor are run on the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> controllers (or core
  nodes in case of mid-scale deployments).
 </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-networkImages-Mid-Scale-AllNetworks.png"><img src="images/media-networkImages-Mid-Scale-AllNetworks.png" width="" /></a></div></div><div class="table" id="security_groups_table"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 11.3: </span><span class="name">Security Groups </span><a title="Permalink" class="permalink" href="#security_groups_table">#</a></h6></div><div class="table-contents"><table summary="Security Groups" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /><col class="c4" /></colgroup><thead><tr><th>Source CIDR/Security Group</th><th>Port/Range</th><th>Protocol</th><th>Notes</th></tr></thead><tbody><tr><td>Any IP</td><td>22</td><td>SSH</td><td>Tenant Admin access</td></tr><tr><td>Any IP/Kubernetes Security Group</td><td>2379-2380</td><td>HTTPS</td><td>Etcd Traffic</td></tr><tr><td>Any IP/Kubernetes Security Group</td><td>6443</td><td>HTTPS</td><td>kube-apiserver</td></tr><tr><td>Any IP/Kubernetes Security Group</td><td>7080</td><td>HTTPS</td><td>kube-apiserver</td></tr><tr><td>Any IP/Kubernetes Security Group</td><td>8080</td><td>HTTPS</td><td>kube-apiserver</td></tr><tr><td>Any IP/Kubernetes Security Group</td><td>30000-32767</td><td>HTTPS</td><td>kube-apiserver</td></tr><tr><td>Any IP/Kubernetes Security Group</td><td>any</td><td>tenant app specific</td><td>tenant app specific</td></tr></tbody></table></div></div><div class="table" id="network_ports_table"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 11.4: </span><span class="name">Network Ports </span><a title="Permalink" class="permalink" href="#network_ports_table">#</a></h6></div><div class="table-contents"><table summary="Network Ports" border="1"><colgroup><col class="c1" /><col class="c2" /><col class="c3" /></colgroup><thead><tr><th>Port/Range</th><th>Protocol</th><th>Notes</th></tr></thead><tbody><tr><td>22</td><td>SSH</td><td>Admin Access</td></tr><tr><td>9511</td><td>HTTPS</td><td>Magnum API Access</td></tr><tr><td>2379-2380</td><td>HTTPS</td><td>Etcd (planned)</td></tr><tr><td> </td><td> </td><td> </td></tr></tbody></table></div></div><p>
  Summary of controls spanning multiple components and interfaces:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <span class="bold"><strong>Audit</strong></span>: Magnum performs logging. Logs are
    collected by the centralized logging service.
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>Authentication</strong></span>: Authentication via
    Keystone tokens at APIs. Password authentication to MQ and DB using
    specific users with randomly-generated passwords.
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>Authorization</strong></span>: OpenStack provides admin
    and non-admin roles that are indicated in session tokens. Processes run at
    minimum privilege. Processes run as unique user/group definitions
    (magnum/magnum). Appropriate filesystem controls prevent other processes
    from accessing service’s files. Magnum config file is mode 600. Logs
    written using group adm, user magnum, mode 640. IPtables ensure that no
    unneeded ports are open. Security Groups provide authorization controls
    between in-cloud components.
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>Availability</strong></span>: Redundant hosts, clustered
    DB, and fail-over provide high availability.
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>Confidentiality</strong></span>: Network connections over
    TLS. Network separation via VLANs. Data and config files protected via
    filesystem controls. Unencrypted local traffic is bound to localhost.
    Separation of customer traffic on the TUL network via Open Flow (VxLANs).
   </p></li><li class="listitem "><p>
    <span class="bold"><strong>Integrity</strong></span>: Network connections over TLS.
    Network separation via VLANs. DB API integrity protected by SQL Alchemy.
    Data and config files are protected by filesystem controls. Unencrypted
    traffic is bound to localhost.
   </p></li></ul></div></div><div class="sect1" id="MagnumInstall"><div class="titlepage"><div><div><h2 class="title" id="MagnumInstall"><span class="number">11.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Install the Magnum Service</span> <a title="Permalink" class="permalink" href="#MagnumInstall">#</a></h2></div></div></div><p>
  Installing the Magnum Service can be performed as part of a new
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> environment or can be added to an existing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>
  environment. Both installations require container management services,
  running in Magnum cluster VMs with access to specific Openstack API
  endpoints. The following TCP ports need to be open in your firewall to allow
  access from VMs to external (public) <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> endpoints.
 </p><div class="informaltable"><table border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>TCP Port</th><th>Service</th></tr></thead><tbody><tr><td>5000</td><td>Identity</td></tr><tr><td>8004</td><td>Heat</td></tr><tr><td>9511</td><td>Magnum</td></tr></tbody></table></div><p>
  Magnum is dependent on the following OpenStack services.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Keystone
   </p></li><li class="listitem "><p>
    Heat
   </p></li><li class="listitem "><p>
    Nova KVM
   </p></li><li class="listitem "><p>
    Neutron
   </p></li><li class="listitem "><p>
    Glance
   </p></li><li class="listitem "><p>
    Cinder
   </p></li><li class="listitem "><p>
    Swift
   </p></li><li class="listitem "><p>
    Barbican
   </p></li><li class="listitem "><p>
    LBaaS v2 (Octavia) - <span class="emphasis"><em>optional</em></span>
   </p></li></ul></div><div id="idm139651561339472" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
   Magnum relies on the public discovery service
   <span class="emphasis"><em>https://discovery.etcd.io</em></span> during cluster bootstrapping
   and update. This service does not perform authentication checks. Although
   running a cluster cannot be harmed by unauthorized changes in the public
   discovery registry, it can be compromised during a cluster update operation.
   To avoid this, it is recommended that you keep your cluster discovery URL
   (that is,
   <code class="literal">https://discovery.etc.io/<em class="replaceable ">SOME_RANDOM_ID</em></code>)
   secret.
  </p></div><div class="sect2" id="idm139651561336992"><div class="titlepage"><div><div><h3 class="title" id="idm139651561336992"><span class="number">11.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing Magnum as part of new <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> environment</span> <a title="Permalink" class="permalink" href="#idm139651561336992">#</a></h3></div></div></div><p>
   Magnum components are already included in example <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> models based on
   Nova KVM, such as <span class="bold"><strong>entry-scale-kvm</strong></span>,
   <span class="bold"><strong>entry-scale-kvm-mml</strong></span> and
   <span class="bold"><strong>mid-scale</strong></span>. These models contain the Magnum
   dependencies (see above). You can follow generic installation instruction
   for Mid-Scale and Entry-Scale KM model by using this guide:
   <a class="xref" href="#install_kvm" title="Chapter 13. Installing Mid-scale and Entry-scale KVM">Chapter 13, <em>Installing Mid-scale and Entry-scale KVM</em></a>.
  </p><div id="idm139651561328880" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
      If you modify the cloud model to utilize a dedicated Cloud Lifecycle Manager, add
      <code class="literal">magnum-client</code> item to the list of service components
      for the Cloud Lifecycle Manager cluster.
     </p></li><li class="listitem "><p>
      Magnum needs a properly configured external endpoint. While preparing the
      cloud model, ensure that <code class="literal">external-name</code> setting in
      <code class="literal">data/network_groups.yml</code> is set to valid hostname,
      which can be resolved on DNS server, and a valid TLS certificate is
      installed for your external endpoint. For non-production test
      installations, you can omit <code class="literal">external-name</code>. In test
      installations, the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installer will use an IP address as a public
      endpoint hostname, and automatically generate a new certificate, signed
      by the internal CA. Please refer to <a class="xref" href="#tls30" title="Chapter 25. Configuring Transport Layer Security (TLS)">Chapter 25, <em>Configuring Transport Layer Security (TLS)</em></a> for more
      details.
     </p></li><li class="listitem "><p>
      To use LBaaS v2 (Octavia) for container management and container
      applications, follow the additional steps to configure LBaaS v2 in the
      guide.
     </p></li></ol></div></div></div><div class="sect2" id="sec.magnum.exist"><div class="titlepage"><div><div><h3 class="title" id="sec.magnum.exist"><span class="number">11.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding Magnum to an Existing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Environment</span> <a title="Permalink" class="permalink" href="#sec.magnum.exist">#</a></h3></div></div></div><p>
   Adding Magnum to an already deployed <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> installation or during
   an upgrade can be achieved by performing the following steps.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Add items listed below to the list of service components in
     <code class="literal">~/openstack/my_cloud/definition/data/control_plane.yml</code>.
     Add them to clusters which have <code class="literal">server-role</code> set to
     <code class="literal">CONTROLLER-ROLE</code> (entry-scale models) or
     <code class="literal">CORE_ROLE</code> (mid-scale model).
    </p><div class="verbatim-wrap"><pre class="screen">- magnum-api
- magnum-conductor</pre></div></li><li class="step "><p>
     If your environment utilizes a dedicated Cloud Lifecycle Manager, add
     <code class="literal">magnum-client</code> to the list of service components for the
     Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Commit your changes to the local git repository. Run the following
     playbooks as described in <a class="xref" href="#using_git" title="Chapter 12. Using Git for Configuration Management">Chapter 12, <em>Using Git for Configuration Management</em></a> for your
     installation.
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       <code class="literal">config-processor-run.yml</code>
      </p></li><li class="listitem "><p>
       <code class="literal">ready-deployment.yml</code>
      </p></li><li class="listitem "><p>
       <code class="literal">site.yml</code>
      </p></li></ul></div></li><li class="step "><p>
     Ensure that your external endpoint is configured correctly. The current
     public endpoint configuration can be verified by running the following
     commands on the Cloud Lifecycle Manager.
    </p><div class="verbatim-wrap"><pre class="screen">$ source service.osrc
$ openstack endpoint list --interface=public --service=identity
+-----------+---------+--------------+----------+---------+-----------+------------------------+
| ID        | Region  | Service Name | Service  | Enabled | Interface | URL                    |
|           |         |              | Type     |         |           |                        |
+-----------+---------+--------------+----------+---------+-----------+------------------------+
| d83...aa3 | region1 | keystone     | identity | True    | public    | https://10.245.41.168: |
|           |         |              |          |         |           |             5000/v2.0  |
+-----------+---------+--------------+----------+---------+-----------+------------------------+</pre></div><p>
     Ensure that the endpoint URL is using either an IP address, or a valid
     hostname, which can be resolved on the DNS server. If the URL is using an
     invalid hostname (for example, <code class="literal">myardana.test</code>), follow
     the steps in <a class="xref" href="#tls30" title="Chapter 25. Configuring Transport Layer Security (TLS)">Chapter 25, <em>Configuring Transport Layer Security (TLS)</em></a> to configure a valid external
     endpoint. You will need to update the <code class="literal">external-name</code>
     setting in the <code class="literal">data/network_groups.yml</code> to a valid
     hostname, which can be resolved on DNS server, and provide a valid TLS
     certificate for the external endpoint. For non-production test
     installations, you can omit the <code class="literal">external-name</code>. The
     <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installer will use an IP address as public endpoint hostname, and
     automatically generate a new certificate, signed by the internal CA.
     For more information, see <a class="xref" href="#tls30" title="Chapter 25. Configuring Transport Layer Security (TLS)">Chapter 25, <em>Configuring Transport Layer Security (TLS)</em></a>.
    </p></li><li class="step "><p>
     Ensure that LBaaS v2 (Octavia) is correctly configured. For more
     information, see <a class="xref" href="#OctaviaInstall" title="Chapter 27. Configuring Load Balancer as a Service">Chapter 27, <em>Configuring Load Balancer as a Service</em></a>.
    </p></li></ol></div></div><div id="idm139651561293792" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
    By default <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> stores the private key used by Magnum and its
    passphrase in Barbican which provides a secure place to store such
    information. You can change this such that this sensitive information is
    stored on the file system or in the database without encryption. Making
    such a change exposes you to the risk of this information being exposed
    to others. If stored in the database then any database backups, or a
    database breach, could lead to the disclosure of the sensitive
    information. Similarly, if stored unencrypted on the file system this
    information is exposed more broadly than if stored in Barbican.
   </p></div></div></div><div class="sect1" id="MagnumIntegrateDNS"><div class="titlepage"><div><div><h2 class="title" id="MagnumIntegrateDNS"><span class="number">11.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Integrate Magnum with the DNS Service</span> <a title="Permalink" class="permalink" href="#MagnumIntegrateDNS">#</a></h2></div></div></div><p>
  Integration with DNSaaS may be needed if:
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    The external endpoint is configured to use <code class="literal">myardana.test</code>
    as host name and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> front-end certificate is issued for this host name.
   </p></li><li class="listitem "><p>
    Minions are registered using Nova VM names as hostnames Kubernetes API
    server. Most kubectl commands will not work if the VM name (for example,
    <code class="literal">cl-mu3eevqizh-1-b3vifun6qtuh-kube-minion-ff4cqjgsuzhy</code>)
    is not getting resolved at the provided DNS server.
   </p></li></ol></div><p>
  Follow these steps to integrate the Magnum Service with the DNS Service.
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Allow connections from VMs to EXT-API
   </p><div class="verbatim-wrap"><pre class="screen">sudo modprobe 8021q
sudo ip link add link virbr5 name vlan108 type vlan id 108
sudo ip link set dev vlan108 up
sudo ip addr add 192.168.14.200/24 dev vlan108
sudo iptables -t nat -A POSTROUTING -o vlan108 -j MASQUERADE</pre></div></li><li class="step "><p>
    Run the designate reconfigure playbook.
   </p><div class="verbatim-wrap"><pre class="screen">$ cd ~/scratch/ansible/next/ardana/ansible/
$ ansible-playbook -i hosts/verb_hosts designate-reconfigure.yml</pre></div></li><li class="step "><p>
    Set up Designate to resolve myardana.test correctly.
   </p><div class="verbatim-wrap"><pre class="screen">$ openstack zone create --email hostmaster@myardana.test myardana.test.
# wait for status to become active
$ EXTERNAL_VIP=$(grep HZN-WEB-extapi /etc/hosts | awk '{ print $1 }')
$ openstack recordset create --records $EXTERNAL_VIP --type A myardana.test. myardana.test.
# wait for status to become active
$ LOCAL_MGMT_IP=$(grep `hostname` /etc/hosts | awk '{ print $1 }')
$ nslookup myardana.test $LOCAL_MGMT_IP
Server:        192.168.14.2
Address:       192.168.14.2#53
Name:          myardana.test
Address:       192.168.14.5</pre></div></li><li class="step "><p>
    If you need to add/override a top level domain record, the following
    example should be used, substituting proxy.example.org with your own real
    address:
   </p><div class="verbatim-wrap"><pre class="screen">$ openstack tld create --name net
$ openstack zone create --email hostmaster@proxy.example.org proxy.example.org.
$ openstack recordset create --records 16.85.88.10 --type A proxy.example.org. proxy.example.org.
$ nslookup proxy.example.org. 192.168.14.2
Server:        192.168.14.2
Address:       192.168.14.2#53
Name:          proxy.example.org
Address:       16.85.88.10</pre></div></li><li class="step "><p>
    Enable propagation of dns_assignment and dns_name attributes to neutron
    ports, as per
    <a class="link" href="https://docs.openstack.org/draft/networking-guide/config-dns-int.html" target="_blank">https://docs.openstack.org/draft/networking-guide/config-dns-int.html</a>
   </p><div class="verbatim-wrap"><pre class="screen"># optionally add 'dns_domain = &lt;some domain name&gt;.' to [DEFAULT] section
# of ardana/ansible/roles/neutron-common/templates/neutron.conf.j2
stack@ksperf2-cp1-c1-m1-mgmt:~/openstack$ cat &lt;&lt;-EOF &gt;&gt;ardana/services/designate/api.yml

   provides-data:
   -   to:
       -   name: neutron-ml2-plugin
       data:
       -   option: extension_drivers
           values:
           -   dns
EOF
$ git commit -a -m "Enable DNS support for neutron ports"
$ cd ardana/ansible
$ ansible-playbook -i hosts/localhost config-processor-run.yml
$ ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
    Enable DNSaaS registration of created VMs by editing the
    <code class="filename">~/openstack/ardana/ansible/roles/neutron-common/templates/neutron.conf.j2</code>
    file. You will need to add <code class="literal">external_dns_driver =
    designate</code> to the <span class="bold"><strong>[DEFAULT]</strong></span>
    section and create a new <span class="bold"><strong>[designate]</strong></span>
    section for the Designate specific configurations.
   </p><div class="verbatim-wrap"><pre class="screen">...
advertise_mtu = False
dns_domain = ksperf.
external_dns_driver = designate
{{ neutron_api_extensions_path|trim }}
{{ neutron_vlan_transparent|trim }}

# Add additional options here

[designate]
url = https://10.240.48.45:9001
admin_auth_url = https://10.240.48.45:35357/v3
admin_username = designate
admin_password = P8lZ9FdHuoW
admin_tenant_name = services
allow_reverse_dns_lookup = True
ipv4_ptr_zone_prefix_size = 24
ipv6_ptr_zone_prefix_size = 116
ca_cert = /etc/ssl/certs/ca-certificates.crt</pre></div></li><li class="step "><p>
    Commit your changes.
   </p><div class="verbatim-wrap"><pre class="screen">$ git commit -a -m "Enable DNSaaS registration of Nova VMs"
[site f4755c0] Enable DNSaaS registration of Nova VMs
1 file changed, 11 insertions(+)</pre></div></li></ol></div></div></div></div><div class="chapter " id="using_git"><div class="titlepage"><div><div><h2 class="title"><span class="number">12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using Git for Configuration Management</span> <a title="Permalink" class="permalink" href="#using_git">#</a></h2></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#idm139651561257664"><span class="number">12.1 </span><span class="name">Initialization on a new deployment</span></a></span></dt><dt><span class="section"><a href="#updating-configuration-including-default-config"><span class="number">12.2 </span><span class="name">Updating any configuration, including the default configuration</span></a></span></dt></dl></div></div><p>
  In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, a local git repository is used to track configuration
  changes; the Configuration Processor (CP) uses this repository. Use of a git
  workflow means that your configuration history is maintained, making
  rollbacks easier and keeping a record of previous configuration settings. The
  git repository also provides a way for you to merge changes that you pull down as
  <span class="quote">“<span class="quote">upstream</span>”</span> updates (that is, updates from <span class="phrase"><span class="phrase">SUSE</span></span>). It also
  allows you to manage your own configuration changes.
 </p><p>
  The git repository is installed by the Cloud Lifecycle Manager on the Cloud Lifecycle Manager node.
 </p><div class="sect1" id="idm139651561257664"><div class="titlepage"><div><div><h2 class="title" id="idm139651561257664"><span class="number">12.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Initialization on a new deployment</span> <a title="Permalink" class="permalink" href="#idm139651561257664">#</a></h2></div></div></div><p>
   On a system new to <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, the Cloud Lifecycle Manager will prepare a git repository
   under <code class="literal">~/openstack</code>. The Cloud Lifecycle Manager provisioning runs the
   <code class="literal">ardana-init-deployer</code> script automatically. This calls
   <code class="literal">ansible-playbook -i hosts/localhost git-00-initialise.yml</code>.
  </p><p>
   As a result, the <code class="literal">~/openstack</code> directory is initialized as
   a git repo (if it is empty). It is initialized with four empty branches:
  </p><div class="variablelist "><dl class="variablelist"><dt id="idm139651561251280"><span class="term ">ardana</span></dt><dd><p>
      This holds the upstream source code corresponding to the contents of the
      <code class="literal">~/openstack</code> directory on a pristine installation.
      Every source code release that is downloaded from <span class="phrase"><span class="phrase">SUSE</span></span> is applied as
      a fresh commit to this branch. This branch contains no customization by
      the end user.
     </p></dd><dt id="idm139651561247952"><span class="term ">site</span></dt><dd><p>
      This branch begins life as a copy of the first <code class="literal">ardana</code>
      drop. It is onto this branch that you commit your configuration changes.
      It is the branch most visible to the end user.
     </p></dd><dt id="idm139651561245552"><span class="term ">ansible</span></dt><dd><p>
      This branch contains the variable definitions generated by the CP that
      our main ansible playbooks need. This includes the
      <code class="literal">verb_hosts</code> file that describes to ansible what servers
      are playing what roles. The <code class="literal">ready-deployment</code> playbook
      takes this output and assembles a <code class="literal">~/scratch</code> directory
      containing the ansible playbooks together with the variable definitions
      in this branch. The result is a working ansible directory
      <code class="literal">~/scratch/ansible/next/ardana/ansible</code> from which the
      main deployment playbooks may be successfully run.
     </p></dd><dt id="idm139651561241520"><span class="term ">cp-persistent</span></dt><dd><p>
      This branch contains the persistent state that the CP needs to maintain.
      That state is mostly the assignment of IP addresses and roles to
      particular servers. Some operational procedures may involve editing the
      contents of this branch: for example, retiring a machine from service or
      repurposing it.
     </p></dd></dl></div><p>
   Two temporary branches are created and populated at run time:
  </p><div class="variablelist "><dl class="variablelist"><dt id="idm139651561238464"><span class="term ">staging-ansible</span></dt><dd><p>
      This branch hosts the most recent commit that will be appended to the
      Ansible branch.
     </p></dd><dt id="idm139651561236576"><span class="term ">staging-cp-persistent</span></dt><dd><p>
      This branch hosts the most recent commit that will be appended to the
      cp-persistent branch.
     </p></dd></dl></div><div id="idm139651561234528" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    The information above provides insight into the workings of the
    configuration processor and the git repository. However, in practice you
    can simply follow the steps below to make configuration changes.
   </p></div></div><div class="sect1" id="updating-configuration-including-default-config"><div class="titlepage"><div><div><h2 class="title" id="updating-configuration-including-default-config"><span class="number">12.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Updating any configuration, including the default configuration</span> <a title="Permalink" class="permalink" href="#updating-configuration-including-default-config">#</a></h2></div></div></div><p>
   When you need to make updates to a configuration you must:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Check out the <code class="literal">site</code> branch. You may already be on that
     branch. If so, git will tell you that and the command will leave you
     there.
    </p><div class="verbatim-wrap"><pre class="screen">git checkout site</pre></div></li><li class="step "><p>
     Edit the YAML file or files that contain the configuration you want to
     change.
    </p></li><li class="step "><p>
     Commit the changes to the <code class="literal">site</code> branch.
    </p><div class="verbatim-wrap"><pre class="screen">git add -A
git commit -m "your commit message goes here in quotes"</pre></div><p>
     If you want to add a single file to your git repository, you can use the
     command below, as opposed to using <code class="command">git add -A</code>.
    </p><div class="verbatim-wrap"><pre class="screen">git add PATH_TO_FILE</pre></div><p>
     For example, if you made a change to your <code class="command">servers.yml</code>
     file and wanted to only commit that change, you would use this command:
    </p><div class="verbatim-wrap"><pre class="screen">git add ~/openstack/my_cloud/definition/data/servers.yml</pre></div></li><li class="step "><p>
     To produce the required configuration processor output from those changes.
     Review the output files manually if required, run the configuration
     processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Ready the deployment area
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the deployment playbooks from the resulting scratch directory.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts site.yml</pre></div></li></ol></div></div></div></div><div class="chapter " id="install_kvm"><div class="titlepage"><div><div><h2 class="title"><span class="number">13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing Mid-scale and Entry-scale KVM</span> <a title="Permalink" class="permalink" href="#install_kvm">#</a></h2></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#sec.kvm.important_notes"><span class="number">13.1 </span><span class="name">Important Notes</span></a></span></dt><dt><span class="section"><a href="#sec.kvm.prereqs"><span class="number">13.2 </span><span class="name">Before You Start</span></a></span></dt><dt><span class="section"><a href="#sec.kvm.configuration"><span class="number">13.3 </span><span class="name">Configuring Your Environment</span></a></span></dt><dt><span class="section"><a href="#sec.kvm.provision"><span class="number">13.4 </span><span class="name">Provisioning Your Baremetal Nodes</span></a></span></dt><dt><span class="section"><a href="#sec.kvm.config_processor"><span class="number">13.5 </span><span class="name">Running the Configuration Processor</span></a></span></dt><dt><span class="section"><a href="#sec.kvm.security"><span class="number">13.6 </span><span class="name">Configuring TLS</span></a></span></dt><dt><span class="section"><a href="#sec.kvm.deploy"><span class="number">13.7 </span><span class="name">Deploying the Cloud</span></a></span></dt><dt><span class="section"><a href="#sec.kvm.configure_backend"><span class="number">13.8 </span><span class="name">Configuring a Block Storage Backend (Optional)</span></a></span></dt><dt><span class="section"><a href="#sec.kvm.post_installation"><span class="number">13.9 </span><span class="name">Post-Installation Verification and Administration</span></a></span></dt></dl></div></div><div class="sect1" id="sec.kvm.important_notes"><div class="titlepage"><div><div><h2 class="title" id="sec.kvm.important_notes"><span class="number">13.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Important Notes</span> <a title="Permalink" class="permalink" href="#sec.kvm.important_notes">#</a></h2></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     If you are looking for information about when to use the GUI installer and
     when to use the command line (CLI), see the
     <a class="xref" href="#install_overview" title="Installation Overview">Installation Overview</a>.
    </p></li><li class="listitem "><p>
     Review the <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 2 “Hardware and Software Support Matrix”</span> that we have listed.
    </p></li><li class="listitem "><p>
     Review the release notes to make yourself aware of any known issues and
     limitations.
    </p></li><li class="listitem "><p>
     The installation process can occur in different phases. For example, you
     can install the control plane only and then add Compute nodes afterwards
     if you would like.
    </p></li><li class="listitem "><p>
     If you run into issues during installation, we have put together a list of
     <a class="xref" href="#troubleshooting_installation" title="Chapter 19. Troubleshooting the Installation">Chapter 19, <em>Troubleshooting the Installation</em></a> you can reference.
    </p></li><li class="listitem "><p>
     Make sure all disks on the system(s) are wiped before you begin the
     install. (For Swift, refer to <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 12 “Modifying Example Configurations for Object Storage using Swift”, Section 12.6 “Swift Requirements for Device Group Drives”</span>.)
    </p></li><li class="listitem "><p>
     There is no requirement to have a dedicated network for OS-install and
     system deployment, this can be shared with the management network. More
     information can be found in <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”</span>.
    </p></li><li class="listitem "><p>
     You may see the terms deployer and Cloud Lifecycle Manager used interchangeably. These are
     referring to the same nodes in your environment.
    </p></li><li class="listitem "><p>
     When running the Ansible playbook in this installation guide, if a runbook
     fails you will see in the error response to use the
     <code class="literal">--limit</code> switch when retrying a playbook. This should be
     avoided. You can simply re-run any playbook without this switch.
    </p></li><li class="listitem "><p>
     DVR is not supported with ESX compute.
    </p></li><li class="listitem "><p>
     When you attach a Cinder volume to the VM running on the ESXi host, the
     volume will not get detected automatically. Make sure to set the image
     metadata <span class="bold"><strong>vmware_adaptertype=lsiLogicsas</strong></span>
     for image before launching the instance. This will help to discover the
     volume change appropriately.
    </p></li></ul></div></div><div class="sect1" id="sec.kvm.prereqs"><div class="titlepage"><div><div><h2 class="title" id="sec.kvm.prereqs"><span class="number">13.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Before You Start</span> <a title="Permalink" class="permalink" href="#sec.kvm.prereqs">#</a></h2></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Review the <a class="xref" href="#preinstall_checklist" title="Chapter 2. Pre-Installation Checklist">Chapter 2, <em>Pre-Installation Checklist</em></a> about recommended
     pre-installation tasks.
    </p></li><li class="step "><p>
     Prepare the Cloud Lifecycle Manager node. The Cloud Lifecycle Manager must be accessible either directly or via
     <code class="literal">ssh</code>, and have SUSE Linux Enterprise Server 12 SP3 installed. All nodes must
     be accessible to the Cloud Lifecycle Manager. If the nodes do not have direct access to
     online Cloud subscription channels, the Cloud Lifecycle Manager node will need to host the
     Cloud repositories.
    </p><ol type="a" class="substeps "><li class="step "><p>
       If you followed the installation instructions for Cloud Lifecycle Manager (see <a class="xref" href="#cha.depl.dep_inst" title="Chapter 3. Installing the Cloud Lifecycle Manager server">Chapter 3, <em>Installing the Cloud Lifecycle Manager server</em></a> <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> should already be
       installed. Double-check whether SUSE Linux Enterprise and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> are properly
       registered at the SUSE Customer Center by starting YaST and running <span class="guimenu">Software</span> › <span class="guimenu">Product
       Registration</span>.
      </p><p>
       If you have not yet installed <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, do so by starting YaST and
       running <span class="guimenu">Software</span> › <span class="guimenu">Product
       Registration</span> › <span class="guimenu">Select
       Extensions</span>. Choose <span class="guimenu"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> 
       and follow the on-screen instructions. Make sure to register
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> during the installation process and to install the software
       pattern <code class="literal">patterns-cloud-ardana</code>.
      </p></li><li class="step "><p>
       Ensure the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> media repositories and updates repositories are
       made available to all nodes in your deployment. This can be accomplished
       either by configuring the Cloud Lifecycle Manager server as an SMT mirror as described in
       <a class="xref" href="#app.deploy.smt_lcm" title="Chapter 4. Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)">Chapter 4, <em>Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)</em></a> or by syncing or mounting the
       Cloud and updates repositories to the Cloud Lifecycle Manager server as described in <a class="xref" href="#cha.depl.repo_conf_lcm" title="Chapter 5. Software Repository Setup">Chapter 5, <em>Software Repository Setup</em></a>.
      </p></li><li class="step "><p>
       Configure passwordless <code class="command">sudo</code> for the user created when
       setting up the node (as described in <a class="xref" href="#sec.depl.adm_inst.user" title="3.5. Creating a User">Section 3.5, “Creating a User”</a>). Note that this is
       <span class="emphasis"><em>not</em></span> the user <code class="systemitem">ardana</code> that will be used later in this
       procedure. In the following we assume you named the user <code class="systemitem">cloud</code>. Run the command
       <code class="command">visudo</code> as user <code class="systemitem">root</code> and add the following line
       to the end of the file:
      </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable ">cloud</em> ALL = (root) NOPASSWD:ALL</pre></div><p>
       Make sure to replace <em class="replaceable ">cloud</em> with your user name
       choice.
      </p></li><li class="step "><p>
       Set the password for the user
       <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen">sudo passwd ardana</pre></div></li><li class="step "><p>
       Become the user <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen">su - ardana</pre></div></li><li class="step "><p>
       Place a copy of the SUSE Linux Enterprise Server 12 SP3 <code class="filename">.iso</code> in the
       <code class="systemitem">ardana</code> home directory,
       <code class="filename">var/lib/ardana</code>, and rename it to
       <code class="filename">sles12sp3.iso</code>.
      </p></li></ol></li></ol></div></div></div><div class="sect1" id="sec.kvm.configuration"><div class="titlepage"><div><div><h2 class="title" id="sec.kvm.configuration"><span class="number">13.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Your Environment</span> <a title="Permalink" class="permalink" href="#sec.kvm.configuration">#</a></h2></div></div></div><p>
   During the configuration phase of the installation you will be making
   modifications to the example configuration input files to match your cloud
   environment. You should use the <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”</span>
   documentation for detailed information on how to do this. There is also a
   <code class="filename">README.md</code> file included in each of the example
   directories on the Cloud Lifecycle Manager that has useful information about the models.
  </p><p>
   In the steps below we show how to set up the directory structure with the
   example input files as well as use the optional encryption methods for your
   sensitive data.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Setup your configuration files, as follows:
    </p><ol type="a" class="substeps "><li class="step "><p>
       Copy the example configuration files into the required setup directory
       and edit them to contain the details of your environment.
      </p><p>
       For example, if you want to use the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Mid-scale KVM model,
       you can use this command to copy the files to your cloud definition
       directory:
      </p><div class="verbatim-wrap"><pre class="screen">cp -r ~/openstack/examples/mid-scale-kvm/* \
~/openstack/my_cloud/definition/</pre></div><p>
       If you want to use the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale KVM model, you can use
       this command to copy the files to your cloud definition directory:
      </p><div class="verbatim-wrap"><pre class="screen">cp -r ~/openstack/examples/entry-scale-kvm/* \
~/openstack/my_cloud/definition/</pre></div></li><li class="step "><p>
       Begin inputting your environment information into the configuration
       files in the <code class="filename">~/openstack/my_cloud/definition</code>
       directory.
      </p></li></ol></li><li class="step "><p><span class="step-optional">(Optional)</span> 
     You can use the <code class="literal">ardanaencrypt.py</code> script to
     encrypt your IPMI passwords. This script uses OpenSSL.
    </p><ol type="a" class="substeps "><li class="step "><p>
       Change to the Ansible directory:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible</pre></div></li><li class="step "><p>
       Put the encryption key into the following environment variable:
      </p><div class="verbatim-wrap"><pre class="screen">export ARDANA_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key&gt;</pre></div></li><li class="step "><p>
       Run the python script below and follow the instructions. Enter a
       password that you want to encrypt.
      </p><div class="verbatim-wrap"><pre class="screen">./ardanaencrypt.py</pre></div></li><li class="step "><p>
       Take the string generated and place it in the
       <code class="literal">ilo-password</code> field in your
       <code class="filename">~/openstack/my_cloud/definition/data/servers.yml</code>
       file, remembering to enclose it in quotes.
      </p></li><li class="step "><p>
       Repeat the above for each server.
      </p><div id="idm139651561137472" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
        Before you run any playbooks, remember that you need to export the
        encryption key in the following environment variable: <code class="literal">export
        ARDANA_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key&gt;</code>
       </p></div></li></ol></li><li class="step "><p>
     Commit your configuration to the local git repo
     (<a class="xref" href="#using_git" title="Chapter 12. Using Git for Configuration Management">Chapter 12, <em>Using Git for Configuration Management</em></a>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div><div id="idm139651561133600" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      This step needs to be repeated any time you make changes to your
      configuration files before you move onto the following steps. See
      <a class="xref" href="#using_git" title="Chapter 12. Using Git for Configuration Management">Chapter 12, <em>Using Git for Configuration Management</em></a> for more information.
     </p></div></li></ol></div></div></div><div class="sect1" id="sec.kvm.provision"><div class="titlepage"><div><div><h2 class="title" id="sec.kvm.provision"><span class="number">13.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Provisioning Your Baremetal Nodes</span> <a title="Permalink" class="permalink" href="#sec.kvm.provision">#</a></h2></div></div></div><p>
   To provision the baremetal nodes in your cloud deployment you can either use
   the automated operating system installation process provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> or
   you can use the 3rd party installation tooling of your choice. We will
   outline both methods below:
  </p><div class="sect2" id="idm139651561127104"><div class="titlepage"><div><div><h3 class="title" id="idm139651561127104"><span class="number">13.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using Third Party Baremetal Installers</span> <a title="Permalink" class="permalink" href="#idm139651561127104">#</a></h3></div></div></div><p>
    If you do not wish to use the automated operating system installation
    tooling included with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> then the requirements that have to be met
    using the installation tooling of your choice are:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      The operating system must be installed via the SLES ISO provided on
      the <a class="link" href="https://scc.suse.com/" target="_blank">SUSE Customer Center</a>.
     </p></li><li class="listitem "><p>
      Each node must have SSH keys in place that allows the same user from the
      Cloud Lifecycle Manager node who will be doing the deployment to SSH to each node without a
      password.
     </p></li><li class="listitem "><p>
      Passwordless sudo needs to be enabled for the user.
     </p></li><li class="listitem "><p>
      There should be a LVM logical volume as <code class="literal">/root</code> on each
      node.
     </p></li><li class="listitem "><p>
      If the LVM volume group name for the volume group holding the
      <code class="literal">root</code> LVM logical volume is
      <code class="literal">ardana-vg</code>, then it will align with the disk input
      models in the examples.
     </p></li><li class="listitem "><p>
      <span class="phrase">Ensure that <code class="literal">openssh-server</code>,
      <code class="literal">python</code>, <code class="literal">python-apt</code>, and
      <code class="literal">rsync</code> are installed.</span>
     </p></li></ul></div><p>
    If you chose this method for installing your baremetal hardware, skip
    forward to the step
    <em class="citetitle ">Running the Configuration Processor</em>.
   </p></div><div class="sect2" id="idm139651561111904"><div class="titlepage"><div><div><h3 class="title" id="idm139651561111904"><span class="number">13.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Automated Operating System Installation Provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <a title="Permalink" class="permalink" href="#idm139651561111904">#</a></h3></div></div></div><p>
    If you would like to use the automated operating system installation tools
    provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, complete the steps below.
   </p><div class="sect3" id="idm139651561107488"><div class="titlepage"><div><div><h4 class="title" id="idm139651561107488"><span class="number">13.4.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Cobbler</span> <a title="Permalink" class="permalink" href="#idm139651561107488">#</a></h4></div></div></div><p>
     This phase of the install process takes the baremetal information that was
     provided in <code class="literal">servers.yml</code> and installs the Cobbler
     provisioning tool and loads this information into Cobbler. This sets each
     node to <code class="literal">netboot-enabled: true</code> in Cobbler. Each node
     will be automatically marked as <code class="literal">netboot-enabled: false</code>
     when it completes its operating system install successfully. Even if the
     node tries to PXE boot subsequently, Cobbler will not serve it. This is
     deliberate so that you can't reimage a live node by accident.
    </p><p>
     The <code class="literal">cobbler-deploy.yml</code> playbook prompts for a password
     - this is the password that will be encrypted and stored in Cobbler, which
     is associated with the user running the command on the Cloud Lifecycle Manager, that you
     will use to log in to the nodes via their consoles after install. The
     username is the same as the user set up in the initial dialogue when
     installing the Cloud Lifecycle Manager from the ISO, and is the same user that is running
     the cobbler-deploy play.
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Run the following playbook which confirms that there is IPMI connectivity
       for each of your nodes so that they are accessible to be re-imaged in a
       later step:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-power-status.yml</pre></div></li><li class="step "><p>
       Run the following playbook to deploy Cobbler:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li></ol></div></div></div><div class="sect3" id="idm139651561099520"><div class="titlepage"><div><div><h4 class="title" id="idm139651561099520"><span class="number">13.4.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Imaging the Nodes</span> <a title="Permalink" class="permalink" href="#idm139651561099520">#</a></h4></div></div></div><p>
     This phase of the install process goes through a number of distinct steps:
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Powers down the nodes to be installed
      </p></li><li class="step "><p>
       Sets the nodes hardware boot order so that the first option is a network
       boot.
      </p></li><li class="step "><p>
       Powers on the nodes. (The nodes will then boot from the network and be
       installed using infrastructure set up in the previous phase)
      </p></li><li class="step "><p>
       Waits for the nodes to power themselves down (this indicates a success
       install). This can take some time.
      </p></li><li class="step "><p>
       Sets the boot order to hard disk and powers on the nodes.
      </p></li><li class="step "><p>
       Waits for the nodes to be ssh-able and verifies that they have the
       signature expected.
      </p></li></ol></div></div><p>
     The reimaging command is:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml \
  [-e nodelist=node1,node2,node3]</pre></div><p>
     If a nodelist is not specified then the set of nodes in cobbler with
     <code class="literal">netboot-enabled: True</code> is selected. The playbook pauses
     at the start to give you a chance to review the set of nodes that it is
     targeting and to confirm that it is correct.
    </p><p>
     You can use the command below which will list all of your nodes with the
     <code class="literal">netboot-enabled: True</code> flag set:
    </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system find --netboot-enabled=1</pre></div></div></div></div><div class="sect1" id="sec.kvm.config_processor"><div class="titlepage"><div><div><h2 class="title" id="sec.kvm.config_processor"><span class="number">13.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Running the Configuration Processor</span> <a title="Permalink" class="permalink" href="#sec.kvm.config_processor">#</a></h2></div></div></div><p>
   Once you have your configuration files setup, you need to run the
   configuration processor to complete your configuration.
  </p><p>
   When you run the configuration processor, you will be prompted for two
   passwords. Enter the first password to make the configuration processor
   encrypt its sensitive data, which consists of the random inter-service
   passwords that it generates and the ansible <code class="literal">group_vars</code>
   and <code class="literal">host_vars</code> that it produces for subsequent deploy
   runs. You will need this password for subsequent Ansible deploy and
   configuration processor runs. If you wish to change an encryption password
   that you have already used when running the configuration processor then
   enter the new password at the second prompt, otherwise just press
   <span class="keycap">Enter</span> to bypass this.
  </p><p>
   Run the configuration processor with this command:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
   For automated installation (for example CI), you can specify the required
   passwords on the ansible command line. For example, the command below will
   disable encryption by the configuration processor:
  </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost config-processor-run.yml \
  -e encrypt="" -e rekey=""</pre></div><p>
   If you receive an error during this step, there is probably an issue with
   one or more of your configuration files. Verify that all information in each
   of your configuration files is correct for your environment. Then commit
   those changes to Git using the instructions in the previous section before
   re-running the configuration processor again.
  </p><p>
   For any troubleshooting information regarding these steps, see
   <a class="xref" href="#sec.trouble-config_processor" title="19.2. Issues while Updating Configuration Files">Section 19.2, “Issues while Updating Configuration Files”</a>.
  </p></div><div class="sect1" id="sec.kvm.security"><div class="titlepage"><div><div><h2 class="title" id="sec.kvm.security"><span class="number">13.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring TLS</span> <a title="Permalink" class="permalink" href="#sec.kvm.security">#</a></h2></div></div></div><div id="idm139651561074304" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    This section is optional, but recommended, for a <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installation.
   </p></div><p>
   After you run the configuration processor the first time, the IP addresses
   for your environment will be generated and populated in the
   <code class="filename">~/openstack/my_cloud/info/address_info.yml</code> file. At
   this point, consider whether to configure TLS and set up an SSL certificate
   for your environment. Please read <a class="xref" href="#tls30" title="Chapter 25. Configuring Transport Layer Security (TLS)">Chapter 25, <em>Configuring Transport Layer Security (TLS)</em></a> before proceeding
   for how to achieve this.
  </p></div><div class="sect1" id="sec.kvm.deploy"><div class="titlepage"><div><div><h2 class="title" id="sec.kvm.deploy"><span class="number">13.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying the Cloud</span> <a title="Permalink" class="permalink" href="#sec.kvm.deploy">#</a></h2></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Use the playbook below to create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     [OPTIONAL] - Run the <code class="literal">wipe_disks.yml</code> playbook to ensure
     all of your partitions on your nodes are completely wiped before
     continuing with the installation. If you are using fresh machines this
     step may not be necessary.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts wipe_disks.yml</pre></div><p>
     If you have used an encryption password when running the configuration
     processor use the command below and enter the encryption password when
     prompted:
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts wipe_disks.yml --ask-vault-pass</pre></div></li><li class="step "><p>
     Run the <code class="literal">site.yml</code> playbook below:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts site.yml</pre></div><p>
     If you have used an encryption password when running the configuration
     processor use the command below and enter the encryption password when
     prompted:
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts site.yml --ask-vault-pass</pre></div><div id="idm139651561059776" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The step above runs <code class="literal">osconfig</code> to configure the cloud
      and <code class="literal">ardana-deploy</code> to deploy the cloud. Therefore, this
      step may run for a while, perhaps 45 minutes or more, depending on the
      number of nodes in your environment.
     </p></div></li><li class="step "><p>
     Verify that the network is working correctly. Ping each IP in the
     <code class="literal">/etc/hosts</code> file from one of the controller nodes.
    </p></li></ol></div></div><p>
   For any troubleshooting information regarding these steps, see
   <a class="xref" href="#sec.trouble-deploy_cloud" title="19.3. Issues while Deploying the Cloud">Section 19.3, “Issues while Deploying the Cloud”</a>.
  </p></div><div class="sect1" id="sec.kvm.configure_backend"><div class="titlepage"><div><div><h2 class="title" id="sec.kvm.configure_backend"><span class="number">13.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring a Block Storage Backend (Optional)</span> <a title="Permalink" class="permalink" href="#sec.kvm.configure_backend">#</a></h2></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> supports multiple block storage backend options. You can use one or
   more of these for setting up multiple block storage backends. Multiple
   volume types are also supported.
  </p><p>
   Whether you have a single or multiple block storage backends defined in your
   <code class="filename">cinder.conf.j2</code> file, you can create one or more volume
   types using the specific attributes associated with the backend. For more
   information, see <a class="xref" href="#config_3par" title="18.1. Configuring for 3PAR Block Storage Backend">Section 18.1, “Configuring for 3PAR Block Storage Backend”</a>.
  </p></div><div class="sect1" id="sec.kvm.post_installation"><div class="titlepage"><div><div><h2 class="title" id="sec.kvm.post_installation"><span class="number">13.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Post-Installation Verification and Administration</span> <a title="Permalink" class="permalink" href="#sec.kvm.post_installation">#</a></h2></div></div></div><p>
   We recommend verifying the installation using the instructions in
   <a class="xref" href="#cloud_verification" title="Chapter 22. Cloud Verification">Chapter 22, <em>Cloud Verification</em></a>.
  </p><p>
   There are also a list of other common post-installation administrative tasks
   listed in the <a class="xref" href="#postinstall_checklist" title="Chapter 28. Other Common Post-Installation Tasks">Chapter 28, <em>Other Common Post-Installation Tasks</em></a> list.
  </p></div></div><div class="chapter " id="install_ironic_overview"><div class="titlepage"><div><div><h2 class="title"><span class="number">14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing Baremetal (Ironic)</span> <a title="Permalink" class="permalink" href="#install_ironic_overview">#</a></h2></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#install_ironic"><span class="number">14.1 </span><span class="name">Installation for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale Cloud with Ironic Flat Network</span></a></span></dt><dt><span class="section"><a href="#ironic_multi_control_plane"><span class="number">14.2 </span><span class="name">Ironic in Multiple Control Plane</span></a></span></dt><dt><span class="section"><a href="#ironic-provisioning"><span class="number">14.3 </span><span class="name">Provisioning Bare-Metal Nodes with Flat Network Model</span></a></span></dt><dt><span class="section"><a href="#ironic-provisioning-multi-tenancy"><span class="number">14.4 </span><span class="name">Provisioning Baremetal Nodes with Multi-Tenancy</span></a></span></dt><dt><span class="section"><a href="#ironic-system-details"><span class="number">14.5 </span><span class="name">View Ironic System Details</span></a></span></dt><dt><span class="section"><a href="#ironic-toubleshooting"><span class="number">14.6 </span><span class="name">Troubleshooting Ironic Installation</span></a></span></dt><dt><span class="section"><a href="#ironic-node-cleaning"><span class="number">14.7 </span><span class="name">Node Cleaning</span></a></span></dt><dt><span class="section"><a href="#ironic_oneview"><span class="number">14.8 </span><span class="name">Ironic and HPE OneView</span></a></span></dt><dt><span class="section"><a href="#ironic_raid_config"><span class="number">14.9 </span><span class="name">RAID Configuration for Ironic</span></a></span></dt><dt><span class="section"><a href="#ironic_audit_support"><span class="number">14.10 </span><span class="name">Audit Support for Ironic</span></a></span></dt></dl></div></div><p>
  Bare Metal as a Service is enabled in this release for deployment of Nova
  instances on bare metal nodes using flat networking.
  
 </p><div class="sect1" id="install_ironic"><div class="titlepage"><div><div><h2 class="title" id="install_ironic"><span class="number">14.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installation for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale Cloud with Ironic Flat Network</span> <a title="Permalink" class="permalink" href="#install_ironic">#</a></h2></div></div></div><p>
  This page describes the installation step requirements for the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  Entry-scale Cloud with Ironic Flat Network.
 </p><div class="sect2" id="idm139651561034256"><div class="titlepage"><div><div><h3 class="title" id="idm139651561034256"><span class="number">14.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure Your Environment</span> <a title="Permalink" class="permalink" href="#idm139651561034256">#</a></h3></div></div></div><p>
   Prior to deploying an operational environment with Ironic, operators need to
   be aware of the nature of TLS certificate authentication. As pre-built
   deployment agent ramdisks images are supplied, these ramdisk images will
   only authenticate known third-party TLS Certificate Authorities in the
   interest of end-to-end security. As such, uses of self-signed certificates
   and private certificate authorities will be unable to leverage ironic
   without modifying the supplied ramdisk images.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Set up your configuration files, as follows:
    </p><ol type="a" class="substeps "><li class="step "><p>
       See the sample sets of configuration files in the
       <code class="literal">~/openstack/examples/</code> directory. Each set will have an
       accompanying README.md file that explains the contents of each of the
       configuration files.
      </p></li><li class="step "><p>
       Copy the example configuration files into the required setup directory
       and edit them to contain the details of your environment:
      </p><div class="verbatim-wrap"><pre class="screen">cp -r ~/openstack/examples/entry-scale-ironic-flat-network/* \
  ~/openstack/my_cloud/definition/</pre></div></li></ol></li><li class="step "><p><span class="step-optional">(Optional)</span> 
     You can use the <code class="literal">ardanaencrypt.py</code> script to
     encrypt your IPMI passwords. This script uses OpenSSL.
    </p><ol type="a" class="substeps "><li class="step "><p>
       Change to the Ansible directory:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible</pre></div></li><li class="step "><p>
       Put the encryption key into the following environment variable:
      </p><div class="verbatim-wrap"><pre class="screen">export ARDANA_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key&gt;</pre></div></li><li class="step "><p>
       Run the python script below and follow the instructions. Enter a
       password that you want to encrypt.
      </p><div class="verbatim-wrap"><pre class="screen">./ardanaencrypt.py</pre></div></li><li class="step "><p>
       Take the string generated and place it in the
       <code class="literal">ilo-password</code> field in your
       <code class="filename">~/openstack/my_cloud/definition/data/servers.yml</code>
       file, remembering to enclose it in quotes.
      </p></li><li class="step "><p>
       Repeat the above for each server.
      </p><div id="idm139651561018784" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
        Before you run any playbooks, remember that you need to export the
        encryption key in the following environment variable: <code class="literal">export
        ARDANA_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key&gt;</code>
       </p></div></li></ol></li><li class="step "><p>
     Commit your configuration to the local git repo
     (<a class="xref" href="#using_git" title="Chapter 12. Using Git for Configuration Management">Chapter 12, <em>Using Git for Configuration Management</em></a>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div><div id="idm139651561014544" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      This step needs to be repeated any time you make changes to your
      configuration files before you move onto the following steps. See
      <a class="xref" href="#using_git" title="Chapter 12. Using Git for Configuration Management">Chapter 12, <em>Using Git for Configuration Management</em></a> for more information.
     </p></div></li></ol></div></div></div><div class="sect2" id="sec.ironic.provision"><div class="titlepage"><div><div><h3 class="title" id="sec.ironic.provision"><span class="number">14.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Provisioning Your Baremetal Nodes</span> <a title="Permalink" class="permalink" href="#sec.ironic.provision">#</a></h3></div></div></div><p>
   To provision the baremetal nodes in your cloud deployment you can either use
   the automated operating system installation process provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> or
   you can use the 3rd party installation tooling of your choice. We will
   outline both methods below:
  </p><div class="sect3" id="idm139651561008048"><div class="titlepage"><div><div><h4 class="title" id="idm139651561008048"><span class="number">14.1.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using Third Party Baremetal Installers</span> <a title="Permalink" class="permalink" href="#idm139651561008048">#</a></h4></div></div></div><p>
    If you do not wish to use the automated operating system installation
    tooling included with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> then the requirements that have to be met
    using the installation tooling of your choice are:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      The operating system must be installed via the SLES ISO provided on
      the <a class="link" href="https://scc.suse.com/" target="_blank">SUSE Customer Center</a>.
     </p></li><li class="listitem "><p>
      Each node must have SSH keys in place that allows the same user from the
      Cloud Lifecycle Manager node who will be doing the deployment to SSH to each node without a
      password.
     </p></li><li class="listitem "><p>
      Passwordless sudo needs to be enabled for the user.
     </p></li><li class="listitem "><p>
      There should be a LVM logical volume as <code class="literal">/root</code> on each
      node.
     </p></li><li class="listitem "><p>
      If the LVM volume group name for the volume group holding the
      <code class="literal">root</code> LVM logical volume is
      <code class="literal">ardana-vg</code>, then it will align with the disk input
      models in the examples.
     </p></li><li class="listitem "><p>
      <span class="phrase">Ensure that <code class="literal">openssh-server</code>,
      <code class="literal">python</code>, <code class="literal">python-apt</code>, and
      <code class="literal">rsync</code> are installed.</span>
     </p></li></ul></div><p>
    If you chose this method for installing your baremetal hardware, skip
    forward to the step
    <em class="citetitle ">Running the Configuration Processor</em>.
   </p></div><div class="sect3" id="idm139651560992848"><div class="titlepage"><div><div><h4 class="title" id="idm139651560992848"><span class="number">14.1.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Automated Operating System Installation Provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <a title="Permalink" class="permalink" href="#idm139651560992848">#</a></h4></div></div></div><p>
    If you would like to use the automated operating system installation tools
    provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, complete the steps below.
   </p><div class="sect4" id="idm139651560988432"><div class="titlepage"><div><div><h5 class="title" id="idm139651560988432"><span class="number">14.1.2.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Cobbler</span> <a title="Permalink" class="permalink" href="#idm139651560988432">#</a></h5></div></div></div><p>
     This phase of the install process takes the baremetal information that was
     provided in <code class="literal">servers.yml</code> and installs the Cobbler
     provisioning tool and loads this information into Cobbler. This sets each
     node to <code class="literal">netboot-enabled: true</code> in Cobbler. Each node
     will be automatically marked as <code class="literal">netboot-enabled: false</code>
     when it completes its operating system install successfully. Even if the
     node tries to PXE boot subsequently, Cobbler will not serve it. This is
     deliberate so that you can't reimage a live node by accident.
    </p><p>
     The <code class="literal">cobbler-deploy.yml</code> playbook prompts for a password
     - this is the password that will be encrypted and stored in Cobbler, which
     is associated with the user running the command on the Cloud Lifecycle Manager, that you
     will use to log in to the nodes via their consoles after install. The
     username is the same as the user set up in the initial dialogue when
     installing the Cloud Lifecycle Manager from the ISO, and is the same user that is running
     the cobbler-deploy play.
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Run the following playbook which confirms that there is IPMI connectivity
       for each of your nodes so that they are accessible to be re-imaged in a
       later step:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-power-status.yml</pre></div></li><li class="step "><p>
       Run the following playbook to deploy Cobbler:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li></ol></div></div></div><div class="sect4" id="idm139651560980464"><div class="titlepage"><div><div><h5 class="title" id="idm139651560980464"><span class="number">14.1.2.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Imaging the Nodes</span> <a title="Permalink" class="permalink" href="#idm139651560980464">#</a></h5></div></div></div><p>
     This phase of the install process goes through a number of distinct steps:
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Powers down the nodes to be installed
      </p></li><li class="step "><p>
       Sets the nodes hardware boot order so that the first option is a network
       boot.
      </p></li><li class="step "><p>
       Powers on the nodes. (The nodes will then boot from the network and be
       installed using infrastructure set up in the previous phase)
      </p></li><li class="step "><p>
       Waits for the nodes to power themselves down (this indicates a success
       install). This can take some time.
      </p></li><li class="step "><p>
       Sets the boot order to hard disk and powers on the nodes.
      </p></li><li class="step "><p>
       Waits for the nodes to be ssh-able and verifies that they have the
       signature expected.
      </p></li></ol></div></div><p>
     The reimaging command is:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml \
  [-e nodelist=node1,node2,node3]</pre></div><p>
     If a nodelist is not specified then the set of nodes in cobbler with
     <code class="literal">netboot-enabled: True</code> is selected. The playbook pauses
     at the start to give you a chance to review the set of nodes that it is
     targeting and to confirm that it is correct.
    </p><p>
     You can use the command below which will list all of your nodes with the
     <code class="literal">netboot-enabled: True</code> flag set:
    </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system find --netboot-enabled=1</pre></div></div></div></div><div class="sect2" id="sec.ironic.config_processor"><div class="titlepage"><div><div><h3 class="title" id="sec.ironic.config_processor"><span class="number">14.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Running the Configuration Processor</span> <a title="Permalink" class="permalink" href="#sec.ironic.config_processor">#</a></h3></div></div></div><p>
   Once you have your configuration files setup, you need to run the
   configuration processor to complete your configuration.
  </p><p>
   When you run the configuration processor, you will be prompted for two
   passwords. Enter the first password to make the configuration processor
   encrypt its sensitive data, which consists of the random inter-service
   passwords that it generates and the ansible <code class="literal">group_vars</code>
   and <code class="literal">host_vars</code> that it produces for subsequent deploy
   runs. You will need this password for subsequent Ansible deploy and
   configuration processor runs. If you wish to change an encryption password
   that you have already used when running the configuration processor then
   enter the new password at the second prompt, otherwise just press
   <span class="keycap">Enter</span> to bypass this.
  </p><p>
   Run the configuration processor with this command:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
   For automated installation (for example CI), you can specify the required
   passwords on the ansible command line. For example, the command below will
   disable encryption by the configuration processor:
  </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost config-processor-run.yml \
  -e encrypt="" -e rekey=""</pre></div><p>
   If you receive an error during this step, there is probably an issue with
   one or more of your configuration files. Verify that all information in each
   of your configuration files is correct for your environment. Then commit
   those changes to Git using the instructions in the previous section before
   re-running the configuration processor again.
  </p><p>
   For any troubleshooting information regarding these steps, see
   <a class="xref" href="#sec.trouble-config_processor" title="19.2. Issues while Updating Configuration Files">Section 19.2, “Issues while Updating Configuration Files”</a>.
  </p></div><div class="sect2" id="sec.ironic.deploy"><div class="titlepage"><div><div><h3 class="title" id="sec.ironic.deploy"><span class="number">14.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying the Cloud</span> <a title="Permalink" class="permalink" href="#sec.ironic.deploy">#</a></h3></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Use the playbook below to create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     [OPTIONAL] - Run the <code class="literal">wipe_disks.yml</code> playbook to ensure
     all of your partitions on your nodes are completely wiped before
     continuing with the installation. If you are using fresh machines this
     step may not be necessary.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts wipe_disks.yml</pre></div><p>
     If you have used an encryption password when running the configuration
     processor use the command below and enter the encryption password when
     prompted:
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts wipe_disks.yml --ask-vault-pass</pre></div></li><li class="step "><p>
     Run the <code class="literal">site.yml</code> playbook below:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts site.yml</pre></div><p>
     If you have used an encryption password when running the configuration
     processor use the command below and enter the encryption password when
     prompted:
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts site.yml --ask-vault-pass</pre></div><div id="idm139651560947472" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The step above runs <code class="literal">osconfig</code> to configure the cloud
      and <code class="literal">ardana-deploy</code> to deploy the cloud. Therefore, this
      step may run for a while, perhaps 45 minutes or more, depending on the
      number of nodes in your environment.
     </p></div></li><li class="step "><p>
     Verify that the network is working correctly. Ping each IP in the
     <code class="literal">/etc/hosts</code> file from one of the controller nodes.
    </p></li></ol></div></div><p>
   For any troubleshooting information regarding these steps, see
   <a class="xref" href="#sec.trouble-deploy_cloud" title="19.3. Issues while Deploying the Cloud">Section 19.3, “Issues while Deploying the Cloud”</a>.
  </p></div><div class="sect2" id="idm139651560942016"><div class="titlepage"><div><div><h3 class="title" id="idm139651560942016"><span class="number">14.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ironic configuration</span> <a title="Permalink" class="permalink" href="#idm139651560942016">#</a></h3></div></div></div><p>
   Run the <code class="literal">ironic-cloud-configure.yml</code> playbook below:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ironic-cloud-configure.yml</pre></div><p>
   This step configures ironic flat network, uploads glance images and sets the
   ironic configuration.
  </p><p>
   To see the images uploaded to glance, run:
  </p><div class="verbatim-wrap"><pre class="screen">$ source ~/service.osrc
$ glance image-list</pre></div><p>
   This will produce output like the following example, showing three images
   that have been added by Ironic:
  </p><div class="verbatim-wrap"><pre class="screen">+--------------------------------------+--------------------------+
| ID                                   | Name                     |
+--------------------------------------+--------------------------+
| d4e2a0ff-9575-4bed-ac5e-5130a1553d93 | ir-deploy-iso-HOS3.0     |
| b759a1f0-3b33-4173-a6cb-be5706032124 | ir-deploy-kernel-HOS3.0  |
| ce5f4037-e368-46f2-941f-c01e9072676c | ir-deploy-ramdisk-HOS3.0 |
+--------------------------------------+--------------------------+</pre></div><p>
   To see the network created by Ironic, run:
  </p><div class="verbatim-wrap"><pre class="screen">$ neutron net-list</pre></div><p>
   This returns details of the "flat-net" generated by the Ironic install:
  </p><div class="verbatim-wrap"><pre class="screen"> +---------------+----------+-------------------------------------------------------+
 | id            | name     | subnets                                               |
 +---------------+----------+-------------------------------------------------------+
 | f9474...11010 | flat-net | ca8f8df8-12c8-4e58-b1eb-76844c4de7e8 192.168.245.0/24 |
 +---------------+----------+-------------------------------------------------------+</pre></div></div><div class="sect2" id="ironic-node-config"><div class="titlepage"><div><div><h3 class="title" id="ironic-node-config"><span class="number">14.1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Node Configuration</span> <a title="Permalink" class="permalink" href="#ironic-node-config">#</a></h3></div></div></div><div class="sect3" id="idm139651560931808"><div class="titlepage"><div><div><h4 class="title" id="idm139651560931808"><span class="number">14.1.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DHCP</span> <a title="Permalink" class="permalink" href="#idm139651560931808">#</a></h4></div></div></div><p>
   Once booted, nodes obtain network configuration via DHCP. If multiple
   interfaces are to be utilized, you may want to pre-build images with
   settings to execute DHCP on all interfaces. An easy way to build custom
   images is with KIWI, the command line utility to build Linux system
   appliances.
  </p><p>
   For information about building custom KIWI images, see
   <a class="xref" href="#sec.ironic-provision.kiwi" title="14.3.11. Building Glance Images Using KIWI">Section 14.3.11, “Building Glance Images Using KIWI”</a>.
   For more information, see the KIWI documentation at
   <a class="link" href="https://suse.github.io/kiwi/" target="_blank">https://suse.github.io/kiwi/</a>.
  </p></div><div class="sect3" id="idm139651560928480"><div class="titlepage"><div><div><h4 class="title" id="idm139651560928480"><span class="number">14.1.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuration Drives</span> <a title="Permalink" class="permalink" href="#idm139651560928480">#</a></h4></div></div></div><div id="idm139651560927744" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
    Configuration Drives are stored unencrypted and should not include any
    sensitive data.
   </p></div><p>
   You can use Configuration Drives to store metadata for initial boot
   setting customization. Configuration Drives are extremely useful for
   initial machine configuration. However, as a general security practice,
   they should not include any
   sensitive data. Configuration Drives should only be trusted upon the initial
   boot of an instance. <code class="literal">cloud-init</code> utilizes a lock file for
   this purpose. Custom instance images should not rely upon the integrity of a
   Configuration Drive beyond the initial boot of a host as an administrative
   user within a deployed instance can potentially modify a configuration drive
   once written to disk and released for use.
  </p><p>
   For more information about Configuration Drives, see
   <a class="link" href="http://docs.openstack.org/user-guide/cli_config_drive.html" target="_blank">http://docs.openstack.org/user-guide/cli_config_drive.html</a>.
  </p></div></div><div class="sect2" id="ironic-tls"><div class="titlepage"><div><div><h3 class="title" id="ironic-tls"><span class="number">14.1.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">TLS Certificates with Ironic Python Agent (IPA) Images</span> <a title="Permalink" class="permalink" href="#ironic-tls">#</a></h3></div></div></div><p>
  As part of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, Ironic Python Agent, better known as IPA in the
  OpenStack community, images are supplied and loaded into Glance. Two types of
  image exist. One is a traditional boot ramdisk which is used by the
  <code class="literal">agent_ipmitool</code>, <code class="literal">pxe_ipmitool</code>, and
  <code class="literal">pxe_ilo</code> drivers. The other is an ISO image that is
  supplied as virtual media to the host when using the
  <code class="literal">agent_ilo</code> driver.
 </p><p>
  As these images are built in advance, they are unaware of any private
  certificate authorities. Users attempting to utilize self-signed certificates
  or a private certificate authority will need to inject their signing
  certificate(s) into the image in order for IPA to be able to boot on a remote
  node, and ensure that the TLS endpoints being connected to in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> can be
  trusted. This is not an issue with publicly signed certificates.
 </p><p>
  As two different types of images exist, below are instructions for
  disassembling the image ramdisk file or the ISO image. Once this has been
  done, you will need to re-upload the files to glance, and update any impacted
  node's <code class="literal">driver_info</code>, for example, the
  <code class="literal">deploy_ramdisk</code> and <code class="literal">ilo_deploy_iso</code>
  settings that were set when the node was first defined. Respectively, this
  can be done with the
 </p><div class="verbatim-wrap"><pre class="screen">ironic node-update &lt;node&gt; replace driver_info/deploy_ramdisk=&lt;glance_id&gt;</pre></div><p>
  or
 </p><div class="verbatim-wrap"><pre class="screen">ironic node-update &lt;node&gt; replace driver_info/ilo_deploy_iso=&lt;glance_id&gt;</pre></div><div class="sect3" id="cert-ramdisk"><div class="titlepage"><div><div><h4 class="title" id="cert-ramdisk"><span class="number">14.1.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding a certificate into a ramdisk image</span> <a title="Permalink" class="permalink" href="#cert-ramdisk">#</a></h4></div></div></div><p>
  As root, from a folder where the ramdisk image is present, perform the
  following steps.
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Download the ramdisk image to /tmp/ and name the file
    ironic-deploy.initramfs
   </p></li><li class="step "><p>
    Change your working directory to /tmp/
   </p><div class="verbatim-wrap"><pre class="screen">cd /tmp/</pre></div></li><li class="step "><p>
    Create a temporary folder that will hold the temporarily extracted image
    contents.
   </p><div class="verbatim-wrap"><pre class="screen">mkdir new_deploy_ramdisk</pre></div></li><li class="step "><p>
    Change your shell working directory to the temporary folder
   </p><div class="verbatim-wrap"><pre class="screen">cd /tmp/new_deploy_ramdisk</pre></div></li><li class="step "><p>
    Extract the original deployment archive file.
   </p><div class="verbatim-wrap"><pre class="screen">zcat /tmp/ironic-deploy.initramfs | cpio --extract --make-directories</pre></div></li><li class="step "><p>
    Append your CA certificate to the file located at
    <code class="literal">/tmp/new_deploy_ramdisk/usr/local/lib/python2.7/dist-packages/requests/cacert.pem</code>,
    example below:
   </p><div class="verbatim-wrap"><pre class="screen">cat your_ca_certificate.pem &gt;&gt; /tmp/new_deploy_ramdisk/usr/local/lib/python2.7/dist-packages/requests/cacert.pem</pre></div><p>
    <code class="literal">your_ca_certificate.pem</code> is
    <code class="literal">/etc/ssl/certs/ca-certificates.crt</code> which can be obtained
    through
   </p><div class="verbatim-wrap"><pre class="screen">cat service.osrc | grep CACERT</pre></div></li><li class="step "><p>
    Package a new ramdisk file. From within the
    <code class="literal">/tmp/new_deploy_ramdisk</code> folder, execute the following
    command:
   </p><div class="verbatim-wrap"><pre class="screen">find . | cpio --create --format='newc' | gzip -c -9 &gt; /tmp/updated-ironic-deploy.initramfs</pre></div></li></ol></div></div><p>
  Once completed, the new image can be found at
  <code class="literal">/tmp/updated-ironic-deploy.initramfs</code>, and will need to be
  uploaded to Glance. New Glance IDs will need to be recorded for any instances
  requiring this new image, as noted in the parent paragraph.
 </p></div></div></div><div class="sect1" id="ironic_multi_control_plane"><div class="titlepage"><div><div><h2 class="title" id="ironic_multi_control_plane"><span class="number">14.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ironic in Multiple Control Plane</span> <a title="Permalink" class="permalink" href="#ironic_multi_control_plane">#</a></h2></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> introduces the concept of multiple control planes and
  multiple regions - see the Input Model documentation for the relevant
  <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 6 “Input Model”, Section 6.2 “Concepts”, Section 6.2.2 “Control Planes”, Section 6.2.2.1 “Control Planes and Regions”</span> and
  <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 7 “Configuration Objects”, Section 7.2 “Control Plane”, Section 7.2.3 “Multiple Control Planes”</span>. This document covers the
  use of an Ironic region in a multiple control plane cloud model in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
 </p><div class="sect2" id="idm139651560884816"><div class="titlepage"><div><div><h3 class="title" id="idm139651560884816"><span class="number">14.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networking for Baremetal in Multiple Control Plane</span> <a title="Permalink" class="permalink" href="#idm139651560884816">#</a></h3></div></div></div><p>
   <span class="bold"><strong>IRONIC-FLAT-NET</strong></span> is the network
   configuration for baremetal control plane.
  </p><p>
   You need to set the environment variable
   <span class="bold"><strong>OS_REGION_NAME</strong></span> to the Ironic region in
   baremetal control plane. This will set up the Ironic flat networking in
   Neutron.
  </p><div class="verbatim-wrap"><pre class="screen">export OS_REGION_NAME=&lt;ironic_region&gt;</pre></div><p>
   To see details of the <code class="literal">IRONIC-FLAT-NETWORK</code> created during
   configuration, use the following command:
  </p><div class="verbatim-wrap"><pre class="screen">neutron net-list</pre></div><div class="figure" id="idm139651560879664"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/media-ironic-ironic_multi_control_plane.png"><img src="images/media-ironic-ironic_multi_control_plane.png" width="" alt="Architecture of Multiple Control Plane with Ironic" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 14.1: </span><span class="name">Architecture of Multiple Control Plane with Ironic </span><a title="Permalink" class="permalink" href="#idm139651560879664">#</a></h6></div></div></div><div class="sect2" id="idm139651560874960"><div class="titlepage"><div><div><h3 class="title" id="idm139651560874960"><span class="number">14.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Handling Optional Swift Service</span> <a title="Permalink" class="permalink" href="#idm139651560874960">#</a></h3></div></div></div><p>
   Swift is very resource-intensive and as a result, it is now optional in the
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> control plane. A number of services depend on Swift, and if it is
   not present, they must provide a fallback strategy. For example, Glance can
   use the filesystem in place of Swift for its backend store.
  </p><p>
   In Ironic, agent-based drivers require Swift. If it is not present, it is
   necessary to disable access to this Ironic feature in the control plane. The
   <code class="literal">enable_agent_driver</code> flag has been added to the Ironic
   configuration data and can have values of <code class="literal">true</code> or
   <code class="literal">false</code>. Setting this flag to <code class="literal">false</code> will
   disable Swift configurations and the agent based drivers in the Ironic
   control plane.
  </p></div><div class="sect2" id="idm139651560869248"><div class="titlepage"><div><div><h3 class="title" id="idm139651560869248"><span class="number">14.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Instance Provisioning</span> <a title="Permalink" class="permalink" href="#idm139651560869248">#</a></h3></div></div></div><p>
   In a multiple control plane cloud setup, changes for Glance container name
   in the Swift namespace of <code class="literal">ironic-conductor.conf</code>
   introduces a conflict with the one in <code class="literal">glance-api.conf</code>.
   Provisioning with agent-based drivers requires the container name to be the
   same in Ironic and Glance. Hence, on instance provisioning with agent-based
   drivers (Swift-enabled), the agent is not able to fetch the images from
   Glance store and fails at that point.
  </p><p>
   You can resolve this issue using the following steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Copy the value of <code class="literal">swift_store_container</code> from the file
     <code class="literal">/opt/stack/service/glance-api/etc/glance-api.conf</code>
    </p></li><li class="step "><p>
     Log in to the Cloud Lifecycle Manager and use the value for
     <code class="literal">swift_container</code> in glance namespace of
     <code class="literal">~/scratch/ansible/next/ardana/ansible/roles/ironic-common/templates/ironic-conductor.conf.j2</code>
    </p></li><li class="step "><p>
     Run the following playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml</pre></div></li></ol></div></div></div></div><div class="sect1" id="ironic-provisioning"><div class="titlepage"><div><div><h2 class="title" id="ironic-provisioning"><span class="number">14.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Provisioning Bare-Metal Nodes with Flat Network Model</span> <a title="Permalink" class="permalink" href="#ironic-provisioning">#</a></h2></div></div></div><div id="idm139651560857920" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
   Providing bare-metal resources to an untrusted third party is not advised
   as a malicious user can potentially modify hardware firmware.
  </p></div><div id="idm139651560856896" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
   The steps outlined in <a class="xref" href="#ironic-tls" title="14.1.7. TLS Certificates with Ironic Python Agent (IPA) Images">Section 14.1.7, “TLS Certificates with Ironic Python Agent (IPA) Images”</a>
   <span class="emphasis"><em>must</em></span> be performed.
  </p></div><p>
  A number of drivers are available to provision and manage bare-metal
  machines. The drivers are named based on the deployment mode and the power
  management interface. <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> has been tested with the following drivers:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    agent_ilo
   </p></li><li class="listitem "><p>
    agent_ipmi
   </p></li><li class="listitem "><p>
    pxe_ilo
   </p></li><li class="listitem "><p>
    pxe_ipmi
   </p></li></ul></div><p>
  Before you start, you should be aware that:
 </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
    Node Cleaning is enabled for all the drivers in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>.
   </p></li><li class="listitem "><p>
    Node parameter settings must have matching flavors in terms of
    <code class="literal">cpus</code>, <code class="literal">local_gb</code>, and
    <code class="literal">memory_mb</code>, <code class="literal">boot_mode</code> and
    <code class="literal">cpu_arch</code>.
   </p></li><li class="listitem "><p>
    It is advisable that nodes enrolled for ipmitool drivers are pre-validated
    in terms of BIOS settings, in terms of boot mode, prior to setting
    capabilities.
   </p></li><li class="listitem "><p>
    Network cabling and interface layout should also be pre-validated in any
    given particular boot mode or configuration that is registered.
   </p></li><li class="listitem "><p>
    The use of <code class="literal">agent_</code> drivers is predicated upon Glance
    images being backed by a Swift image store, specifically the need for the
    temporary file access features. Using the file system as a Glance back-end
    image store means that the <code class="literal">agent_</code> drivers cannot be
    used.
   </p></li><li class="listitem "><p>
    Manual Cleaning (RAID) and Node inspection is supported by ilo drivers
    (<code class="literal">agent_ilo</code> and <code class="literal">pxe_ilo)</code>
   </p></li></ol></div><div class="sect2" id="sec.ironic-provision.image"><div class="titlepage"><div><div><h3 class="title" id="sec.ironic-provision.image"><span class="number">14.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Supplied Images</span> <a title="Permalink" class="permalink" href="#sec.ironic-provision.image">#</a></h3></div></div></div><p>
   As part of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale Ironic Cloud installation,
   Ironic Python Agent (IPA) images are supplied and loaded into Glance.
   To see the images that have been loaded, execute the following commands on
   the deployer node:
  </p><div class="verbatim-wrap"><pre class="screen">$ source ~/service.osrc
glance image-list</pre></div><p>
   This will produce output like the following example, showing three images
   that have been added by Ironic:
  </p><div class="verbatim-wrap"><pre class="screen">+------------------+--------------------------+-----------------------------+
| ID               | Name                     |                             |
+------------------+--------------------------+-----------------------------+
| <em class="replaceable ">DEPLOY_UUID</em>      | ir-deploy-iso-ARDANA5.0     |
| <em class="replaceable ">KERNEL_UUID</em>      | ir-deploy-kernel-ARDANA5.0  |
| <em class="replaceable ">RAMDISK_UUID</em>     | ir-deploy-ramdisk-ARDANA5.0 |
+------------------+--------------------------+-----------------------------+</pre></div><p>
   The <code class="literal">ir-deploy-ramdisk</code> image is a traditional boot ramdisk
   used by the <code class="literal">agent_ipmitool</code>,
   <code class="literal">pxe_ipmitool</code>, and <code class="literal">pxe_ilo</code> drivers
   while <code class="literal">ir-deploy-iso</code> is an ISO image that is supplied as
   virtual media to the host when using the <code class="literal">agent_ilo</code>
   driver.
  </p></div><div class="sect2" id="sec.ironic-provision.provision"><div class="titlepage"><div><div><h3 class="title" id="sec.ironic-provision.provision"><span class="number">14.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Provisioning a Node</span> <a title="Permalink" class="permalink" href="#sec.ironic-provision.provision">#</a></h3></div></div></div><p>
   The information required to provision a node varies slightly depending on
   the driver used. In general the following details are required.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Network access information and credentials to connect to the management
     interface of the node.
    </p></li><li class="listitem "><p>
     Sufficient properties to allow for Nova flavor matching.
    </p></li><li class="listitem "><p>
     A deployment image to perform the actual deployment of the guest operating
     system to the bare-metal node.
    </p></li></ul></div><p>
   A combination of the <code class="literal">ironic node-create</code> and
   <code class="literal">ironic node-update</code> commands are used for registering a
   node's characteristics with the Ironic service. In particular,
   <code class="literal">ironic node-update &lt;nodeid&gt;
   <em class="replaceable ">add</em></code> and <code class="literal">ironic node-update
   &lt;nodeid&gt; <em class="replaceable ">replace</em></code> can be used to
   modify the properties of a node after it has been created while
   <code class="literal">ironic node-update &lt;nodeid&gt;
   <em class="replaceable ">remove</em></code> will remove a property.
  </p></div><div class="sect2" id="sec.ironic-provision.create-ilo"><div class="titlepage"><div><div><h3 class="title" id="sec.ironic-provision.create-ilo"><span class="number">14.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a Node Using <code class="command">agent_ilo</code></span> <a title="Permalink" class="permalink" href="#sec.ironic-provision.create-ilo">#</a></h3></div></div></div><p>
   If you want to use a boot mode of BIOS as opposed to UEFI, then you need to
   ensure that the boot mode has been set correctly on the IPMI:
  </p><p>
   While the iLO driver can automatically set a node to boot in UEFI mode via
   the <code class="literal">boot_mode</code> defined capability, it cannot set BIOS boot
   mode once UEFI mode has been set.
  </p><p>
   Use the <code class="literal">ironic node-create</code> command to specify the
   <code class="literal">agent_ilo</code> driver, network access and credential
   information for the IPMI, properties of the node and the Glance ID of the
   supplied ISO IPA image. Note that memory size is specified in megabytes while
   disk size is specified in gigabytes.
  </p><div class="verbatim-wrap"><pre class="screen">ironic node-create -d agent_ilo -i ilo_address=<em class="replaceable ">IP_ADDRESS</em> -i \
  ilo_username=Administrator -i ilo_password=<em class="replaceable ">PASSWORD</em> \
  -p cpus=2 -p cpu_arch=x86_64 -p memory_mb=64000 -p local_gb=99 \
  -i ilo_deploy_iso=<em class="replaceable ">DEPLOY_UUID</em></pre></div><p>
   This will generate output similar to the following:
  </p><div class="verbatim-wrap"><pre class="screen">+--------------+---------------------------------------------------------------+
| Property     | Value                                                         |
+--------------+---------------------------------------------------------------+
| uuid         | <em class="replaceable ">NODE_UUID</em>                                                     |
| driver_info  | {u'ilo_address': u'<em class="replaceable ">IP_ADDRESS</em>', u'ilo_password': u'******',   |
|              | u'ilo_deploy_iso': u'<em class="replaceable ">DEPLOY_UUID</em>',                            |
|              | u'ilo_username': u'Administrator'}                            |
| extra        | {}                                                            |
| driver       | agent_ilo                                                     |
| chassis_uuid |                                                               |
| properties   | {u'memory_mb': 64000, u'local_gb': 99, u'cpus': 2,            |
|              | u'cpu_arch': u'x86_64'}                                       |
| name         | None                                                          |
+--------------+---------------------------------------------------------------+</pre></div><p>
   Now update the node with <code class="literal">boot_mode</code> and
   <code class="literal">boot_option</code> properties:
  </p><div class="verbatim-wrap"><pre class="screen">ironic node-update <em class="replaceable ">NODE_UUID</em> add \
  properties/capabilities="boot_mode:bios,boot_option:local"</pre></div><p>
   The <code class="literal">ironic node-update</code> command returns details for all of
   the node's characteristics.
  </p><div class="verbatim-wrap"><pre class="screen">+------------------------+------------------------------------------------------------------+
| Property               | Value                                                            |
+------------------------+------------------------------------------------------------------+
| target_power_state     | None                                                             |
| extra                  | {}                                                               |
| last_error             | None                                                             |
| updated_at             | None                                                             |
| maintenance_reason     | None                                                             |
| provision_state        | available                                                        |
| clean_step             | {}                                                               |
| uuid                   | <em class="replaceable ">NODE_UUID</em>                                                        |
| console_enabled        | False                                                            |
| target_provision_state | None                                                             |
| provision_updated_at   | None                                                             |
| maintenance            | False                                                            |
| inspection_started_at  | None                                                             |
| inspection_finished_at | None                                                             |
| power_state            | None                                                             |
| driver                 | agent_ilo                                                        |
| reservation            | None                                                             |
| properties             | {u'memory_mb': 64000, u'cpu_arch': u'x86_64', u'local_gb': 99,   |
|                        | u'cpus': 2, u'capabilities': u'boot_mode:bios,boot_option:local'}|
| instance_uuid          | None                                                             |
| name                   | None                                                             |
| driver_info            | {u'ilo_address': u'10.1.196.117', u'ilo_password': u'******',    |
|                        | u'ilo_deploy_iso': u'<em class="replaceable ">DEPLOY_UUID</em>',                               |
|                        | u'ilo_username': u'Administrator'}                               |
| created_at             | 2016-03-11T10:17:10+00:00                                        |
| driver_internal_info   | {}                                                               |
| chassis_uuid           |                                                                  |
| instance_info          | {}                                                               |
+------------------------+------------------------------------------------------------------+</pre></div></div><div class="sect2" id="sec.ironic-provision.create-ipmi"><div class="titlepage"><div><div><h3 class="title" id="sec.ironic-provision.create-ipmi"><span class="number">14.3.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a Node Using <code class="command">agent_ipmi</code></span> <a title="Permalink" class="permalink" href="#sec.ironic-provision.create-ipmi">#</a></h3></div></div></div><p>
   Use the <code class="literal">ironic node-create</code> command to specify the
   <code class="literal">agent_ipmi</code> driver, network access and credential
   information for the IPMI, properties of the node and the Glance IDs of the
   supplied kernel and ramdisk images. Note that memory size is specified in
   megabytes while disk size is specified in gigabytes.
  </p><div class="verbatim-wrap"><pre class="screen">ironic node-create -d <span class="bold"><strong>agent_ipmitool</strong></span> \
  -i ipmi_address=<em class="replaceable ">IP_ADDRESS</em> \
  -i ipmi_username=Administrator -i ipmi_password=<em class="replaceable ">PASSWORD</em> \
  -p cpus=2 -p memory_mb=64000 -p local_gb=99 -p cpu_arch=x86_64 \
  -i deploy_kernel=<em class="replaceable ">KERNEL_UUID</em> \
  -i deploy_ramdisk=<em class="replaceable ">RAMDISK_UUID</em></pre></div><p>
   This will generate output similar to the following:
  </p><div class="verbatim-wrap"><pre class="screen">+--------------+-----------------------------------------------------------------------+
| Property     | Value                                                                 |
+--------------+-----------------------------------------------------------------------+
| uuid         | <em class="replaceable ">NODE2_UUID</em>                                                            |
| driver_info  | {u'deploy_kernel': u'<em class="replaceable ">KERNEL_UUID</em>',                                    |
|              | u'ipmi_address': u'<em class="replaceable ">IP_ADDRESS</em>', u'ipmi_username': u'Administrator',   |
|              | u'ipmi_password': u'******',                                          |
|              | u'deploy_ramdisk': u'<em class="replaceable ">RAMDISK_UUID</em>'}                                   |
| extra        | {}                                                                    |
| driver       | agent_ipmitool                                                        |
| chassis_uuid |                                                                       |
| properties   | {u'memory_mb': 64000, u'cpu_arch': u'x86_64', u'local_gb': 99,        |
|              | u'cpus': 2}                                                           |
| name         | None                                                                  |
+--------------+-----------------------------------------------------------------------+</pre></div><p>
   Now update the node with <code class="literal">boot_mode</code> and
   <code class="literal">boot_option</code> properties:
  </p><div class="verbatim-wrap"><pre class="screen">ironic node-update <em class="replaceable ">NODE_UUID</em> add \
  properties/capabilities="boot_mode:bios,boot_option:local"</pre></div><p>
   The <code class="literal">ironic node-update</code> command returns details for all of
   the node's characteristics.
  </p><div class="verbatim-wrap"><pre class="screen">+------------------------+-----------------------------------------------------------------+
| Property               | Value                                                           |
+------------------------+-----------------------------------------------------------------+
| target_power_state     | None                                                            |
| extra                  | {}                                                              |
| last_error             | None                                                            |
| updated_at             | None                                                            |
| maintenance_reason     | None                                                            |
| provision_state        | available                                                       |
| clean_step             | {}                                                              |
| uuid                   | <em class="replaceable ">NODE2_UUID</em>                                                      |
| console_enabled        | False                                                           |
| target_provision_state | None                                                            |
| provision_updated_at   | None                                                            |
| maintenance            | False                                                           |
| inspection_started_at  | None                                                            |
| inspection_finished_at | None                                                            |
| power_state            | None                                                            |
| driver                 | agent_ipmitool                                                  |
| reservation            | None                                                            |
| properties             | {u'memory_mb': 64000, u'cpu_arch': u'x86_64',                   |
|                        | u'local_gb': 99, u'cpus': 2,                                    |
|                        | u'capabilities': u'boot_mode:bios,boot_option:local'}           |
| instance_uuid          | None                                                            |
| name                   | None                                                            |
| driver_info            | {u'ipmi_password': u'******', u'ipmi_address': u'<em class="replaceable ">IP_ADDRESS</em>',   |
|                        | u'ipmi_username': u'Administrator', u'deploy_kernel':           |
|                        | u'<em class="replaceable ">KERNEL_UUID</em>',                                                 |
|                        | u'deploy_ramdisk': u'<em class="replaceable ">RAMDISK_UUID</em>'}                             |
| created_at             | 2016-03-11T14:19:18+00:00                                       |
| driver_internal_info   | {}                                                              |
| chassis_uuid           |                                                                 |
| instance_info          | {}                                                              |
+------------------------+-----------------------------------------------------------------+</pre></div><p>
   For more information on node enrollment, see the <span class="productname">OpenStack</span> documentation at
   <a class="link" href="http://docs.openstack.org/developer/ironic/deploy/install-guide.html#enrollment" target="_blank">http://docs.openstack.org/developer/ironic/deploy/install-guide.html#enrollment</a>.
  </p></div><div class="sect2" id="sec.ironic-provision.flavor"><div class="titlepage"><div><div><h3 class="title" id="sec.ironic-provision.flavor"><span class="number">14.3.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a Flavor</span> <a title="Permalink" class="permalink" href="#sec.ironic-provision.flavor">#</a></h3></div></div></div><p>
   Nova uses flavors when fulfilling requests for bare-metal nodes. The Nova
   scheduler attempts to match the requested flavor against the properties of
   the created Ironic nodes. So an administrator needs to set up flavors that
   correspond to the available bare-metal nodes using the command
   <code class="command">nova flavor-create</code>:
  </p><div class="verbatim-wrap"><pre class="screen">nova flavor-create bmtest auto 64000  99 2

+----------------+--------+--------+------+-----------+------+-------+-------------+-----------+
| ID             | Name   | Mem_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public |
+----------------+--------+--------+------+-----------+------+-------+-------------+-----------+
| 645de0...b1348 | bmtest | 64000  | 99   | 0         |      | 2     | 1.0         | True      |
+----------------+--------+--------+------+-----------+------+-------+-------------+-----------+</pre></div><p>
   To see a list of all the available flavors, run <code class="command">nova
   flavor-list</code>:
  </p><div class="verbatim-wrap"><pre class="screen">nova flavor-list

+-------------+--------------+--------+------+-----------+------+-------+--------+-----------+
| ID          | Name         | Mem_MB | Disk | Ephemeral | Swap | VCPUs |  RXTX  | Is_Public |
|             |              |        |      |           |      |       | Factor |           |
+-------------+--------------+--------+------+-----------+------+-------+--------+-----------+
| 1           | m1.tiny      | 512    | 1    | 0         |      | 1     | 1.0    | True      |
| 2           | m1.small     | 2048   | 20   | 0         |      | 1     | 1.0    | True      |
| 3           | m1.medium    | 4096   | 40   | 0         |      | 2     | 1.0    | True      |
| 4           | m1.large     | 8192   | 80   | 0         |      | 4     | 1.0    | True      |
| 5           | m1.xlarge    | 16384  | 160  | 0         |      | 8     | 1.0    | True      |
| 6           | m1.baremetal | 4096   | 80   | 0         |      | 2     | 1.0    | True      |
| 645d...1348 | bmtest       | 64000  | 99   | 0         |      | 2     | 1.0    | True      |
+-------------+--------------+--------+------+-----------+------+-------+--------+-----------+</pre></div><p>
   Now set the CPU architecture and boot mode and boot option capabilities:
  </p><div class="verbatim-wrap"><pre class="screen">nova flavor-key 645de08d-2bc6-43f1-8a5f-2315a75b1348 set cpu_arch=x86_64
nova flavor-key 645de08d-2bc6-43f1-8a5f-2315a75b1348 set capabilities:boot_option="local"
nova flavor-key 645de08d-2bc6-43f1-8a5f-2315a75b1348 set capabilities:boot_mode="bios"</pre></div><p>
   For more information on flavor creation, see the <span class="productname">OpenStack</span> documentation at
   <a class="link" href="http://docs.openstack.org/developer/ironic/deploy/install-guide.html#flavor-creation" target="_blank">http://docs.openstack.org/developer/ironic/deploy/install-guide.html#flavor-creation</a>.
  </p></div><div class="sect2" id="sec.ironic-provision.net"><div class="titlepage"><div><div><h3 class="title" id="sec.ironic-provision.net"><span class="number">14.3.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a Network Port</span> <a title="Permalink" class="permalink" href="#sec.ironic-provision.net">#</a></h3></div></div></div><p>
   Register the MAC addresses of all connected physical network interfaces
   intended for use with the bare-metal node.
  </p><div class="verbatim-wrap"><pre class="screen">ironic port-create -a 5c:b9:01:88:f0:a4 -n ea7246fd-e1d6-4637-9699-0b7c59c22e67</pre></div></div><div class="sect2" id="sec.ironic-provision.glance-image"><div class="titlepage"><div><div><h3 class="title" id="sec.ironic-provision.glance-image"><span class="number">14.3.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a Glance Image</span> <a title="Permalink" class="permalink" href="#sec.ironic-provision.glance-image">#</a></h3></div></div></div><p>
   You can create a complete disk image using the instructions at
   <a class="xref" href="#sec.ironic-provision.kiwi" title="14.3.11. Building Glance Images Using KIWI">Section 14.3.11, “Building Glance Images Using KIWI”</a>.
  </p><p>
   The image you create can then be loaded into Glance:
  </p><div class="verbatim-wrap"><pre class="screen">glance image-create --name='leap' --disk-format=raw \
  --container-format=bare \
  --file /tmp/myimage/LimeJeOS-Leap-42.3.x86_64-1.42.3.raw

+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| checksum         | 45a4a06997e64f7120795c68beeb0e3c     |
| container_format | bare                                 |
| created_at       | 2018-02-17T10:42:14Z                 |
| disk_format      | raw                                  |
| id               | <span class="bold"><strong>17e4915a-ada0-4b95-bacf-ba67133f39a7</strong></span> |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | leap                                 |
| owner            | 821b7bb8148f439191d108764301af64     |
| protected        | False                                |
| size             | 372047872                            |
| status           | active                               |
| tags             | []                                   |
| updated_at       | 2018-02-17T10:42:23Z                 |
| virtual_size     | None                                 |
| visibility       | private                              |
+------------------+--------------------------------------+</pre></div><p>
   This image will subsequently be used to boot the bare-metal node.
  </p></div><div class="sect2" id="sec.ironic-provision.key"><div class="titlepage"><div><div><h3 class="title" id="sec.ironic-provision.key"><span class="number">14.3.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Generating a Key Pair</span> <a title="Permalink" class="permalink" href="#sec.ironic-provision.key">#</a></h3></div></div></div><p>
   Create a key pair that you will use when you login to the newly booted node:
  </p><div class="verbatim-wrap"><pre class="screen">nova keypair-add <span class="bold"><strong>ironic_kp</strong></span> &gt; ironic_kp.pem</pre></div></div><div class="sect2" id="sec.ironic-provision.neutron-id"><div class="titlepage"><div><div><h3 class="title" id="sec.ironic-provision.neutron-id"><span class="number">14.3.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Determining the Neutron Network ID</span> <a title="Permalink" class="permalink" href="#sec.ironic-provision.neutron-id">#</a></h3></div></div></div><div class="verbatim-wrap"><pre class="screen">neutron net-list

+---------------+----------+----------------------------------------------------+
| id            | name     | subnets                                            |
+---------------+----------+----------------------------------------------------+
| <span class="bold"><strong>c0102...1ca8c </strong></span>| flat-net | 709ee2a1-4110-4b26-ba4d-deb74553adb9 192.3.15.0/24 |
+---------------+----------+----------------------------------------------------+</pre></div></div><div class="sect2" id="sec.ironic-provision.boot"><div class="titlepage"><div><div><h3 class="title" id="sec.ironic-provision.boot"><span class="number">14.3.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Booting the Node</span> <a title="Permalink" class="permalink" href="#sec.ironic-provision.boot">#</a></h3></div></div></div><p>
   Before booting, it is advisable to power down the node:
  </p><div class="verbatim-wrap"><pre class="screen">ironic node-set-power-state ea7246fd-e1d6-4637-9699-0b7c59c22e67 off</pre></div><p>
   You can now boot the bare-metal node with the information compiled in the
   preceding steps, using the Neutron network ID, the whole disk image ID, the
   matching flavor and the key name:
  </p><div class="verbatim-wrap"><pre class="screen">nova boot --nic net-id=c010267c-9424-45be-8c05-99d68531ca8c \
  --image 17e4915a-ada0-4b95-bacf-ba67133f39a7 \
  --flavor 645de08d-2bc6-43f1-8a5f-2315a75b1348 \
  --key-name ironic_kp leap</pre></div><p>
   This command returns information about the state of the node that is booting:
  </p><div class="verbatim-wrap"><pre class="screen">+--------------------------------------+------------------------+
| Property                             | Value                  |
+--------------------------------------+------------------------+
| OS-EXT-AZ:availability_zone          |                        |
| OS-EXT-SRV-ATTR:host                 | -                      |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | -                      |
| OS-EXT-SRV-ATTR:instance_name        | instance-00000001      |
| OS-EXT-STS:power_state               | 0                      |
| OS-EXT-STS:task_state                | scheduling             |
| OS-EXT-STS:vm_state                  | building               |
| OS-SRV-USG:launched_at               | -                      |
| OS-SRV-USG:terminated_at             | -                      |
| accessIPv4                           |                        |
| accessIPv6                           |                        |
| adminPass                            | adpHw3KKTjHk           |
| config_drive                         |                        |
| created                              | 2018-03-11T11:00:28Z   |
| flavor                               | bmtest (645de...b1348) |
| hostId                               |                        |
| id                                   | a9012...3007e          |
| image                                | leap (17e49...f39a7)   |
| key_name                             | ironic_kp              |
| metadata                             | {}                     |
| name                                 | leap                   |
| os-extended-volumes:volumes_attached | []                     |
| progress                             | 0                      |
| security_groups                      | default                |
| status                               | BUILD                  |
| tenant_id                            | d53bcaf...baa60dd      |
| updated                              | 2016-03-11T11:00:28Z   |
| user_id                              | e580c64...4aaf990      |
+--------------------------------------+------------------------+</pre></div><p>
   The boot process can take up to 10 minutes. Monitor the progress with the
   IPMI console or with <code class="literal">nova list</code>, <code class="literal">nova show
   &lt;nova_node_id&gt;</code>, and <code class="literal">ironic node-show
   &lt;ironic_node_id&gt;</code> commands.
  </p><div class="verbatim-wrap"><pre class="screen">nova list

+---------------+--------+--------+------------+-------------+----------------------+
| ID            | Name   | Status | Task State | Power State | Networks             |
+---------------+--------+--------+------------+-------------+----------------------+
| a9012...3007e | leap   | BUILD  | spawning   | NOSTATE     | flat-net=192.3.15.12 |
+---------------+--------+--------+------------+-------------+----------------------+</pre></div><p>
   During the boot procedure, a login prompt will appear for SLES:
  </p><p>
   Ignore this login screen and wait for the login screen of your target
   operating system to appear:
  </p><p>
   If you now run the command <code class="command">nova list</code>, it should show the
   node in the running state:
  </p><div class="verbatim-wrap"><pre class="screen">nova list
+---------------+--------+--------+------------+-------------+----------------------+
| ID            | Name   | Status | Task State | Power State | Networks             |
+---------------+--------+--------+------------+-------------+----------------------+
| a9012...3007e | leap   | ACTIVE | -          | Running     | flat-net=<span class="bold"><strong>192.3.15.14</strong></span> |
+---------------+--------+--------+------------+-------------+----------------------+</pre></div><p>
   You can now log in to the booted node using the key you generated earlier.
   (You may be prompted to change the permissions of your private key files, so
   that they are not accessible by others).
  </p><div class="verbatim-wrap"><pre class="screen">ssh leap@192.3.15.14 -i ironic_kp.pem</pre></div></div><div class="sect2" id="sec.ironic-provision.kiwi"><div class="titlepage"><div><div><h3 class="title" id="sec.ironic-provision.kiwi"><span class="number">14.3.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Building Glance Images Using KIWI</span> <a title="Permalink" class="permalink" href="#sec.ironic-provision.kiwi">#</a></h3></div></div></div><p>
   The following sections show you how to create your own images using KIWI,
   the command line utility to build Linux system appliances. For information
   on installing KIWI, see <a class="link" href="https://suse.github.io/kiwi/installation.html" target="_blank">https://suse.github.io/kiwi/installation.html</a>.
  </p><p>
   KIWI creates images in a two-step process:
  </p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>
     The <code class="literal">prepare</code> operation generates an unpacked image tree
     using the information provided in the image description.
    </p></li><li class="listitem "><p>
     The <code class="literal">create</code> operation creates the packed image based on
     the unpacked image and the information provided in the configuration file
     (<code class="filename">config.xml</code>).
    </p></li></ol></div><p>
   Instructions for installing KIWI are available at
   <a class="link" href="https://suse.github.io/kiwi/installation.html" target="_blank">https://suse.github.io/kiwi/installation.html</a>.
  </p><p>
   Image creation with KIWI is automated and does not require any user
   interaction. The information required for the image creation process is
   provided by the image description.
  </p><p>
   To use and run KIWI requires:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     A recent Linux distribution such as:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       openSUSE Leap 42.3
      </p></li><li class="listitem "><p>
       SUSE Linux Enterprise 12 SP3
      </p></li><li class="listitem "><p>
       openSUSE Tumbleweed
      </p></li></ul></div></li><li class="listitem "><p>
     Enough free disk space to build and store the image (a minimum of 10 GB is
     recommended).
    </p></li><li class="listitem "><p>
     Python version 2.7, 3.4 or higher. KIWI supports both Python 2 and 3
     versions
    </p></li><li class="listitem "><p>
     Git (package <span class="package">git-core</span>) to clone a repository.
    </p></li><li class="listitem "><p>
     Virtualization technology to start the image (QEMU is recommended).
    </p></li></ul></div></div><div class="sect2" id="sec.ironic-provision.opensuse"><div class="titlepage"><div><div><h3 class="title" id="sec.ironic-provision.opensuse"><span class="number">14.3.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating an openSUSE Image with KIWI</span> <a title="Permalink" class="permalink" href="#sec.ironic-provision.opensuse">#</a></h3></div></div></div><p>
   The following example shows how to build an openSUSE Leap image that is
   ready to run in QEMU.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Retrieve the example image descriptions.
    </p><div class="verbatim-wrap"><pre class="screen">git clone https://github.com/SUSE/kiwi-descriptions</pre></div></li><li class="step "><p>
     Build the image with KIWI:
    </p><div class="verbatim-wrap"><pre class="screen">sudo kiwi-ng --type vmx system build \
  --description kiwi-descriptions/suse/x86_64/suse-leap-42.3-JeOS \
  --target-dir /tmp/myimage</pre></div><p>
     A <code class="filename">.raw</code> image will be built in the
     <code class="filename">/tmp/myimage</code> directory.
    </p></li><li class="step "><p>
     Test the live image with QEMU:
    </p><div class="verbatim-wrap"><pre class="screen">qemu \
  -drive file=LimeJeOS-Leap-42.3.x86_64-1.42.3.raw,format=raw,if=virtio \
  -m 4096</pre></div></li><li class="step "><p>
     With a successful test, the image is complete.
    </p></li></ol></div></div><p>
   By default, KIWI generates a file in the <code class="filename">.raw</code> format.
   The <code class="filename">.raw</code> file is a disk image with a structure
   equivalent to a physical hard disk. <code class="filename">.raw</code> images are
   supported by any hypervisor, but are not compressed and do not offer the
   best performance.
  </p><p>
   Virtualization systems support their own formats (such as
   <code class="literal">qcow2</code> or <code class="literal">vmdk</code>) with compression and
   improved I/O performance. To build an image in a format other than
   <code class="filename">.raw</code>, add the format attribute to the type definition
   in in the preferences section of <code class="filename">config.xml</code>. Using
   <code class="literal">qcow2</code> for example:
  </p><div class="verbatim-wrap"><pre class="screen">&lt;image ...&gt;
  &lt;preferences&gt;
    &lt;type format="qcow2" .../&gt;
    ...
  &lt;/preferences&gt;
  ...
&lt;/image</pre></div><p>
   More information about KIWI is at
   <a class="link" href="https://suse.github.io/kiwi/" target="_blank">https://suse.github.io/kiwi/</a>.
  </p></div></div><div class="sect1" id="ironic-provisioning-multi-tenancy"><div class="titlepage"><div><div><h2 class="title" id="ironic-provisioning-multi-tenancy"><span class="number">14.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Provisioning Baremetal Nodes with Multi-Tenancy</span> <a title="Permalink" class="permalink" href="#ironic-provisioning-multi-tenancy">#</a></h2></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Create a network and a subnet:
   </p><div class="verbatim-wrap"><pre class="screen">$ neutron net-create guest-net-1
Created a new network:
+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | True                                 |
| availability_zone_hints   |                                      |
| availability_zones        |                                      |
| created_at                | 2017-06-10T02:49:56Z                 |
| description               |                                      |
| id                        | 256d55a6-9430-4f49-8a4c-cc5192f5321e |
| ipv4_address_scope        |                                      |
| ipv6_address_scope        |                                      |
| mtu                       | 1500                                 |
| name                      | guest-net-1                          |
| project_id                | 57b792cdcdd74d16a08fc7a396ee05b6     |
| provider:network_type     | vlan                                 |
| provider:physical_network | physnet1                             |
| provider:segmentation_id  | 1152                                 |
| revision_number           | 2                                    |
| router:external           | False                                |
| shared                    | False                                |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tags                      |                                      |
| tenant_id                 | 57b792cdcdd74d16a08fc7a396ee05b6     |
| updated_at                | 2017-06-10T02:49:57Z                 |
+---------------------------+--------------------------------------+

$ neutron  subnet-create guest-net-1 200.0.0.0/24
Created a new subnet:
+-------------------+----------------------------------------------+
| Field             | Value                                        |
+-------------------+----------------------------------------------+
| allocation_pools  | {"start": "200.0.0.2", "end": "200.0.0.254"} |
| cidr              | 200.0.0.0/24                                 |
| created_at        | 2017-06-10T02:53:08Z                         |
| description       |                                              |
| dns_nameservers   |                                              |
| enable_dhcp       | True                                         |
| gateway_ip        | 200.0.0.1                                    |
| host_routes       |                                              |
| id                | 53accf35-ae02-43ae-95d8-7b5efed18ae9         |
| ip_version        | 4                                            |
| ipv6_address_mode |                                              |
| ipv6_ra_mode      |                                              |
| name              |                                              |
| network_id        | 256d55a6-9430-4f49-8a4c-cc5192f5321e         |
| project_id        | 57b792cdcdd74d16a08fc7a396ee05b6             |
| revision_number   | 2                                            |
| service_types     |                                              |
| subnetpool_id     |                                              |
| tenant_id         | 57b792cdcdd74d16a08fc7a396ee05b6             |
| updated_at        | 2017-06-10T02:53:08Z                         |
+-------------------+----------------------------------------------+</pre></div></li><li class="step "><p>
    Review glance image list
   </p><div class="verbatim-wrap"><pre class="screen">$ glance image-list
+--------------------------------------+--------------------------+
| ID                                   | Name                     |
+--------------------------------------+--------------------------+
| 0526d2d7-c196-4c62-bfe5-a13bce5c7f39 | cirros-0.4.0-x86_64      |
+--------------------------------------+--------------------------+</pre></div></li><li class="step "><p>
    Create Ironic node
   </p><div class="verbatim-wrap"><pre class="screen">$ ironic --ironic-api-version 1.22 node-create -d agent_ipmitool \
  -n test-node-1 -i ipmi_address=192.168.9.69 -i ipmi_username=ipmi_user \
  -i ipmi_password=XXXXXXXX --network-interface neutron -p  memory_mb=4096 \
  -p cpu_arch=x86_64 -p local_gb=80 -p cpus=2 \
  -p capabilities=boot_mode:bios,boot_option:local \
  -p root_device='{"name":"/dev/sda"}' \
  -i deploy_kernel=db3d131f-2fb0-4189-bb8d-424ee0886e4c \
  -i deploy_ramdisk=304cae15-3fe5-4f1c-8478-c65da5092a2c

+-------------------+-------------------------------------------------------------------+
| Property          | Value                                                             |
+-------------------+-------------------------------------------------------------------+
| chassis_uuid      |                                                                   |
| driver            | agent_ipmitool                                                    |
| driver_info       | {u'deploy_kernel': u'db3d131f-2fb0-4189-bb8d-424ee0886e4c',       |
|                   | u'ipmi_address': u'192.168.9.69',                                 |
|                   | u'ipmi_username': u'gozer', u'ipmi_password': u'******',          |
|                   | u'deploy_ramdisk': u'304cae15-3fe5-4f1c-8478-c65da5092a2c'}       |
| extra             | {}                                                                |
| name              | test-node-1                                                       |
| network_interface | neutron                                                           |
| properties        | {u'cpu_arch': u'x86_64', u'root_device': {u'name': u'/dev/sda'},  |
|                   | u'cpus': 2, u'capabilities': u'boot_mode:bios,boot_option:local', |
|                   | u'memory_mb': 4096, u'local_gb': 80}                              |
| resource_class    | None                                                              |
| uuid              | cb4dda0d-f3b0-48b9-ac90-ba77b8c66162                              |
+-------------------+-------------------------------------------------------------------+</pre></div><p>
    ipmi_address, ipmi_username and ipmi_password are IPMI access parameters for
    baremetal Ironic node. Adjust memory_mb, cpus, local_gb to your node size
    requirements. They also need to be reflected in flavor setting (see below).
    Use capabilities boot_mode:bios for baremetal nodes operating in Legacy
    BIOS mode. For UEFI baremetal nodes, use boot_mode:uefi lookup
    deploy_kernel and deploy_ramdisk in glance image list output above.
   </p><div id="idm139651560689808" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
     Since we are using Ironic API version 1.22, node is created initial state
     <span class="bold"><strong>enroll</strong></span>. It needs to be explicitly moved
     to <span class="bold"><strong>available</strong></span> state. This behavior changed
     in API version 1.11
    </p></div></li><li class="step "><p>
    Create port
   </p><div class="verbatim-wrap"><pre class="screen">$ ironic --ironic-api-version 1.22 port-create --address f0:92:1c:05:6c:40 \
  --node cb4dda0d-f3b0-48b9-ac90-ba77b8c66162 -l switch_id=e8:f7:24:bf:07:2e -l \
  switch_info=hp59srv1-a-11b -l port_id="Ten-GigabitEthernet 1/0/34" \
  --pxe-enabled true
+-----------------------+--------------------------------------------+
| Property              | Value                                      |
+-----------------------+--------------------------------------------+
| address               | f0:92:1c:05:6c:40                          |
| extra                 | {}                                         |
| local_link_connection | {u'switch_info': u'hp59srv1-a-11b',        |
|                       | u'port_id': u'Ten-GigabitEthernet 1/0/34', |
|                       | u'switch_id': u'e8:f7:24:bf:07:2e'}        |
| node_uuid             | cb4dda0d-f3b0-48b9-ac90-ba77b8c66162       |
| pxe_enabled           | True                                       |
| uuid                  | a49491f3-5595-413b-b4a7-bb6f9abec212       |
+-----------------------+--------------------------------------------+</pre></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      for <code class="option">--address</code>, use MAC of 1st NIC of ironic baremetal
      node, which will be used for PXE boot
     </p></li><li class="listitem "><p>
      for <code class="option">--node</code>, use ironic node uuid (see above)
     </p></li><li class="listitem "><p>
      for <code class="option">-l switch_id</code>, use switch management interface MAC
      address. It can be
      retrieved by pinging switch management IP and looking up MAC address in
      'arp -l -n' command output.
     </p></li><li class="listitem "><p>
      for <code class="option">-l switch_info</code>, use switch_id from
      <code class="filename">data/ironic/ironic_config.yml</code>
      file. If you have several switch config definitions, use the right switch
      your baremetal node is connected to.
     </p></li><li class="listitem "><p>
      for -l port_id, use port ID on the switch
     </p></li></ul></div></li><li class="step "><p>
    Move ironic node to manage and then available state
   </p><div class="verbatim-wrap"><pre class="screen">$ ironic node-set-provision-state test-node-1 manage
$ ironic node-set-provision-state test-node-1 provide</pre></div></li><li class="step "><p>
    Once node is successfully moved to available state, its resources should
    be included into Nova hypervisor statistics
   </p><div class="verbatim-wrap"><pre class="screen">$ nova hypervisor-stats
+----------------------+-------+
| Property             | Value |
+----------------------+-------+
| count                | 1     |
| current_workload     | 0     |
| disk_available_least | 80    |
| free_disk_gb         | 80    |
| free_ram_mb          | 4096  |
| local_gb             | 80    |
| local_gb_used        | 0     |
| memory_mb            | 4096  |
| memory_mb_used       | 0     |
| running_vms          | 0     |
| vcpus                | 2     |
| vcpus_used           | 0     |
+----------------------+-------+</pre></div></li><li class="step "><p>
    Prepare a keypair, which will be used for logging into the node
   </p><div class="verbatim-wrap"><pre class="screen">$ nova keypair-add ironic_kp &gt; ironic_kp.pem</pre></div></li><li class="step "><p>
    Obtain user image and upload it to glance. Please refer to OpenStack
    documentation on user image creation:
    <a class="link" href="https://docs.openstack.org/project-install-guide/baremetal/draft/configure-glance-images.html" target="_blank">https://docs.openstack.org/project-install-guide/baremetal/draft/configure-glance-images.html</a>.
   </p><div id="idm139651560670160" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     Deployed images are already populated by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installer.
    </p></div><div class="verbatim-wrap"><pre class="screen">$ glance image-create --name='Ubuntu Trusty 14.04' --disk-format=qcow2 \
  --container-format=bare --file ~/ubuntu-trusty.qcow2
+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| checksum         | d586d8d2107f328665760fee4c81caf0     |
| container_format | bare                                 |
| created_at       | 2017-06-13T22:38:45Z                 |
| disk_format      | qcow2                                |
| id               | 9fdd54a3-ccf5-459c-a084-e50071d0aa39 |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | Ubuntu Trusty 14.04                  |
| owner            | 57b792cdcdd74d16a08fc7a396ee05b6     |
| protected        | False                                |
| size             | 371508736                            |
| status           | active                               |
| tags             | []                                   |
| updated_at       | 2017-06-13T22:38:55Z                 |
| virtual_size     | None                                 |
| visibility       | private                              |
+------------------+--------------------------------------+

$ glance image-list
+--------------------------------------+---------------------------+
| ID                                   | Name                      |
+--------------------------------------+---------------------------+
| 0526d2d7-c196-4c62-bfe5-a13bce5c7f39 | cirros-0.4.0-x86_64       |
| 83eecf9c-d675-4bf9-a5d5-9cf1fe9ee9c2 | ir-deploy-iso-<em class="replaceable ">EXAMPLE</em>     |
| db3d131f-2fb0-4189-bb8d-424ee0886e4c | ir-deploy-kernel-<em class="replaceable ">EXAMPLE</em>  |
| 304cae15-3fe5-4f1c-8478-c65da5092a2c | ir-deploy-ramdisk-<em class="replaceable "> EXAMPLE</em> |
| 9fdd54a3-ccf5-459c-a084-e50071d0aa39 | Ubuntu Trusty 14.04       |
+--------------------------------------+---------------------------+</pre></div></li><li class="step "><p>
    Create a baremetal flavor and set flavor keys specifying requested node
    size, architecture and boot mode. A flavor can be re-used for several nodes
    having the same size, architecture and boot mode
   </p><div class="verbatim-wrap"><pre class="screen">$ nova flavor-create m1.ironic auto 4096 80 2
+-------------+-----------+--------+------+---------+------+-------+-------------+-----------+
| ID          | Name      | Mem_MB | Disk | Ephemrl | Swap | VCPUs | RXTX_Factor | Is_Public |
+-------------+-----------+--------+------+---------+------+-------+-------------+-----------+
| ab69...87bf | m1.ironic | 4096   | 80   | 0       |      | 2     | 1.0         | True      |
+-------------+-----------+--------+------+---------+------+-------+-------------+-----------+

$ nova flavor-key ab6988...e28694c87bf set cpu_arch=x86_64
$ nova flavor-key ab6988...e28694c87bf set capabilities:boot_option="local"
$ nova flavor-key ab6988...e28694c87bf set capabilities:boot_mode="bios"</pre></div><p>
    Parameters must match parameters of ironic node above. Use
    <code class="literal">capabilities:boot_mode="bios"</code> for Legacy BIOS nodes. For
    UEFI nodes, use <code class="literal">capabilities:boot_mode="uefi"</code>
   </p></li><li class="step "><p>
    Boot the node
   </p><div class="verbatim-wrap"><pre class="screen">$ nova boot --flavor m1.ironic --image 9fdd54a3-ccf5-459c-a084-e50071d0aa39 \
  --key-name ironic_kp --nic net-id=256d55a6-9430-4f49-8a4c-cc5192f5321e \
  test-node-1
+--------------------------------------+-------------------------------------------------+
| Property                             | Value                                           |
+--------------------------------------+-------------------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                                          |
| OS-EXT-AZ:availability_zone          |                                                 |
| OS-EXT-SRV-ATTR:host                 | -                                               |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | -                                               |
| OS-EXT-SRV-ATTR:instance_name        |                                                 |
| OS-EXT-STS:power_state               | 0                                               |
| OS-EXT-STS:task_state                | scheduling                                      |
| OS-EXT-STS:vm_state                  | building                                        |
| OS-SRV-USG:launched_at               | -                                               |
| OS-SRV-USG:terminated_at             | -                                               |
| accessIPv4                           |                                                 |
| accessIPv6                           |                                                 |
| adminPass                            | XXXXXXXXXXXX                                    |
| config_drive                         |                                                 |
| created                              | 2017-06-14T21:25:18Z                            |
| flavor                               | m1.ironic (ab69881...5a-497d-93ae-6e28694c87bf) |
| hostId                               |                                                 |
| id                                   | f1a8c63e-da7b-4d9a-8648-b1baa6929682            |
| image                                | Ubuntu Trusty 14.04 (9fdd54a3-ccf5-4a0...0aa39) |
| key_name                             | ironic_kp                                       |
| metadata                             | {}                                              |
| name                                 | test-node-1                                     |
| os-extended-volumes:volumes_attached | []                                              |
| progress                             | 0                                               |
| security_groups                      | default                                         |
| status                               | BUILD                                           |
| tenant_id                            | 57b792cdcdd74d16a08fc7a396ee05b6                |
| updated                              | 2017-06-14T21:25:17Z                            |
| user_id                              | cc76d7469658401fbd4cf772278483d9                |
+--------------------------------------+-------------------------------------------------+</pre></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      for <code class="option">--image</code>, use the ID of user image created at
      previous step
     </p></li><li class="listitem "><p>
      for <code class="option">--nic net-id</code>, use ID of
      the tenant network created at the beginning
     </p></li></ul></div><div id="idm139651560651904" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
     During the node provisioning, the following is happening in the
     background:
    </p><p>
     Neutron connects to switch management interfaces and assigns provisioning
     VLAN to baremetal node port on the switch. Ironic powers up the node using
     IPMI interface. Node is booting IPA image via PXE. IPA image is writing
     provided user image onto specified root device
     (<code class="filename">/dev/sda</code>) and powers node
     down. Neutron connects to switch management interfaces and assigns tenant
     VLAN to baremetal node port on the switch. A VLAN ID is selected from
     provided range. Ironic powers up the node using IPMI interface. Node is
     booting user image from disk.
    </p></div></li><li class="step "><p>
    Once provisioned, node will join the private tenant network. Access to
    private network from other networks is defined by switch configuration.
   </p></li></ol></div></div></div><div class="sect1" id="ironic-system-details"><div class="titlepage"><div><div><h2 class="title" id="ironic-system-details"><span class="number">14.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">View Ironic System Details</span> <a title="Permalink" class="permalink" href="#ironic-system-details">#</a></h2></div></div></div><div class="sect2" id="idm139651560645616"><div class="titlepage"><div><div><h3 class="title" id="idm139651560645616"><span class="number">14.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">View details about the server using <code class="command">nova show &lt;nova-node-id&gt;</code></span> <a title="Permalink" class="permalink" href="#idm139651560645616">#</a></h3></div></div></div><div class="verbatim-wrap"><pre class="screen">nova show a90122ce-bba8-496f-92a0-8a7cb143007e

+--------------------------------------+-----------------------------------------------+
| Property                             | Value                                         |
+--------------------------------------+-----------------------------------------------+
| OS-EXT-AZ:availability_zone          | nova                                          |
| OS-EXT-SRV-ATTR:host                 | ardana-cp1-ir-compute0001-mgmt                |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | ea7246fd-e1d6-4637-9699-0b7c59c22e67          |
| OS-EXT-SRV-ATTR:instance_name        | instance-0000000a                             |
| OS-EXT-STS:power_state               | 1                                             |
| OS-EXT-STS:task_state                | -                                             |
| OS-EXT-STS:vm_state                  | active                                        |
| OS-SRV-USG:launched_at               | 2016-03-11T12:26:25.000000                    |
| OS-SRV-USG:terminated_at             | -                                             |
| accessIPv4                           |                                               |
| accessIPv6                           |                                               |
| config_drive                         |                                               |
| created                              | 2016-03-11T12:17:54Z                          |
| flat-net network                     | 192.3.15.14                                   |
| flavor                               | bmtest (645de08d-2bc6-43f1-8a5f-2315a75b1348) |
| hostId                               | ecafa4f40eb5f72f7298...3bad47cbc01aa0a076114f |
| id                                   | a90122ce-bba8-496f-92a0-8a7cb143007e          |
| image                                | ubuntu (17e4915a-ada0-4b95-bacf-ba67133f39a7) |
| key_name                             | ironic_kp                                     |
| metadata                             | {}                                            |
| name                                 | ubuntu                                        |
| os-extended-volumes:volumes_attached | []                                            |
| progress                             | 0                                             |
| security_groups                      | default                                       |
| status                               | ACTIVE                                        |
| tenant_id                            | d53bcaf15afb4cb5aea3adaedbaa60dd              |
| updated                              | 2016-03-11T12:26:26Z                          |
| user_id                              | e580c645bfec4faeadef7dbd24aaf990              |
+--------------------------------------+-----------------------------------------------+</pre></div></div><div class="sect2" id="idm139651560640752"><div class="titlepage"><div><div><h3 class="title" id="idm139651560640752"><span class="number">14.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">View detailed information about a node using <code class="command">ironic node-show &lt;ironic-node-id&gt;</code></span> <a title="Permalink" class="permalink" href="#idm139651560640752">#</a></h3></div></div></div><div class="verbatim-wrap"><pre class="screen">ironic node-show  ea7246fd-e1d6-4637-9699-0b7c59c22e67

+------------------------+--------------------------------------------------------------------------+
| Property               | Value                                                                    |
+------------------------+--------------------------------------------------------------------------+
| target_power_state     | None                                                                     |
| extra                  | {}                                                                       |
| last_error             | None                                                                     |
| updated_at             | 2016-03-11T12:26:25+00:00                                                |
| maintenance_reason     | None                                                                     |
| provision_state        | active                                                                   |
| clean_step             | {}                                                                       |
| uuid                   | ea7246fd-e1d6-4637-9699-0b7c59c22e67                                     |
| console_enabled        | False                                                                    |
| target_provision_state | None                                                                     |
| provision_updated_at   | 2016-03-11T12:26:25+00:00                                                |
| maintenance            | False                                                                    |
| inspection_started_at  | None                                                                     |
| inspection_finished_at | None                                                                     |
| power_state            | power on                                                                 |
| driver                 | agent_ilo                                                                |
| reservation            | None                                                                     |
| properties             | {u'memory_mb': 64000, u'cpu_arch': u'x86_64', u'local_gb': 99,           |
|                        | u'cpus': 2, u'capabilities': u'boot_mode:bios,boot_option:local'}        |
| instance_uuid          | a90122ce-bba8-496f-92a0-8a7cb143007e                                     |
| name                   | None                                                                     |
| driver_info            | {u'ilo_address': u'10.1.196.117', u'ilo_password': u'******',            |
|                        | u'ilo_deploy_iso': u'b9499494-7db3-4448-b67f-233b86489c1f',              |
|                        | u'ilo_username': u'Administrator'}                                       |
| created_at             | 2016-03-11T10:17:10+00:00                                                |
| driver_internal_info   | {u'agent_url': u'http://192.3.15.14:9999',                               |
|                        | u'is_whole_disk_image': True, u'agent_last_heartbeat': 1457699159}       |
| chassis_uuid           |                                                                          |
| instance_info          | {u'root_gb': u'99', u'display_name': u'ubuntu', u'image_source': u       |
|                        | '17e4915a-ada0-4b95-bacf-ba67133f39a7', u'capabilities': u'{"boot_mode": |
|                        | "bios", "boot_option": "local"}', u'memory_mb': u'64000', u'vcpus':      |
|                        | u'2', u'image_url': u'http://192.168.12.2:8080/v1/AUTH_ba121db7732f4ac3a |
|                        | 50cc4999a10d58d/glance/17e4915a-ada0-4b95-bacf-ba67133f39a7?temp_url_sig |
|                        | =ada691726337805981ac002c0fbfc905eb9783ea&amp;temp_url_expires=1457699878',  |
|                        | u'image_container_format': u'bare', u'local_gb': u'99',                  |
|                        | u'image_disk_format': u'qcow2', u'image_checksum':                       |
|                        | u'2d7bb1e78b26f32c50bd9da99102150b', u'swap_mb': u'0'}                   |
+------------------------+--------------------------------------------------------------------------+</pre></div></div><div class="sect2" id="idm139651560634528"><div class="titlepage"><div><div><h3 class="title" id="idm139651560634528"><span class="number">14.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">View detailed information about a port using <code class="command">ironic port-show &lt;ironic-port-id&gt;</code></span> <a title="Permalink" class="permalink" href="#idm139651560634528">#</a></h3></div></div></div><div class="verbatim-wrap"><pre class="screen">ironic port-show a17a4ef8-a711-40e2-aa27-2189c43f0b67

+------------+-----------------------------------------------------------+
| Property   | Value                                                     |
+------------+-----------------------------------------------------------+
| node_uuid  | ea7246fd-e1d6-4637-9699-0b7c59c22e67                      |
| uuid       | a17a4ef8-a711-40e2-aa27-2189c43f0b67                      |
| extra      | {u'vif_port_id': u'82a5ab28-76a8-4c9d-bfb4-624aeb9721ea'} |
| created_at | 2016-03-11T10:40:53+00:00                                 |
| updated_at | 2016-03-11T12:17:56+00:00                                 |
| address    | 5c:b9:01:88:f0:a4                                         |
+------------+-----------------------------------------------------------+</pre></div></div><div class="sect2" id="idm139651560632064"><div class="titlepage"><div><div><h3 class="title" id="idm139651560632064"><span class="number">14.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">View detailed information about a hypervisor using <code class="command">nova hypervisor-list</code> and <code class="command">nova hypervisor-show</code></span> <a title="Permalink" class="permalink" href="#idm139651560632064">#</a></h3></div></div></div><div class="verbatim-wrap"><pre class="screen">nova hypervisor-list

+-----+--------------------------------------+-------+---------+
| ID  | Hypervisor hostname                  | State | Status  |
+-----+--------------------------------------+-------+---------+
| 541 | ea7246fd-e1d6-4637-9699-0b7c59c22e67 | up    | enabled |
+-----+--------------------------------------+-------+---------+</pre></div><div class="verbatim-wrap"><pre class="screen">nova hypervisor-show ea7246fd-e1d6-4637-9699-0b7c59c22e67

+-------------------------+--------------------------------------+
| Property                | Value                                |
+-------------------------+--------------------------------------+
| cpu_info                |                                      |
| current_workload        | 0                                    |
| disk_available_least    | 0                                    |
| free_disk_gb            | 0                                    |
| free_ram_mb             | 0                                    |
| host_ip                 | 192.168.12.6                         |
| hypervisor_hostname     | ea7246fd-e1d6-4637-9699-0b7c59c22e67 |
| hypervisor_type         | ironic                               |
| hypervisor_version      | 1                                    |
| id                      | 541                                  |
| local_gb                | 99                                   |
| local_gb_used           | 99                                   |
| memory_mb               | 64000                                |
| memory_mb_used          | 64000                                |
| running_vms             | 1                                    |
| service_disabled_reason | None                                 |
| service_host            | ardana-cp1-ir-compute0001-mgmt       |
| service_id              | 25                                   |
| state                   | up                                   |
| status                  | enabled                              |
| vcpus                   | 2                                    |
| vcpus_used              | 2                                    |
+-------------------------+--------------------------------------+</pre></div></div><div class="sect2" id="idm139651560627376"><div class="titlepage"><div><div><h3 class="title" id="idm139651560627376"><span class="number">14.5.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">View a list of all running services using <code class="command">nova service-list</code></span> <a title="Permalink" class="permalink" href="#idm139651560627376">#</a></h3></div></div></div><div class="verbatim-wrap"><pre class="screen">nova service-list

+----+------------------+-----------------------+----------+---------+-------+------------+----------+
| Id | Binary           | Host                  | Zone     | Status  | State | Updated_at | Disabled |
|    |                  |                       |          |         |       |            | Reason   |
+----+------------------+-----------------------+----------+---------+-------+------------+----------+
| 1  | nova-conductor   | ardana-cp1-c1-m1-mgmt | internal | enabled | up    | date:time  | -        |
| 7  | nova-conductor   |  " -cp1-c1-m2-mgmt    | internal | enabled | up    | date:time  | -        |
| 10 | nova-conductor   |  " -cp1-c1-m3-mgmt    | internal | enabled | up    | date:time  | -        |
| 13 | nova-scheduler   |  " -cp1-c1-m1-mgmt    | internal | enabled | up    | date:time  | -        |
| 16 | nova-scheduler   |  " -cp1-c1-m3-mgmt    | internal | enabled | up    | date:time  | -        |
| 19 | nova-scheduler   |  " -cp1-c1-m2-mgmt    | internal | enabled | up    | date:time  | -        |
| 22 | nova-consoleauth |  " -cp1-c1-m1-mgmt    | internal | enabled | up    | date:time  | -        |
| 25 | nova-compute     |  " -cp1-ir- | nova    |          | enabled | up    | date:time  | -        |
|    |                  |      compute0001-mgmt |          |         |       |            |          |
+----+------------------+-----------------------+----------+---------+-------+------------+----------+</pre></div></div></div><div class="sect1" id="ironic-toubleshooting"><div class="titlepage"><div><div><h2 class="title" id="ironic-toubleshooting"><span class="number">14.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting Ironic Installation</span> <a title="Permalink" class="permalink" href="#ironic-toubleshooting">#</a></h2></div></div></div><p>
  Sometimes the <code class="literal">nova boot</code> command does not succeed and when
  you do a <code class="literal">nova list</code>, you will see output like the
  following:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova list

+------------------+--------------+--------+------------+-------------+----------+
| ID               | Name         | Status | Task State | Power State | Networks |
+------------------+--------------+--------+------------+-------------+----------+
| ee08f82...624e5f | OpenSUSE42.3 | ERROR  | -          | NOSTATE     |          |
+------------------+--------------+--------+------------+-------------+----------+</pre></div><p>
  You should execute the <code class="literal">nova show &lt;nova-node-id&gt;</code> and
  <code class="literal">ironic node-show &lt;ironic-node-id&gt;</code> commands to get
  more information about the error.
 </p><div class="sect2" id="idm139651560617280"><div class="titlepage"><div><div><h3 class="title" id="idm139651560617280"><span class="number">14.6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Error: No valid host was found.</span> <a title="Permalink" class="permalink" href="#idm139651560617280">#</a></h3></div></div></div><p>
   The error <code class="literal">No valid host was found. There are not enough
   hosts.</code> is typically seen when performing the <code class="literal">nova
   boot</code> where there is a mismatch between the properties set on the
   node and the flavor used. For example, the output from a <code class="literal">nova
   show</code> command may look like this:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova show ee08f82e-8920-4360-be51-a3f995624e5f

+------------------------+------------------------------------------------------------------------------+
| Property               | Value                                                                        |
+------------------------+------------------------------------------------------------------------------+
| OS-EXT-AZ:             |                                                                              |
|   availability_zone    |                                                                              |
| OS-EXT-SRV-ATTR:host   | -                                                                            |
| OS-EXT-SRV-ATTR:       |                                                                              |
|   hypervisor_hostname  | -                                                                            |
| OS-EXT-SRV-ATTR:       |                                                                              |
|   instance_name        | instance-00000001                                                            |
| OS-EXT-STS:power_state | 0                                                                            |
| OS-EXT-STS:task_state  | -                                                                            |
| OS-EXT-STS:vm_state    | error                                                                        |
| OS-SRV-USG:launched_at | -                                                                            |
| OS-SRV-USG:            |                                                                              |
|    terminated_at       | -                                                                            |
| accessIPv4             |                                                                              |
| accessIPv6             |                                                                              |
| config_drive           |                                                                              |
| created                | 2016-03-11T11:00:28Z                                                         |
| fault                  | {"message": "<span class="bold"><strong>No valid host was found. There are not enough hosts             |
|                        |  available.</strong></span>", "code": 500, "details": "  File \<span class="bold"><strong>"/opt/stack/                  |
|                        |  venv/nova-20160308T002421Z/lib/python2.7/site-packages/nova/                |
|                        |  conductor/manager.py\"</strong></span>, line 739, in build_instances                        |
|                        |     request_spec, filter_properties)                                         |
|                        |   File \<span class="bold"><strong>"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/nova/scheduler/utils.py\"</strong></span>, line 343, in wrapped              |
|                        |     return func(*args, **kwargs)                                             |
|                        |   File \<span class="bold"><strong>"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/nova/scheduler/client/__init__.py\"</strong></span>, line 52,                |
|                        |     in select_destinations context, request_spec, filter_properties)         |
|                        |   File \"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/nova/scheduler/client/__init__.py\",line 37,in __run_method  |
|                        |     return getattr(self.instance, __name)(*args, **kwargs)                   |
|                        |   File \"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/nova/scheduler/client/query.py\", line 34,                   |
|                        |     in select_destinations context, request_spec, filter_properties)         |
|                        |   File \"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/nova/scheduler/rpcapi.py\", line 120, in select_destinations |
|                        |     request_spec=request_spec, filter_properties=filter_properties)          |
|                        |   File \"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/oslo_messaging/rpc/client.py\", line 158, in call            |
|                        |     retry=self.retry)                                                        |
|                        |   File \"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/oslo_messaging/transport.py\", line 90, in _send             |
|                        |     timeout=timeout, retry=retry)                                            |
|                        |   File \"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/oslo_messaging/_drivers/amqpdriver.py\", line 462, in send   |
|                        |     retry=retry)                                                             |
|                        |   File \"/opt/stack/venv/nova-20160308T002421Z/lib/python2.7/                |
|                        |   site-packages/oslo_messaging/_drivers/amqpdriver.py\", line 453, in _send  |
|                        |     raise result                                                             |
|                        | ", "created": "2016-03-11T11:00:29Z"}                                        |
| flavor                 | bmtest (645de08d-2bc6-43f1-8a5f-2315a75b1348)                                |
| hostId                 |                                                                              |
| id                     | ee08f82e-8920-4360-be51-a3f995624e5f                                         |
| image                  | opensuse (17e4915a-ada0-4b95-bacf-ba67133f39a7)                              |
| key_name               | ironic_kp                                                                    |
| metadata               | {}                                                                           |
| name                   | opensuse                                                                     |
| os-extended-volumes:   |                                                                              |
|    volumes_attached    | []                                                                           |
| status                 | ERROR                                                                        |
| tenant_id              | d53bcaf15afb4cb5aea3adaedbaa60dd                                             |
| updated                | 2016-03-11T11:00:28Z                                                         |
| user_id                | e580c645bfec4faeadef7dbd24aaf990                                             |
+------------------------+------------------------------------------------------------------------------+</pre></div><p>
   You can find more information about the error by inspecting the log file at
   <code class="literal">/var/log/nova/nova-scheduler.log</code> or alternatively by
   viewing the error location in the source files listed in the stack-trace (in
   bold above).
  </p><p>
   To find the mismatch, compare the properties of the ironic node:
  </p><div class="verbatim-wrap"><pre class="screen">+------------------------+---------------------------------------------------------------------+
| Property               | Value                                                               |
+------------------------+---------------------------------------------------------------------+
| target_power_state     | None                                                                |
| extra                  | {}                                                                  |
| last_error             | None                                                                |
| updated_at             | None                                                                |
| maintenance_reason     | None                                                                |
| provision_state        | available                                                           |
| clean_step             | {}                                                                  |
| uuid                   | ea7246fd-e1d6-4637-9699-0b7c59c22e67                                |
| console_enabled        | False                                                               |
| target_provision_state | None                                                                |
| provision_updated_at   | None                                                                |
| maintenance            | False                                                               |
| inspection_started_at  | None                                                                |
| inspection_finished_at | None                                                                |
| power_state            | None                                                                |
| driver                 | agent_ilo                                                           |
| reservation            | None                                                                |
| properties             | <span class="bold"><strong>{u'memory_mb': 64000, u'local_gb': 99, u'cpus': 2, u'capabilities':</strong></span> |
|                        | <span class="bold"><strong>u'boot_mode:bios,boot_option:local'} </strong></span>                               |
| instance_uuid          | None                                                                |
| name                   | None                                                                |
| driver_info            | {u'ilo_address': u'10.1.196.117', u'ilo_password': u'******',       |
|                        | u'ilo_deploy_iso': u'b9499494-7db3-4448-b67f-233b86489c1f',         |
|                        | u'ilo_username': u'Administrator'}                                  |
| created_at             | 2016-03-11T10:17:10+00:00                                           |
| driver_internal_info   | {}                                                                  |
| chassis_uuid           |                                                                     |
| instance_info          | {}                                                                  |
+------------------------+---------------------------------------------------------------------+</pre></div><p>
   with the flavor characteristics:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>nova flavor-show

+----------------------------+-------------------------------------------------------------------+
| Property                   | Value                                                             |
+----------------------------+-------------------------------------------------------------------+
| OS-FLV-DISABLED:disabled   | False                                                             |
| OS-FLV-EXT-DATA:ephemeral  | 0                                                                 |
| disk                       | <span class="bold"><strong>99 </strong></span>                                                               |
| extra_specs                | <span class="bold"><strong>{"capabilities:boot_option": "local", "cpu_arch": "x86_64",       |
|                            | "capabilities:boot_mode": "bios"}</strong></span>                                 |
| id                         | 645de08d-2bc6-43f1-8a5f-2315a75b1348                              |
| name                       | bmtest                                                            |
| os-flavor-access:is_public | True                                                              |
| ram                        | <span class="bold"><strong>64000</strong></span>                                                             |
| rxtx_factor                | 1.0                                                               |
| swap                       |                                                                   |
| vcpus                      | <span class="bold"><strong>2</strong></span>                                                                 |
+----------------------------+-------------------------------------------------------------------+</pre></div><p>
   In this instance, the problem is caused by the absence of the
   <span class="bold"><strong>"cpu_arch": "x86_64"</strong></span> property on the ironic
   node. This can be resolved by updating the ironic node, adding the missing
   property:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ironic node-update ea7246fd-e1d6-4637-9699-0b7c59c22e67 \
  <span class="bold"><strong>add properties/cpu_arch=x86_64</strong></span></pre></div><p>
   and then re-running the <code class="literal">nova boot</code> command.
  </p></div><div class="sect2" id="idm139651560586544"><div class="titlepage"><div><div><h3 class="title" id="idm139651560586544"><span class="number">14.6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deployment to a node fails and in "ironic node-list" command, the power_state column for the node is shown as "None"</span> <a title="Permalink" class="permalink" href="#idm139651560586544">#</a></h3></div></div></div><p>
   <span class="bold"><strong>Possible cause: </strong></span> The IPMI commands to the
   node take longer to change the power state of the server.
  </p><p>
   <span class="bold"><strong>Resolution: </strong></span> Check if the node power state
   can be changed using the following command
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ironic node-set-power-state $NODEUUID on</pre></div><p>
   If the above command succeeds and the power_state column is updated
   correctly, then the following steps are required to increase the power sync
   interval time.
  </p><p>
   On the first controller, reconfigure Ironic to increase the power sync
   interval time. In the example below, it is set to 120 seconds. This value
   may have to be tuned based on the setup.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Go to the <code class="literal">~/openstack/my_cloud/config/ironic/</code> directory
     and edit <code class="literal">ironic-conductor.conf.j2</code> to set the
     <code class="literal">sync_power_state_interval</code> value:
    </p><div class="verbatim-wrap"><pre class="screen">[conductor]
sync_power_state_interval = 120</pre></div></li><li class="step "><p>
     Save the file and then run the following playbooks:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect2" id="idm139651560574016"><div class="titlepage"><div><div><h3 class="title" id="idm139651560574016"><span class="number">14.6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Error Downloading Image</span> <a title="Permalink" class="permalink" href="#idm139651560574016">#</a></h3></div></div></div><p>
   If you encounter the error below during the deployment:
  </p><div class="verbatim-wrap"><pre class="screen">"u'message': u'Error downloading image: Download of image id 77700...96551 failed:
Image download failed for all URLs.',
u'code': 500,
u'type': u'ImageDownloadError',
u'details': u'Download of image id 77700b53-9e15-406c-b2d5-13e7d9b96551 failed:
Image download failed for all URLs.'"</pre></div><p>
   you should visit the Single Sign-On Settings in the Security page of IPMI and
   change the Single Sign-On Trust Mode setting from the default of "Trust None
   (SSO disabled)" to "Trust by Certificate".
  </p></div><div class="sect2" id="idm139651560571264"><div class="titlepage"><div><div><h3 class="title" id="idm139651560571264"><span class="number">14.6.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using <code class="literal">node-inspection</code> can cause temporary claim of IP addresses</span> <a title="Permalink" class="permalink" href="#idm139651560571264">#</a></h3></div></div></div><p>
   <span class="bold"><strong>Possible cause: </strong></span> Running
   <code class="literal">node-inspection</code> on a node discovers all the NIC ports
   including the NICs that don’t have any connectivity. This causes a
   temporary consumption of the network IPs and increased usage of the
   allocated quota. As a result, other nodes are deprived of IP addresses and
   deployments can fail.
  </p><p>
   <span class="bold"><strong>Resolution:</strong></span>You can add node properties
   manually added instead of using the inspection tool.
  </p><p>
   Note: Upgrade <code class="literal">ipmitool</code> to a version &gt;= 1.8.15 or it
   may not return detailed information about the NIC interface for
   <code class="literal">node-inspection</code>.
  </p></div><div class="sect2" id="idm139651560565344"><div class="titlepage"><div><div><h3 class="title" id="idm139651560565344"><span class="number">14.6.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Node permanently stuck in deploying state</span> <a title="Permalink" class="permalink" href="#idm139651560565344">#</a></h3></div></div></div><p>
   <span class="bold"><strong>Possible causes:</strong></span>
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Ironic conductor service associated with the node could go down.
    </p></li><li class="listitem "><p>
     There might be a properties mismatch. MAC address registered for the node
     could be incorrect.
    </p></li></ul></div><p>
   <span class="bold"><strong>Resolution:</strong></span> To recover from this
   condition, set the provision state of the node to <code class="literal">Error</code>
   and maintenance to <code class="literal">True</code>. Delete the node and re-register
   again.
  </p></div><div class="sect2" id="idm139651560558688"><div class="titlepage"><div><div><h3 class="title" id="idm139651560558688"><span class="number">14.6.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">The NICs in the baremetal node should come first in boot order</span> <a title="Permalink" class="permalink" href="#idm139651560558688">#</a></h3></div></div></div><p>
   <span class="bold"><strong>Possible causes:</strong></span> By default, the boot
   order of baremetal node is set as NIC1, HDD and NIC2. If NIC1 fails, the
   nodes starts booting from HDD and the provisioning fails.
  </p><p>
   <span class="bold"><strong>Resolution:</strong></span> Set boot order so that all the
   NICs appear before the hard disk of the baremetal as NIC1, NIC2…, HDD.
  </p></div><div class="sect2" id="idm139651560555168"><div class="titlepage"><div><div><h3 class="title" id="idm139651560555168"><span class="number">14.6.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Increase in the number of nodes can cause power commands to fail</span> <a title="Permalink" class="permalink" href="#idm139651560555168">#</a></h3></div></div></div><p>
   <span class="bold"><strong>Possible causes:</strong></span>Ironic periodically
   performs a power state sync with all the baremetal nodes. When the number of
   nodes increase, ironic does not get sufficient time to perform power
   operations.
  </p><p>
   <span class="bold"><strong>Resolution:</strong></span> The following procedure gives a
   way to increase <code class="literal">sync_power_state_interval</code>:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Edit the file
     <code class="literal">~/openstack/my_cloud/config/ironic/ironic-conductor.conf.j2</code>
     and navigate to the section for <code class="literal">[conductor]</code>
    </p></li><li class="step "><p>
     Increase the <code class="literal">sync_power_state_interval</code>. For example,
     for 100 nodes, set <code class="literal">sync_power_state_interval = 90</code> and
     save the file.
    </p></li><li class="step "><p>
     Execute the following set of commands to reconfigure Ironic:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml
<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect2" id="idm139651560543216"><div class="titlepage"><div><div><h3 class="title" id="idm139651560543216"><span class="number">14.6.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">DHCP succeeds with PXE but times out with iPXE</span> <a title="Permalink" class="permalink" href="#idm139651560543216">#</a></h3></div></div></div><p>
   If you see DHCP error "No configuration methods succeeded" in iPXE right
   after successful DHCP performed by embedded NIC firmware, there may be an
   issue with Spanning Tree Protocol on the switch.
  </p><p>
   To avoid this error, Rapid Spanning Tree Protocol needs to be enabled on the
   switch. If this is not an option due to conservative loop detection
   strategies, use the steps outlined below to install the iPXE binary with
   increased DHCP timeouts.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Clone iPXE source code
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>git clone git://git.ipxe.org/ipxe.git
<code class="prompt user">tux &gt; </code>cd ipxe/src</pre></div></li><li class="step "><p>
     Modify lines 22-25 in file <code class="literal">config/dhcp.h</code>, which declare
     reduced DHCP timeouts (1-10 secs). Comment out lines with reduced timeouts
     and uncomment normal PXE timeouts (4-32)
    </p><div class="verbatim-wrap"><pre class="screen">//#define DHCP_DISC_START_TIMEOUT_SEC     1
//#define DHCP_DISC_END_TIMEOUT_SEC       10
#define DHCP_DISC_START_TIMEOUT_SEC   4       /* as per PXE spec */
#define DHCP_DISC_END_TIMEOUT_SEC     32      /* as per PXE spec */</pre></div></li><li class="step "><p>
     Make <code class="literal">undionly.kpxe</code> (BIOS) and
     <code class="literal">ipxe.efi</code> (UEFI) images
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>make bin/undionly.kpxe
<code class="prompt user">tux &gt; </code>make bin-x86_64-efi/ipxe.efi</pre></div></li><li class="step "><p>
     Copy iPXE images to Cloud Lifecycle Manager
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>scp bin/undionly.kpxe bin-x86_64-efi/ipxe.efi stack@10.0.0.4:
stack@10.0.0.4's password:
undionly.kpxe                                    100%   66KB  65.6KB/s   00:00
ipxe.efi                                         100%  918KB 918.2KB/s   00:00</pre></div></li><li class="step "><p>
     From deployer, distribute image files onto all 3 controllers
    </p><div class="verbatim-wrap"><pre class="screen">stack@ardana-cp1-c1-m1-mgmt:<code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/

stack@ardana-cp1-c1-m1-mgmt:<code class="prompt user">ardana &gt; </code>~/scratch/ansible/next/ardana/ansible$ ansible -i hosts/verb_hosts \
IRN-CND -m copy -b -a 'src=/home/stack/ipxe.efi dest=/tftpboot'
...
stack@ardana-cp1-c1-m1-mgmt:<code class="prompt user">ardana &gt; </code>~/scratch/ansible/next/ardana/ansible$ ansible -i hosts/verb_hosts \
IRN-CND -m copy -b -a 'src=/home/stack/undionly.kpxe dest=/tftpboot'
...</pre></div></li></ol></div></div><p>
   There is no need to restart services. With next PXE boot attempt, iPXE
   binary with the increased timeout will be downloaded to the target node via
   TFTP.
  </p><div class="sect3" id="idm139651560527552"><div class="titlepage"><div><div><h4 class="title" id="idm139651560527552"><span class="number">14.6.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ironic Support and Limitations</span> <a title="Permalink" class="permalink" href="#idm139651560527552">#</a></h4></div></div></div><p>
   The following drivers are supported and tested:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      <code class="systemitem">pxe_ipmitool</code> (UEFI and Legacy BIOS mode, flat-network)
     </p></li><li class="listitem "><p>
      <code class="systemitem">pxe_ipmitool</code> (UEFI and Legacy BIOS mode, flat-network)
     </p></li><li class="listitem "><p>
      <code class="systemitem">pxe_ilo</code> (UEFI and Legacy BIOS mode, flat-network)
     </p></li><li class="listitem "><p>
      <code class="systemitem">agent_ipmitool</code> (UEFI and Legacy BIOS mode, flat-network)
     </p></li><li class="listitem "><p>
      <code class="systemitem">agent_ilo</code> (UEFI and Legacy BIOS mode, flat-network)
     </p></li></ul></div><p>
   When using the <code class="systemitem">agent_ilo</code> driver, provision will
   fail if the size of the user image exceeds the free space
   available on the ramdisk partition. This will produce an error in the Ironic
   Conductor logs that may look like as follows
  </p><div class="verbatim-wrap"><pre class="screen">"ERROR root [-] Command failed: prepare_image, error: Error downloading
image: Download of image id 0c4d74e4-58f1-4f8d-8c1d-8a49129a2163 failed: Unable
to write image to /tmp/0c4d74e4-58f1-4f8d-8c1d-8a49129a2163. Error: [Errno 28]
No space left on device: ImageDownloadError: Error downloading image: Download
of image id 0c4d74e4-58f1-4f8d-8c1d-8a49129a2163 failed: Unable to write image
to /tmp/0c4d74e4-58f1-4f8d-8c1d-8a49129a2163. Error: [Errno 28] No space left
on device"</pre></div><p>
   By default, the total amount of space allocated to ramdisk is 4GB. To
   increase the space allocated for the ramdisk, you can update the deploy
   ISO image using the following workaround.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Save the deploy ISO to a file:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack image save --file deploy.iso<em class="replaceable ">IMAGE_ID</em></pre></div><p>
    Replace <em class="replaceable ">IMAGE_ID</em> with the ID of the deploy ISO
    stored in Glance. The ID can be obtained using the <code class="command">openstack image list</code>.
   </p></li><li class="step "><p>
     Mount the saved ISO:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>mkdir /tmp/mnt
<code class="prompt user">tux &gt; </code>sudo mount -t iso9660 -o loop deploy.iso /tmp/mnt</pre></div><p>
     Since the mount directory is read-only, it is necessary to copy its
     content to be able to make modifications.
    </p></li><li class="step "><p>
     Copy the content of the mount directory to a custom directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>mkdir /tmp/custom
<code class="prompt user">tux &gt; </code>cp -aRvf /tmp/mnt/* /tmp/custom/</pre></div></li><li class="step "><p>
     Modify the bootloader files to increase the size of the ramdisk:
    </p><div class="verbatim-wrap"><pre class="screen">/tmp/custom/boot/x86_64/loader/isolinux.cfg
/tmp/custom/EFI/BOOT/grub.cfg
/tmp/custom/boot/grub2/grub.cfg</pre></div><p>
     Find the <code class="literal">openstack-ironic-image</code> label and modify the
     <code class="literal">ramdisk_size</code> parameter in the <code class="literal">append</code>
     property. The <code class="literal">ramdisk_size</code> value must be specified in Kilobytes.
    </p><div class="verbatim-wrap"><pre class="screen">label openstack-ironic-image
  kernel linux
  append initrd=initrd ramdisk_size=10485760 ramdisk_blocksize=4096 \
boot_method=vmedia showopts</pre></div><p>
      Make sure that your baremetal node has the
      amount of RAM that equals or exceeds  the <code class="literal">ramdisk_size</code> value.
     </p></li><li class="step "><p>
     Repackage the ISO using the genisoimage tool:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>cd /tmp/custom
<code class="prompt user">tux &gt; </code>genisoimage -b boot/x86_64/loader/isolinux.bin -R -J -pad -joliet-long \
-iso-level 4 -A '0xaa2dab53' -no-emul-boot -boot-info-table \
-boot-load-size 4 -c boot/x86_64/boot.catalog -hide boot/x86_64/boot.catalog \
-hide-joliet boot/x86_64/boot.catalog -eltorito-alt-boot -b boot/x86_64/efi \
-no-emul-boot -joliet-long -hide glump -hide-joliet glump -o /tmp/custom_deploy.iso ./</pre></div><div id="idm139651560500176" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
       When repackaging the ISO, make sure that you use the same label. You can
       find the label file in the <code class="filename">/tmp/custom/boot/</code>
       directory. The label begins with <code class="literal">0x</code>. For example, <code class="literal">0x51e568cb</code>.
      </p></div></li><li class="step "><p>
     Delete the existing deploy ISO in Glance:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack image delete <em class="replaceable ">IMAGE_ID</em></pre></div></li><li class="step "><p>
     Create a new image with <code class="literal">custom_deploy.iso</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack image create --container-format bare \
--disk-format iso --public --file custom_deploy.iso ir-deploy-iso-ARDANA5.0</pre></div></li><li class="step "><p>
     Re-create or update the Ironic node, if needed. 
    </p></li></ol></div></div></div></div></div><div class="sect1" id="ironic-node-cleaning"><div class="titlepage"><div><div><h2 class="title" id="ironic-node-cleaning"><span class="number">14.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Node Cleaning</span> <a title="Permalink" class="permalink" href="#ironic-node-cleaning">#</a></h2></div></div></div><p>
  Cleaning is the process by which data is removed after a previous tenant has
  used the node. Cleaning requires use of ironic's agent_ drivers. It is
  extremely important to note that if the pxe_ drivers are utilized, no node
  cleaning operations will occur, and a previous tenant's data could be found
  on the node. The same risk of a previous tenant's data possibly can occur if
  cleaning is explicitly disabled as part of the installation.
 </p><p>
  By default, cleaning attempts to utilize ATA secure erase to wipe the
  contents of the disk. If secure erase is unavailable, the cleaning
  functionality built into the Ironic Python Agent falls back to an operation
  referred to as "shred" where random data is written over the contents of the
  disk, and then followed up by writing "0"s across the disk. This can be a
  time-consuming process.
 </p><p>
  An additional feature of cleaning is the ability to update firmware or
  potentially assert new hardware configuration, however, this is an advanced
  feature that must be built into the Ironic Python Agent image. Due to the
  complex nature of such operations, and the fact that no one size fits all,
  this requires a custom Ironic Python Agent image to be constructed with an
  appropriate hardware manager. For more information on hardware managers, see
  <a class="link" href="http://docs.openstack.org/developer/ironic-python-agent/#hardware-managers" target="_blank">http://docs.openstack.org/developer/ironic-python-agent/#hardware-managers</a>
 </p><p>
  Ironic's upstream documentation for cleaning may be found here:
  <a class="link" href="http://docs.openstack.org/developer/ironic/deploy/cleaning.html" target="_blank">http://docs.openstack.org/developer/ironic/deploy/cleaning.html</a>
 </p><div class="sect2" id="idm139651560485168"><div class="titlepage"><div><div><h3 class="title" id="idm139651560485168"><span class="number">14.7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setup</span> <a title="Permalink" class="permalink" href="#idm139651560485168">#</a></h3></div></div></div><p>
   Cleaning is enabled by default in ironic when installed via the Cloud Lifecycle Manager.
   You can verify this by examining the ironic-conductor.conf file.
   Look for:
  </p><div class="verbatim-wrap"><pre class="screen">[conductor]
clean_nodes=true</pre></div></div><div class="sect2" id="idm139651560483200"><div class="titlepage"><div><div><h3 class="title" id="idm139651560483200"><span class="number">14.7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">In use</span> <a title="Permalink" class="permalink" href="#idm139651560483200">#</a></h3></div></div></div><p>
   When enabled, cleaning will be run automatically when nodes go from active
   to available state or from manageable to available. To monitor what step of
   cleaning the node is in, run <code class="literal">ironic node-show</code>:
  </p><div class="verbatim-wrap"><pre class="screen">stack@ardana-cp1-c1-m1-mgmt:~$ ironic node-show 4e6d4273-2535-4830-a826-7f67e71783ed
+------------------------+-----------------------------------------------------------------------+
| Property               | Value                                                                 |
+------------------------+-----------------------------------------------------------------------+
| target_power_state     | None                                                                  |
| extra                  | {}                                                                    |
| last_error             | None                                                                  |
| updated_at             | 2016-04-15T09:33:16+00:00                                             |
| maintenance_reason     | None                                                                  |
| provision_state        | cleaning                                                              |
| clean_step             | {}                                                                    |
| uuid                   | 4e6d4273-2535-4830-a826-7f67e71783ed                                  |
| console_enabled        | False                                                                 |
| target_provision_state | available                                                             |
| provision_updated_at   | 2016-04-15T09:33:16+00:00                                             |
| maintenance            | False                                                                 |
| inspection_started_at  | None                                                                  |
| inspection_finished_at | None                                                                  |
| power_state            | power off                                                             |
| driver                 | agent_ilo                                                             |
| reservation            | ardana-cp1-c1-m1-mgmt                                                 |
| properties             | {u'memory_mb': 4096, u'cpu_arch': u'amd64', u'local_gb': 80,          |
|                        | u'cpus': 2, u'capabilities': u'boot_mode:uefi,boot_option:local'}     |
| instance_uuid          | None                                                                  |
| name                   | None                                                                  |
| driver_info            | {u'ilo_deploy_iso': u'249bf095-e741-441d-bc28-0f44a9b8cd80',          |
|                        | u'ipmi_username': u'Administrator', u'deploy_kernel':                 |
|                        | u'3a78c0a9-3d8d-4764-9300-3e9c00e167a1', u'ilo_address':              |
|                        | u'10.1.196.113', u'ipmi_address': u'10.1.196.113', u'deploy_ramdisk': |
|                        | u'd02c811c-e521-4926-9f26-0c88bbd2ee6d', u'ipmi_password': u'******', |
|                        | u'ilo_password': u'******', u'ilo_username': u'Administrator'}        |
| created_at             | 2016-04-14T08:30:08+00:00                                             |
| driver_internal_info   | {<span class="bold"><strong>u'clean_steps': None</strong></span>,                      |
|                        | u'hardware_manager_version': {u'generic_hardware_manager': u'1.0'},   |
|                        | u'is_whole_disk_image': True, u'agent_erase_devices_iterations': 1,   |
|                        | u'agent_url': u'http://192.168.246.245:9999',                         |
|                        | u'agent_last_heartbeat': 1460633166}                                  |
| chassis_uuid           |                                                                       |
| instance_info          | {}                                                                    |
+------------------------+-----------------------------------------------------------------------+</pre></div><p>
   The status will be in the <code class="literal">driver_internal_info</code> field. You
   will also be able to see the <code class="literal">clean_steps</code> list there.
  </p></div><div class="sect2" id="idm139651560474480"><div class="titlepage"><div><div><h3 class="title" id="idm139651560474480"><span class="number">14.7.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting</span> <a title="Permalink" class="permalink" href="#idm139651560474480">#</a></h3></div></div></div><p>
   If an error occurs during the cleaning process, the node will enter the
   clean failed state so that it is not deployed. The node remains powered on
   for debugging purposes. The node can be moved to the manageable state to
   attempt a fix using the following command:
  </p><div class="verbatim-wrap"><pre class="screen">ironic node-set-provision-state &lt;node id&gt; manage</pre></div><p>
   Once you have identified and fixed the issue, you can return the node to
   available state by executing the following commands:
  </p><div class="verbatim-wrap"><pre class="screen">ironic node-set-maintenance &lt;node id&gt; false
ironic node-set-provision-state &lt;node id&gt; provide</pre></div><p>
   This will retry the cleaning steps and set the node to available state upon
   their successful completion.
  </p></div><div class="sect2" id="idm139651560470768"><div class="titlepage"><div><div><h3 class="title" id="idm139651560470768"><span class="number">14.7.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Disabling Node Cleaning</span> <a title="Permalink" class="permalink" href="#idm139651560470768">#</a></h3></div></div></div><p>
   To disable node cleaning, edit
   <code class="literal">~/openstack/my_cloud/definition/data/ironic/ironic_config.yml</code>
   and set <code class="literal">enable_node_cleaning</code> to <code class="literal">false</code>.
  </p><p>
   Commit your changes:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "disable node cleaning"</pre></div><p>
   Deploy these changes by re-running the configuration processor and
   reconfigure the ironic installation:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml
ansible-playbook -i hosts/localhost ready-deployment.yml
cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml</pre></div></div></div><div class="sect1" id="ironic_oneview"><div class="titlepage"><div><div><h2 class="title" id="ironic_oneview"><span class="number">14.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ironic and HPE OneView</span> <a title="Permalink" class="permalink" href="#ironic_oneview">#</a></h2></div></div></div><div class="sect2" id="idm139651560463248"><div class="titlepage"><div><div><h3 class="title" id="idm139651560463248"><span class="number">14.8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling Ironic HPE OneView driver in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <a title="Permalink" class="permalink" href="#idm139651560463248">#</a></h3></div></div></div><p>
   Edit the file
   <code class="literal">~/openstack/my_cloud/definition/data/ironic/ironicconfig.yml</code>
   and set the value
  </p><div class="verbatim-wrap"><pre class="screen">enable_oneview: true</pre></div><p>
   This will enable the HPE OneView driver for Ironic in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
  </p></div><div class="sect2" id="idm139651560457456"><div class="titlepage"><div><div><h3 class="title" id="idm139651560457456"><span class="number">14.8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adding HPE OneView Appliance Credentials</span> <a title="Permalink" class="permalink" href="#idm139651560457456">#</a></h3></div></div></div><div class="verbatim-wrap"><pre class="screen">manage_url: https://&lt;Onview appliance URL&gt;

oneview_username: "&lt;Appliance username&gt;"

oneview_encrypted_password: "&lt;Encrypted password&gt;"

oneview_allow_insecure_connections: &lt;true/false&gt;

tls_cacert_file: &lt;CA certificate for connection&gt;</pre></div></div><div class="sect2" id="idm139651560455856"><div class="titlepage"><div><div><h3 class="title" id="idm139651560455856"><span class="number">14.8.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Encrypting the HPE OneView Password</span> <a title="Permalink" class="permalink" href="#idm139651560455856">#</a></h3></div></div></div><p>
   Encryption can be applied using <code class="literal">ardanaencrypt.py</code> or using
   <code class="literal">openssl</code>. On the Cloud Lifecycle Manager node, export the key
   used for encryption as environment variable:
  </p><div class="verbatim-wrap"><pre class="screen">export ARDANA_USER_PASSWORD_ENCRYPT_KEY="<em class="replaceable ">ENCRYPTION_KEY</em>"</pre></div><p>
   And then execute the following commands:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
python ardanaencrypt.py</pre></div><p>
   Enter password to be encrypted when prompted. The script uses the key that
   was exported in the <code class="literal">ARDANA_USER_PASSWORD_ENCRYPT_KEY</code> to do
   the encryption.
  </p><p>
   For more information, see <span class="intraxref">Book “Security Guide”, Chapter 9 “Encryption of Passwords and Sensitive Data”</span>.
  </p></div><div class="sect2" id="idm139651560449568"><div class="titlepage"><div><div><h3 class="title" id="idm139651560449568"><span class="number">14.8.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Decrypting the HPE OneView Password</span> <a title="Permalink" class="permalink" href="#idm139651560449568">#</a></h3></div></div></div><p>
   Before running the <code class="literal">site.yml</code> playbook, export the key used
   for encryption as environment variable:
  </p><div class="verbatim-wrap"><pre class="screen">export ARDANA_USER_PASSWORD_ENCRYPT_KEY="<em class="replaceable ">ENCRYPTION_KEY</em>"</pre></div><p>
   The decryption of the password is then automatically handled in
   ironic-ansible playbooks.
  </p></div><div class="sect2" id="idm139651560446240"><div class="titlepage"><div><div><h3 class="title" id="idm139651560446240"><span class="number">14.8.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Registering Baremetal Node for HPE OneView Driver</span> <a title="Permalink" class="permalink" href="#idm139651560446240">#</a></h3></div></div></div><div class="verbatim-wrap"><pre class="screen">ironic node-create -d agent_pxe_oneview</pre></div><p>
   Update node driver-info:
  </p><div class="verbatim-wrap"><pre class="screen"> ironic node-update $NODE_UUID add driver_info/server_hardware_uri=$SH_URI</pre></div></div><div class="sect2" id="idm139651560443856"><div class="titlepage"><div><div><h3 class="title" id="idm139651560443856"><span class="number">14.8.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Updating Node Properties</span> <a title="Permalink" class="permalink" href="#idm139651560443856">#</a></h3></div></div></div><div class="verbatim-wrap"><pre class="screen">ironic node-update $NODE_UUID add \
  properties/capabilities=server_hardware_type_uri:$SHT_URI,\
	enclosure_group_uri:$EG_URI,server_profile_template_uri=$SPT_URI</pre></div></div><div class="sect2" id="idm139651560442352"><div class="titlepage"><div><div><h3 class="title" id="idm139651560442352"><span class="number">14.8.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating Port for Driver</span> <a title="Permalink" class="permalink" href="#idm139651560442352">#</a></h3></div></div></div><div class="verbatim-wrap"><pre class="screen">ironic port-create -n $NODE_UUID -a $MAC_ADDRESS</pre></div></div><div class="sect2" id="idm139651560440960"><div class="titlepage"><div><div><h3 class="title" id="idm139651560440960"><span class="number">14.8.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating a Node</span> <a title="Permalink" class="permalink" href="#idm139651560440960">#</a></h3></div></div></div><p>
   Create Node:
  </p><div class="verbatim-wrap"><pre class="screen">ironic node-create -n ovbay7 -d agent_pxe_oneview</pre></div><p>
   Update driver info:
  </p><div class="verbatim-wrap"><pre class="screen">ironic node-update $ID add driver_info/server_hardware_uri="/rest/server-hardware/3037...464B" \
driver_info/deploy_kernel="$KERNELDISK" driver_info/deploy_ramdisk="$RAMDISK"</pre></div><p>
   Update node properties:
  </p><div class="verbatim-wrap"><pre class="screen">ironic node-update $ID add properties/local_gb=10
ironic node-update $ID add properties/cpus=24 properties/memory_mb=262144 \
properties/cpu_arch=x86_64</pre></div><div class="verbatim-wrap"><pre class="screen">ironic node-update \
$ID add properties/capabilities=server_hardware_type_uri:'/rest/server-hardware-types/B31...F69E',\
enclosure_group_uri:'/rest/enclosure-groups/80efe...b79fa',\
server_profile_template_uri:'/rest/server-profile-templates/faafc3c0-6c81-47ca-a407-f67d11262da5'</pre></div></div><div class="sect2" id="idm139651560435328"><div class="titlepage"><div><div><h3 class="title" id="idm139651560435328"><span class="number">14.8.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Getting Data using REST API</span> <a title="Permalink" class="permalink" href="#idm139651560435328">#</a></h3></div></div></div><p>
   GET login session auth id:
  </p><div class="verbatim-wrap"><pre class="screen">curl -k https://<em class="replaceable ">ONEVIEW_MANAGER_URL</em>/rest/login-sessions \
  -H "content-type:application/json" \
  -X POST \
  -d '{"userName":"<em class="replaceable ">USER_NAME</em>", "password":"<em class="replaceable ">PASSWORD</em>"}'</pre></div><p>
   Get the complete node details in JSON format:
  </p><div class="verbatim-wrap"><pre class="screen">curl -k "https://<em class="replaceable ">ONEVIEW_MANAGER_URL</em>;/rest/server-hardware/30373237-3132-4753-4835-32325652464B" -H "content-type:application/json" -H "Auth:&lt;auth_session_id&gt;"| python -m json.tool</pre></div></div><div class="sect2" id="idm139651560430576"><div class="titlepage"><div><div><h3 class="title" id="idm139651560430576"><span class="number">14.8.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ironic HPE OneView CLI</span> <a title="Permalink" class="permalink" href="#idm139651560430576">#</a></h3></div></div></div><p>
   <code class="literal">ironic-oneview-cli</code> is already installed in
   <code class="literal">ironicclient</code> venv with a symbolic link to it. To generate
   an <code class="literal">rc</code> file for the HPE OneView CLI, follow these steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Run the following commands:
    </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc
glance image-list</pre></div></li><li class="step "><p>
     Note the <code class="literal">deploy-kernel</code> and
     <code class="literal">deploy-ramdisk</code> UUID and then run the following command
     to generate the <code class="literal">rc</code> file:
    </p><div class="verbatim-wrap"><pre class="screen">ironic-oneview genrc</pre></div><p>
     You will be prompted to enter:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       HPE OneView Manager URL
      </p></li><li class="listitem "><p>
       Username
      </p></li><li class="listitem "><p>
       deploy-kernel
      </p></li><li class="listitem "><p>
       deploy-ramdisk
      </p></li><li class="listitem "><p>
       allow_insecure_connection
      </p></li><li class="listitem "><p>
       cacert file
      </p></li></ul></div><p>
     The <code class="literal">ironic-oneview.rc</code> file will be generated in the
     current directory, by default. It is possible to specify a different
     location.
    </p></li><li class="step "><p>
     Source the generated file:
    </p><div class="verbatim-wrap"><pre class="screen">source ironic-oneview.rc</pre></div><p>
     Now enter the password of the HPE OneView appliance.
    </p></li><li class="step "><p>
     You can now use the CLI for node and flavor creation as follows:
    </p><div class="verbatim-wrap"><pre class="screen">ironic-oneview node-create
ironic-oneview flavor-create</pre></div></li></ol></div></div></div></div><div class="sect1" id="ironic_raid_config"><div class="titlepage"><div><div><h2 class="title" id="ironic_raid_config"><span class="number">14.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">RAID Configuration for Ironic</span> <a title="Permalink" class="permalink" href="#ironic_raid_config">#</a></h2></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    Node Creation:
   </p><p>
    Check the raid capabilities of the driver:
   </p><div class="verbatim-wrap"><pre class="screen">ironic --ironic-api-version 1.15 driver-raid-logical-disk-properties pxe_ilo</pre></div><p>
    This will generate output similar to the following:
   </p><div class="verbatim-wrap"><pre class="screen">+----------------------+-------------------------------------------------------------------------+
| Property             | Description                                                             |
+----------------------+-------------------------------------------------------------------------+
| controller           | Controller to use for this logical disk. If not specified, the          |
|                      | driver will choose a suitable RAID controller on the bare metal node.   |
|                      | Optional.                                                               |
| disk_type            | The type of disk preferred. Valid values are 'hdd' and 'ssd'. If this   |
|                      | is not specified, disk type will not be a selection criterion for       |
|                      | choosing backing physical disks. Optional.                              |
| interface_type       | The interface type of disk. Valid values are 'sata', 'scsi' and 'sas'.  |
|                      | If this is not specified, interface type will not be a selection        |
|                      | criterion for choosing backing physical disks. Optional.                |
| is_root_volume       | Specifies whether this disk is a root volume. By default, this is False.|
|                      | Optional.                                                               |
| #_of_physical_disks  | Number of physical disks to use for this logical disk. By default, the  |
|                      | driver uses the minimum number of disks required for that RAID level.   |
|                      | Optional.                                                               |
| physical_disks       | The physical disks to use for this logical disk. If not specified, the  |
|                      | driver will choose suitable physical disks to use. Optional.            |
| <span class="bold"><strong>raid_level           | RAID level for the logical disk. Valid values are '0', '1', '2', '5', </strong></span>  |
|                      | <span class="bold"><strong>'6', '1+0', '5+0' and '6+0'. Required.</strong></span>                                  |
| share_physical_disks | Specifies whether other logical disks can share physical disks with this|
|                      | logical disk. By default, this is False. Optional.                      |
| <span class="bold"><strong>size_gb              | Size in GiB (Integer) for the logical disk. Use 'MAX' as size_gb if </strong></span>    |
|                      | <span class="bold"><strong>this logical disk is supposed to use the rest of</strong></span>                        |
|                      | <span class="bold"><strong>the space available. Required.</strong></span>                                          |
| volume_name          | Name of the volume to be created. If this is not specified, it will be  |
|                      | auto-generated. Optional.                                               |
+----------------------+-------------------------------------------------------------------------+</pre></div><p>
    Node State will be <span class="bold"><strong>Available</strong></span>
   </p><div class="verbatim-wrap"><pre class="screen">ironic node-create -d pxe_ilo -i ilo_address=&lt;ip_address&gt; \
  -i ilo_username=&lt;username&gt; -i ilo_password=&lt;password&gt; \
  -i ilo_deploy_iso=&lt;iso_id&gt; -i deploy_kernel=&lt;kernel_id&gt; \
  -i deploy_ramdisk=&lt;ramdisk_id&gt; -p cpus=2 -p memory_mb=4096 \
  -p local_gb=80  -p cpu_arch=amd64 \
  -p capabilities="boot_option:local,boot_mode:bios"</pre></div><div class="verbatim-wrap"><pre class="screen">ironic port-create -a &lt;port&gt; -n &lt;node-uuid&gt;</pre></div></li><li class="step "><p>
    Apply the target raid configuration on the node:
   </p><p>
    See the OpenStack documentation for RAID configuration at
    <a class="link" href="http://docs.openstack.org/developer/ironic/deploy/raid.html" target="_blank">http://docs.openstack.org/developer/ironic/deploy/raid.html</a>.
   </p><p>
    Set the target RAID configuration by editing the file
    <code class="literal">raid_conf.json</code> and setting the appropriate values, for
    example:
   </p><div class="verbatim-wrap"><pre class="screen">{ "logical_disks": [ {"size_gb": 5, "raid_level": "0", "is_root_volume": true} ] }</pre></div><p>
    and then run the following command:
   </p><div class="verbatim-wrap"><pre class="screen">ironic --ironic-api-version 1.15 node-set-target-raid-config &lt;node-uuid&gt; raid_conf.json</pre></div><p>
    The output produced should be similar to the following:
   </p><div class="verbatim-wrap"><pre class="screen">+-----------------------+-------------------------------------------------------------------------+
| Property              | Value                                                                   |
+-----------------------+-------------------------------------------------------------------------+
| chassis_uuid          |                                                                         |
| clean_step            | {}                                                                      |
| console_enabled       | False                                                                   |
| created_at            | 2016-06-14T14:58:07+00:00                                               |
| driver                | pxe_ilo                                                                 |
| driver_info           | {u'ilo_deploy_iso': u'd43e589a-07db-4fce-a06e-98e2f38340b4',            |
|                       | u'deploy_kernel': u'915c5c74-1ceb-4f78-bdb4-8547a90ac9c0',              |
|                       | u'ilo_address': u'10.1.196.116', u'deploy_ramdisk':                     |
|                       | u'154e7024-bf18-4ad2-95b0-726c09ce417a', u'ilo_password': u'******',    |
|                       | u'ilo_username': u'Administrator'}                                      |
| driver_internal_info  | {u'agent_cached_clean_steps_refreshed': u'2016-06-15 07:16:08.264091',  |
|                       | u'agent_cached_clean_steps': {u'raid': [{u'interface': u'raid',         |
|                       | u'priority': 0, u'step': u'delete_configuration'}, {u'interface':       |
|                       | u'raid', u'priority': 0, u'step': u'create_configuration'}], u'deploy': |
|                       | [{u'priority': 10, u'interface': u'deploy', u'reboot_requested': False, |
|                       | u'abortable': True, u'step': u'erase_devices'}]}, u'clean_steps': None, |
|                       | u'hardware_manager_version': {u'generic_hardware_manager': u'3'},       |
|                       | u'agent_erase_devices_iterations': 1, u'agent_url':                     |
|                       | u'http://192.168.245.143:9999', u'agent_last_heartbeat': 1465974974}    |
| extra                 | {}                                                                      |
| inspection_finished_at| None                                                                    |
| inspection_started_at | None                                                                    |
| instance_info         | {u'deploy_key': u'XXN2ON0V9ER429MECETJMUG5YHTKOQOZ'}                    |
| instance_uuid         | None                                                                    |
| last_error            | None                                                                    |
| maintenance           | False                                                                   |
| maintenance_reason    | None                                                                    |
| name                  | None                                                                    |
| power_state           | power off                                                               |
| properties            | {u'cpu_arch': u'amd64', u'root_device': {u'wwn': u'0x600508b1001ce286'},|
|                       | u'cpus': 2, u'capabilities':                                            |
|                       | u'boot_mode:bios,raid_level:6,boot_option:local', u'memory_mb': 4096,   |
|                       | u'local_gb': 4}                                                         |
| provision_state       | available                                                               |
| provision_updated_at  | 2016-06-15T07:16:27+00:00                                               |
| reservation           | padawan-ironic-cp1-c1-m2-mgmt                                           |
| target_power_state    | power off                                                               |
| target_provision_state| None                                                                    |
| <span class="bold"><strong>target_raid_config</strong></span>    | {u'logical_disks': [{u'size_gb': 5, <span class="bold"><strong>u'raid_level': u'6',</strong></span>                |
|                       | u'is_root_volume': True}]}                                              |
| updated_at            | 2016-06-15T07:44:22+00:00                                               |
| uuid                  | 22ab9f85-71a1-4748-8d6b-f6411558127e                                    |
+-----------------------+-------------------------------------------------------------------------+</pre></div><p>
    Now set the state of the node to
    <span class="bold"><strong>manageable</strong></span>:
   </p><div class="verbatim-wrap"><pre class="screen">ironic --ironic-api-version latest node-set-provision-state &lt;node-uuid&gt; manage</pre></div></li><li class="step "><p>
    Manual cleaning steps:
   </p><p>
    Manual cleaning is enabled by default in production - the following are the
    steps to enable cleaning if the manual cleaning has been disabled.
   </p><ol type="a" class="substeps "><li class="step "><p>
      Provide <code class="literal">cleaning_network_uuid</code> in
      <code class="literal">ironic-conductor.conf</code>
     </p></li><li class="step "><p>
      Edit the file
      <code class="literal">~/openstack/my_cloud/definition/data/ironic/ironic_config.yml</code>
      and set <code class="literal">enable_node_cleaning</code> to
      <code class="literal">true</code>.
     </p></li><li class="step "><p>
      Then run the following set of commands:
     </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "enabling node cleaning"
cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml
ansible-playbook -i hosts/localhost ready-deployment.yml
cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml</pre></div><p>
      After performing these steps, the state of the node will become
      <span class="bold"><strong>Cleaning</strong></span>.
     </p></li></ol><p>
    Run the following command:
   </p><div class="verbatim-wrap"><pre class="screen">ironic --ironic-api-version latest node-set-provision-state 2123254e-8b31...aa6fd \
  clean --clean-steps '[{ "interface": "raid","step": "delete_configuration"}, \
  { "interface": "raid" ,"step": "create_configuration"}]'</pre></div><p>
    Node-information after a Manual cleaning:
   </p><div class="verbatim-wrap"><pre class="screen">+-----------------------+-------------------------------------------------------------------------+
| Property              | Value                                                                   |
+-----------------------+-------------------------------------------------------------------------+
| chassis_uuid          |                                                                         |
| clean_step            | {}                                                                      |
| console_enabled       | False                                                                   |
| created_at            | 2016-06-14T14:58:07+00:00                                               |
| driver                | pxe_ilo                                                                 |
| driver_info           | {u'ilo_deploy_iso': u'd43e589a-07db-4fce-a06e-98e2f38340b4',            |
|                       | u'deploy_kernel': u'915c5c74-1ceb-4f78-bdb4-8547a90ac9c0',              |
|                       | u'ilo_address': u'10.1.196.116', u'deploy_ramdisk':                     |
|                       | u'154e7024-bf18-4ad2-95b0-726c09ce417a', u'ilo_password': u'******',    |
|                       | u'ilo_username': u'Administrator'}                                      |
| driver_internal_info  | {u'agent_cached_clean_steps_refreshed': u'2016-06-15 07:16:08.264091',  |
|                       | u'agent_cached_clean_steps': {u'raid': [{u'interface': u'raid',         |
|                       | u'priority': 0, u'step': u'delete_configuration'}, {u'interface':       |
|                       | u'raid', u'priority': 0, u'step': u'create_configuration'}], u'deploy': |
|                       | [{u'priority': 10, u'interface': u'deploy', u'reboot_requested': False, |
|                       | u'abortable': True, u'step': u'erase_devices'}]}, u'clean_steps': None, |
|                       | u'hardware_manager_version': {u'generic_hardware_manager': u'3'},       |
|                       | u'agent_erase_devices_iterations': 1, u'agent_url':                     |
|                       | u'http://192.168.245.143:9999', u'agent_last_heartbeat': 1465974974}    |
| extra                 | {}                                                                      |
| inspection_finished_at| None                                                                    |
| inspection_started_at | None                                                                    |
| instance_info         | {u'deploy_key': u'XXN2ON0V9ER429MECETJMUG5YHTKOQOZ'}                    |
| instance_uuid         | None                                                                    |
| last_error            | None                                                                    |
| maintenance           | False                                                                   |
| maintenance_reason    | None                                                                    |
| name                  | None                                                                    |
| power_state           | power off                                                               |
| properties            | {u'cpu_arch': u'amd64', u'root_device': {u'wwn': u'0x600508b1001ce286'},|
|                       | u'cpus': 2, u'capabilities':                                            |
|                       | u'boot_mode:bios,raid_level:6,boot_option:local', u'memory_mb': 4096,   |
|                       | u'local_gb': 4}                                                         |
| provision_state       | manageable                                                              |
| provision_updated_at  | 2016-06-15T07:16:27+00:00                                               |
| raid_config           | {u'last_updated': u'2016-06-15 07:16:14.584014', u'physical_disks':     |
|                       | [{u'status': u'ready', u'size_gb': 1024, u'interface_type': u'sata',    |
|                       | u'firmware': u'HPGC', u'controller': u'Smart Array P440ar in Slot 0     |
|                       | (Embedded)', u'model': u'ATA     MM1000GBKAL', u'disk_type': u'hdd',    |
|                       | u'id': u'1I:3:3'}, {u'status': u'ready', u'size_gb': 1024,              |
|                       | u'interface_type': u'sata', u'firmware': u'HPGC', u'controller': u'Smart|
|                       | Array P440ar in Slot 0 (Embedded)', u'model': u'ATA     MM1000GBKAL',   |
|                       | u'disk_type': u'hdd', u'id': u'1I:3:1'}, {u'status': u'active',         |
|                       | u'size_gb': 1024, u'interface_type': u'sata', u'firmware': u'HPGC',     |
|                       | u'controller': u'Smart Array P440ar in Slot 0 (Embedded)', u'model':    |
|                       | u'ATA     MM1000GBKAL', u'disk_type': u'hdd', u'id': u'1I:3:2'},        |
|                       | {u'status': u'active', u'size_gb': 1024, u'interface_type': u'sata',    |
|                       | u'firmware': u'HPGC', u'controller': u'Smart Array P440ar in Slot 0     |
|                       | (Embedded)', u'model': u'ATA     MM1000GBKAL', u'disk_type': u'hdd',    |
|                       | u'id': u'2I:3:6'}, {u'status': u'active', u'size_gb': 1024,             |
|                       | u'interface_type': u'sata', u'firmware': u'HPGC', u'controller': u'Smart|
|                       | Array P440ar in Slot 0 (Embedded)', u'model': u'ATA     MM1000GBKAL',   |
|                       | u'disk_type': u'hdd', u'id': u'2I:3:5'}, {u'status': u'active',         |
|                       | u'size_gb': 1024, u'interface_type': u'sata', u'firmware': u'HPGC',     |
|                       | u'controller': u'Smart Array P440ar in Slot 0 (Embedded)', u'model':    |
|                       | u'ATA     MM1000GBKAL', u'disk_type': u'hdd', u'id': u'1I:3:4'}],       |
|                       | u'logical_disks': [{u'size_gb': 4, u'physical_disks': [u'1I:3:2',       |
|                       | u'2I:3:6', u'2I:3:5', u'1I:3:4'], u'raid_level': u'6',                  |
|                       | u'is_root_volume': True, u'root_device_hint': {u'wwn':                  |
|                       | u'0x600508b1001ce286'}, u'controller': u'Smart Array P440ar in Slot 0   |
|                       | (Embedded)', u'volume_name': u'015E795CPDNLH0BRH8N406AAB7'}]}           |
| reservation           | padawan-ironic-cp1-c1-m2-mgmt                                           |
| target_power_state    | power off                                                               |
| target_provision_state| None                                                                    |
| target_raid_config    | {u'logical_disks': [{u'size_gb': 5, u'raid_level': u'6',                |
|                       | u'is_root_volume': True}]}                                              |
| updated_at            | 2016-06-15T07:44:22+00:00                                               |
| uuid                  | 22ab9f85-71a1-4748-8d6b-f6411558127e                                    |
+-----------------------+-------------------------------------------------------------------------+</pre></div><p>
    After the manual cleaning, run the following command to change the state of
    a node to <span class="bold"><strong>available</strong></span>:
   </p><div class="verbatim-wrap"><pre class="screen">ironic --ironic-api-version latest node-set-provision-state &lt;node-uuid&gt; \
  provide</pre></div></li></ol></div></div></div><div class="sect1" id="ironic_audit_support"><div class="titlepage"><div><div><h2 class="title" id="ironic_audit_support"><span class="number">14.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Audit Support for Ironic</span> <a title="Permalink" class="permalink" href="#ironic_audit_support">#</a></h2></div></div></div><div class="sect2" id="idm139651560361264"><div class="titlepage"><div><div><h3 class="title" id="idm139651560361264"><span class="number">14.10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">API Audit Logging</span> <a title="Permalink" class="permalink" href="#idm139651560361264">#</a></h3></div></div></div><p>
   Audit middleware supports delivery of CADF audit events via Oslo messaging
   notifier capability. Based on <code class="literal">notification_driver</code>
   configuration, audit events can be routed to messaging infrastructure
   (<code class="literal">notification_driver = messagingv2</code>) or can be routed to a
   log file (<code class="literal">notification_driver = log</code>).
  </p><p>
   Audit middleware creates two events per REST API interaction. The first
   event has information extracted from request data and the second one
   contains information on the request outcome (response).
  </p></div><div class="sect2" id="idm139651560357728"><div class="titlepage"><div><div><h3 class="title" id="idm139651560357728"><span class="number">14.10.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling API Audit Logging</span> <a title="Permalink" class="permalink" href="#idm139651560357728">#</a></h3></div></div></div><p>
   You can enable audit logging for Ironic by changing the configuration in the
   input model. Edit the file
   <code class="literal">~/openstack/my_cloud/definition/cloudConfig.yml</code> and in the
   <code class="literal">audit-settings</code> section, change the
   <code class="literal">default</code> value to <code class="literal">enabled</code>. The
   ironic-ansible playbooks will now enable audit support for Ironic.
  </p><p>
   API audit events will be logged in the corresponding audit directory, for
   example, <code class="literal">/var/audit/ironic/ironic-api-audit.log</code>. An audit
   event will be logged in the log file for every request and response for an
   API call.
  </p></div><div class="sect2" id="idm139651560353280"><div class="titlepage"><div><div><h3 class="title" id="idm139651560353280"><span class="number">14.10.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Sample Audit Event</span> <a title="Permalink" class="permalink" href="#idm139651560353280">#</a></h3></div></div></div><p>
   The following output is an example of an audit event for an <code class="literal">ironic
   node-list</code> command:
  </p><div class="verbatim-wrap"><pre class="screen">{
   "event_type":"audit.http.request",
   "timestamp":"2016-06-15 06:04:30.904397",
   "payload":{
      "typeURI":"http://schemas.dmtf.org/cloud/audit/1.0/event",
      "eventTime":"2016-06-15T06:04:30.903071+0000",
      "target":{
         "id":"ironic",
         "typeURI":"unknown",
         "addresses":[
            {
               "url":"http://{ironic_admin_host}:6385",
               "name":"admin"
            },
           {
               "url":"http://{ironic_internal_host}:6385",
               "name":"private"
           },
           {
               "url":"http://{ironic_public_host}:6385",
               "name":"public"
           }
         ],
         "name":"ironic"
      },
      "observer":{
         "id":"target"
      },
      "tags":[
         "correlation_id?value=685f1abb-620e-5d5d-b74a-b4135fb32373"
      ],
      "eventType":"activity",
      "initiator":{
         "typeURI":"service/security/account/user",
         "name":"admin",
         "credential":{
            "token":"***",
            "identity_status":"Confirmed"
         },
         "host":{
            "agent":"python-ironicclient",
            "address":"10.1.200.129"
         },
         "project_id":"d8f52dd7d9e1475dbbf3ba47a4a83313",
         "id":"8c1a948bad3948929aa5d5b50627a174"
      },
      "action":"read",
      "outcome":"pending",
      "id":"061b7aa7-5879-5225-a331-c002cf23cb6c",
      "requestPath":"/v1/nodes/?associated=True"
   },
   "priority":"INFO",
   "publisher_id":"ironic-api",
   "message_id":"2f61ebaa-2d3e-4023-afba-f9fca6f21fc2"
}</pre></div></div></div></div><div class="chapter " id="install_swift"><div class="titlepage"><div><div><h2 class="title"><span class="number">15 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installation for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale Cloud with Swift Only</span> <a title="Permalink" class="permalink" href="#install_swift">#</a></h2></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#sec.swift.important_notes"><span class="number">15.1 </span><span class="name">Important Notes</span></a></span></dt><dt><span class="section"><a href="#sec.swift.prereqs"><span class="number">15.2 </span><span class="name">Before You Start</span></a></span></dt><dt><span class="section"><a href="#sec.swift.setup_deployer"><span class="number">15.3 </span><span class="name">Setting Up the Cloud Lifecycle Manager</span></a></span></dt><dt><span class="section"><a href="#idm139651560258336"><span class="number">15.4 </span><span class="name">Configure Your Environment</span></a></span></dt><dt><span class="section"><a href="#sec.swift.provision"><span class="number">15.5 </span><span class="name">Provisioning Your Baremetal Nodes</span></a></span></dt><dt><span class="section"><a href="#sec.swift.config_processor"><span class="number">15.6 </span><span class="name">Running the Configuration Processor</span></a></span></dt><dt><span class="section"><a href="#sec.swift.deploy"><span class="number">15.7 </span><span class="name">Deploying the Cloud</span></a></span></dt><dt><span class="section"><a href="#sec.swift.post_installation"><span class="number">15.8 </span><span class="name">Post-Installation Verification and Administration</span></a></span></dt></dl></div></div><p>
  This page describes the installation step requirements for the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  Entry-scale Cloud with Swift Only model.
 </p><div class="sect1" id="sec.swift.important_notes"><div class="titlepage"><div><div><h2 class="title" id="sec.swift.important_notes"><span class="number">15.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Important Notes</span> <a title="Permalink" class="permalink" href="#sec.swift.important_notes">#</a></h2></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     If you are looking for information about when to use the GUI installer and
     when to use the command line (CLI), see the
     <a class="xref" href="#install_overview" title="Installation Overview">Installation Overview</a>.
    </p></li><li class="listitem "><p>
     Review the <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 2 “Hardware and Software Support Matrix”</span> that we have listed.
    </p></li><li class="listitem "><p>
     Review the release notes to make yourself aware of any known issues and
     limitations.
    </p></li><li class="listitem "><p>
     The installation process can occur in different phases. For example, you
     can install the control plane only and then add Compute nodes afterwards
     if you would like.
    </p></li><li class="listitem "><p>
     If you run into issues during installation, we have put together a list of
     <a class="xref" href="#troubleshooting_installation" title="Chapter 19. Troubleshooting the Installation">Chapter 19, <em>Troubleshooting the Installation</em></a> you can reference.
    </p></li><li class="listitem "><p>
     Make sure all disks on the system(s) are wiped before you begin the
     install. (For Swift, refer to <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 12 “Modifying Example Configurations for Object Storage using Swift”, Section 12.6 “Swift Requirements for Device Group Drives”</span>.)
    </p></li><li class="listitem "><p>
     There is no requirement to have a dedicated network for OS-install and
     system deployment, this can be shared with the management network. More
     information can be found in <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”</span>.
    </p></li><li class="listitem "><p>
     You may see the terms deployer and Cloud Lifecycle Manager used interchangeably. These are
     referring to the same nodes in your environment.
    </p></li><li class="listitem "><p>
     When running the Ansible playbook in this installation guide, if a runbook
     fails you will see in the error response to use the
     <code class="literal">--limit</code> switch when retrying a playbook. This should be
     avoided. You can simply re-run any playbook without this switch.
    </p></li><li class="listitem "><p>
     DVR is not supported with ESX compute.
    </p></li><li class="listitem "><p>
     When you attach a Cinder volume to the VM running on the ESXi host, the
     volume will not get detected automatically. Make sure to set the image
     metadata <span class="bold"><strong>vmware_adaptertype=lsiLogicsas</strong></span>
     for image before launching the instance. This will help to discover the
     volume change appropriately.
    </p></li></ul></div></div><div class="sect1" id="sec.swift.prereqs"><div class="titlepage"><div><div><h2 class="title" id="sec.swift.prereqs"><span class="number">15.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Before You Start</span> <a title="Permalink" class="permalink" href="#sec.swift.prereqs">#</a></h2></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Review the <a class="xref" href="#preinstall_checklist" title="Chapter 2. Pre-Installation Checklist">Chapter 2, <em>Pre-Installation Checklist</em></a> about recommended
     pre-installation tasks.
    </p></li><li class="step "><p>
     Prepare the Cloud Lifecycle Manager node. The Cloud Lifecycle Manager must be accessible either directly or via
     <code class="literal">ssh</code>, and have SUSE Linux Enterprise Server 12 SP3 installed. All nodes must
     be accessible to the Cloud Lifecycle Manager. If the nodes do not have direct access to
     online Cloud subscription channels, the Cloud Lifecycle Manager node will need to host the
     Cloud repositories.
    </p><ol type="a" class="substeps "><li class="step "><p>
       If you followed the installation instructions for Cloud Lifecycle Manager (see <a class="xref" href="#cha.depl.dep_inst" title="Chapter 3. Installing the Cloud Lifecycle Manager server">Chapter 3, <em>Installing the Cloud Lifecycle Manager server</em></a> <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> should already be
       installed. Double-check whether SUSE Linux Enterprise and <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> are properly
       registered at the SUSE Customer Center by starting YaST and running <span class="guimenu">Software</span> › <span class="guimenu">Product
       Registration</span>.
      </p><p>
       If you have not yet installed <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, do so by starting YaST and
       running <span class="guimenu">Software</span> › <span class="guimenu">Product
       Registration</span> › <span class="guimenu">Select
       Extensions</span>. Choose <span class="guimenu"><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> 
       and follow the on-screen instructions. Make sure to register
       <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> during the installation process and to install the software
       pattern <code class="literal">patterns-cloud-ardana</code>.
      </p></li><li class="step "><p>
       Ensure the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> media repositories and updates repositories are
       made available to all nodes in your deployment. This can be accomplished
       either by configuring the Cloud Lifecycle Manager server as an SMT mirror as described in
       <a class="xref" href="#app.deploy.smt_lcm" title="Chapter 4. Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)">Chapter 4, <em>Installing and Setting Up an SMT Server on the Cloud Lifecycle Manager server (Optional)</em></a> or by syncing or mounting the
       Cloud and updates repositories to the Cloud Lifecycle Manager server as described in <a class="xref" href="#cha.depl.repo_conf_lcm" title="Chapter 5. Software Repository Setup">Chapter 5, <em>Software Repository Setup</em></a>.
      </p></li><li class="step "><p>
       Configure passwordless <code class="command">sudo</code> for the user created when
       setting up the node (as described in <a class="xref" href="#sec.depl.adm_inst.user" title="3.5. Creating a User">Section 3.5, “Creating a User”</a>). Note that this is
       <span class="emphasis"><em>not</em></span> the user <code class="systemitem">ardana</code> that will be used later in this
       procedure. In the following we assume you named the user <code class="systemitem">cloud</code>. Run the command
       <code class="command">visudo</code> as user <code class="systemitem">root</code> and add the following line
       to the end of the file:
      </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable ">cloud</em> ALL = (root) NOPASSWD:ALL</pre></div><p>
       Make sure to replace <em class="replaceable ">cloud</em> with your user name
       choice.
      </p></li><li class="step "><p>
       Set the password for the user
       <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen">sudo passwd ardana</pre></div></li><li class="step "><p>
       Become the user <code class="systemitem">ardana</code>:
      </p><div class="verbatim-wrap"><pre class="screen">su - ardana</pre></div></li><li class="step "><p>
       Place a copy of the SUSE Linux Enterprise Server 12 SP3 <code class="filename">.iso</code> in the
       <code class="systemitem">ardana</code> home directory,
       <code class="filename">var/lib/ardana</code>, and rename it to
       <code class="filename">sles12sp3.iso</code>.
      </p></li></ol></li></ol></div></div></div><div class="sect1" id="sec.swift.setup_deployer"><div class="titlepage"><div><div><h2 class="title" id="sec.swift.setup_deployer"><span class="number">15.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setting Up the Cloud Lifecycle Manager</span> <a title="Permalink" class="permalink" href="#sec.swift.setup_deployer">#</a></h2></div></div></div><div class="sect2" id="idm139651560285616"><div class="titlepage"><div><div><h3 class="title" id="idm139651560285616"><span class="number">15.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing the Cloud Lifecycle Manager</span> <a title="Permalink" class="permalink" href="#idm139651560285616">#</a></h3></div></div></div><p>
    Running the <code class="command">ARDANA_INIT_AUTO=1</code> command is optional to
    avoid stopping for authentication at any step. You can also run
    <code class="command">ardana-init</code>to launch the Cloud Lifecycle Manager.  You will be prompted to
    enter an optional SSH passphrase, which is used to protect the key used by
    Ansible when connecting to its client nodes.  If you do not want to use a
    passphrase, press <span class="keycap">Enter</span> at the prompt.
   </p><p>
    If you have protected the SSH key with a passphrase, you can avoid having
    to enter the passphrase on every attempt by Ansible to connect to its
    client nodes with the following commands:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>eval $(ssh-agent)
<code class="prompt user">ardana &gt; </code>ssh-add ~/.ssh/id_rsa</pre></div><p>
    The Cloud Lifecycle Manager will contain the installation scripts and configuration files to
    deploy your cloud. You can set up the Cloud Lifecycle Manager on a dedicated node or you do
    so on your first controller node. The default choice is to use the first
    controller node as the Cloud Lifecycle Manager.
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Download the product from:
     </p><ol type="a" class="substeps "><li class="step "><p>
        <a class="link" href="https://scc.suse.com/" target="_blank">SUSE Customer Center</a>
       </p></li></ol></li><li class="step "><p>
      Boot your Cloud Lifecycle Manager from the SLES ISO contained in the download.
     </p></li><li class="step "><p>
      Enter <code class="literal">install</code> (all lower-case, exactly as spelled out
      here) to start installation.
     </p></li><li class="step "><p>
      Select the language. Note that only the English language selection is
      currently supported.
     </p></li><li class="step "><p>
      Select the location.
     </p></li><li class="step "><p>
      Select the keyboard layout.
     </p></li><li class="step "><p>
      Select the primary network interface, if prompted:
     </p><ol type="a" class="substeps "><li class="step "><p>
        Assign IP address, subnet mask, and default gateway
       </p></li></ol></li><li class="step "><p>
      Create new account:
     </p><ol type="a" class="substeps "><li class="step "><p>
        Enter a username.
       </p></li><li class="step "><p>
        Enter a password.
       </p></li><li class="step "><p>
        Enter time zone.
       </p></li></ol></li></ol></div></div><p>
    Once the initial installation is finished, complete the Cloud Lifecycle Manager setup with
    these steps:
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Ensure your Cloud Lifecycle Manager has a valid DNS nameserver specified in
      <code class="literal">/etc/resolv.conf</code>.
     </p></li><li class="step "><p>
      Set the environment variable LC_ALL:
     </p><div class="verbatim-wrap"><pre class="screen">export LC_ALL=C</pre></div><div id="idm139651560261392" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
       This can be added to <code class="filename">~/.bashrc</code> or
       <code class="filename">/etc/bash.bashrc</code>.
      </p></div></li></ol></div></div><p>
    The node should now have a working SLES setup.
   </p></div></div><div class="sect1" id="idm139651560258336"><div class="titlepage"><div><div><h2 class="title" id="idm139651560258336"><span class="number">15.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure Your Environment</span> <a title="Permalink" class="permalink" href="#idm139651560258336">#</a></h2></div></div></div><p>
   This part of the install is going to depend on the specific cloud
   configuration you are going to use.
  </p><p>
     Setup your configuration files, as follows:
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       See the sample sets of configuration files in the
       <code class="literal">~/openstack/examples/</code> directory. Each set will have an
       accompanying README.md file that explains the contents of each of the
       configuration files.
      </p></li><li class="step "><p>
       Copy the example configuration files into the required setup directory
       and edit them to contain the details of your environment:
      </p><div class="verbatim-wrap"><pre class="screen">cp -r ~/openstack/examples/entry-scale-swift/* \
  ~/openstack/my_cloud/definition/</pre></div></li><li class="step "><p>
       Begin inputting your environment information into the configuration
       files in the <code class="literal">~/openstack/my_cloud/definition</code>
       directory.
      </p><p>
       Full details of how to do this can be found here:
       <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 12 “Modifying Example Configurations for Object Storage using Swift”, Section 12.10 “Understanding Swift Ring Specifications”, Section 12.10.1 “Ring Specifications in the Input Model”</span>.
      </p><p>
       In many cases, the example models provide most of the data you need to
       create a valid input model. However, there are two important aspects you
       must plan and configure before starting a deploy as follows:
      </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
         Check the disk model used by your nodes. Specifically, check that all
         disk drives are correctly named and used as described in
         <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 12 “Modifying Example Configurations for Object Storage using Swift”, Section 12.6 “Swift Requirements for Device Group Drives”</span>.
        </p></li><li class="listitem "><p>
         Select an appropriate partition power for your rings. Detailed
         information about this is provided at
         <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 12 “Modifying Example Configurations for Object Storage using Swift”, Section 12.10 “Understanding Swift Ring Specifications”</span>.
        </p></li></ul></div></li></ol></div></div><p>
     Optionally, you can use the <code class="literal">ardanaencrypt.py</code> script to
     encrypt your IPMI passwords. This script uses OpenSSL.
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Change to the Ansible directory:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible</pre></div></li><li class="step "><p>
       Put the encryption key into the following environment variable:
      </p><div class="verbatim-wrap"><pre class="screen">export ARDANA_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key&gt;</pre></div></li><li class="step "><p>
       Run the python script below and follow the instructions. Enter a
       password that you want to encrypt.
      </p><div class="verbatim-wrap"><pre class="screen">ardanaencrypt.py</pre></div></li><li class="step "><p>
       Take the string generated and place it in the
       <code class="literal">"ilo_password"</code> field in your
       <code class="literal">~/openstack/my_cloud/definition/data/servers.yml</code> file,
       remembering to enclose it in quotes.
      </p></li><li class="step "><p>
       Repeat the above for each server.
      </p></li></ol></div></div><div id="idm139651560237680" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      Before you run any playbooks, remember that you need to export the
      encryption key in the following environment variable: <code class="literal">export
      ARDANA_USER_PASSWORD_ENCRYPT_KEY=&lt;encryption key&gt;</code>
     </p></div><p>
   Commit your configuration to the local git repo
   (<a class="xref" href="#using_git" title="Chapter 12. Using Git for Configuration Management">Chapter 12, <em>Using Git for Configuration Management</em></a>), as follows:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div><div id="idm139651560234592" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    This step needs to be repeated any time you make changes to your
    configuration files before you move onto the following steps. See
    <a class="xref" href="#using_git" title="Chapter 12. Using Git for Configuration Management">Chapter 12, <em>Using Git for Configuration Management</em></a> for more information.
   </p></div></div><div class="sect1" id="sec.swift.provision"><div class="titlepage"><div><div><h2 class="title" id="sec.swift.provision"><span class="number">15.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Provisioning Your Baremetal Nodes</span> <a title="Permalink" class="permalink" href="#sec.swift.provision">#</a></h2></div></div></div><p>
   To provision the baremetal nodes in your cloud deployment you can either use
   the automated operating system installation process provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> or
   you can use the 3rd party installation tooling of your choice. We will
   outline both methods below:
  </p><div class="sect2" id="idm139651560228544"><div class="titlepage"><div><div><h3 class="title" id="idm139651560228544"><span class="number">15.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using Third Party Baremetal Installers</span> <a title="Permalink" class="permalink" href="#idm139651560228544">#</a></h3></div></div></div><p>
    If you do not wish to use the automated operating system installation
    tooling included with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> then the requirements that have to be met
    using the installation tooling of your choice are:
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      The operating system must be installed via the SLES ISO provided on
      the <a class="link" href="https://scc.suse.com/" target="_blank">SUSE Customer Center</a>.
     </p></li><li class="listitem "><p>
      Each node must have SSH keys in place that allows the same user from the
      Cloud Lifecycle Manager node who will be doing the deployment to SSH to each node without a
      password.
     </p></li><li class="listitem "><p>
      Passwordless sudo needs to be enabled for the user.
     </p></li><li class="listitem "><p>
      There should be a LVM logical volume as <code class="literal">/root</code> on each
      node.
     </p></li><li class="listitem "><p>
      If the LVM volume group name for the volume group holding the
      <code class="literal">root</code> LVM logical volume is
      <code class="literal">ardana-vg</code>, then it will align with the disk input
      models in the examples.
     </p></li><li class="listitem "><p>
      <span class="phrase">Ensure that <code class="literal">openssh-server</code>,
      <code class="literal">python</code>, <code class="literal">python-apt</code>, and
      <code class="literal">rsync</code> are installed.</span>
     </p></li></ul></div><p>
    If you chose this method for installing your baremetal hardware, skip
    forward to the step
    <em class="citetitle ">Running the Configuration Processor</em>.
   </p></div><div class="sect2" id="idm139651560213344"><div class="titlepage"><div><div><h3 class="title" id="idm139651560213344"><span class="number">15.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Automated Operating System Installation Provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span></span> <a title="Permalink" class="permalink" href="#idm139651560213344">#</a></h3></div></div></div><p>
    If you would like to use the automated operating system installation tools
    provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>, complete the steps below.
   </p><div class="sect3" id="idm139651560208928"><div class="titlepage"><div><div><h4 class="title" id="idm139651560208928"><span class="number">15.5.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying Cobbler</span> <a title="Permalink" class="permalink" href="#idm139651560208928">#</a></h4></div></div></div><p>
     This phase of the install process takes the baremetal information that was
     provided in <code class="literal">servers.yml</code> and installs the Cobbler
     provisioning tool and loads this information into Cobbler. This sets each
     node to <code class="literal">netboot-enabled: true</code> in Cobbler. Each node
     will be automatically marked as <code class="literal">netboot-enabled: false</code>
     when it completes its operating system install successfully. Even if the
     node tries to PXE boot subsequently, Cobbler will not serve it. This is
     deliberate so that you can't reimage a live node by accident.
    </p><p>
     The <code class="literal">cobbler-deploy.yml</code> playbook prompts for a password
     - this is the password that will be encrypted and stored in Cobbler, which
     is associated with the user running the command on the Cloud Lifecycle Manager, that you
     will use to log in to the nodes via their consoles after install. The
     username is the same as the user set up in the initial dialogue when
     installing the Cloud Lifecycle Manager from the ISO, and is the same user that is running
     the cobbler-deploy play.
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Run the following playbook which confirms that there is IPMI connectivity
       for each of your nodes so that they are accessible to be re-imaged in a
       later step:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-power-status.yml</pre></div></li><li class="step "><p>
       Run the following playbook to deploy Cobbler:
      </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li></ol></div></div></div><div class="sect3" id="idm139651560200960"><div class="titlepage"><div><div><h4 class="title" id="idm139651560200960"><span class="number">15.5.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Imaging the Nodes</span> <a title="Permalink" class="permalink" href="#idm139651560200960">#</a></h4></div></div></div><p>
     This phase of the install process goes through a number of distinct steps:
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Powers down the nodes to be installed
      </p></li><li class="step "><p>
       Sets the nodes hardware boot order so that the first option is a network
       boot.
      </p></li><li class="step "><p>
       Powers on the nodes. (The nodes will then boot from the network and be
       installed using infrastructure set up in the previous phase)
      </p></li><li class="step "><p>
       Waits for the nodes to power themselves down (this indicates a success
       install). This can take some time.
      </p></li><li class="step "><p>
       Sets the boot order to hard disk and powers on the nodes.
      </p></li><li class="step "><p>
       Waits for the nodes to be ssh-able and verifies that they have the
       signature expected.
      </p></li></ol></div></div><p>
     The reimaging command is:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml \
  [-e nodelist=node1,node2,node3]</pre></div><p>
     If a nodelist is not specified then the set of nodes in cobbler with
     <code class="literal">netboot-enabled: True</code> is selected. The playbook pauses
     at the start to give you a chance to review the set of nodes that it is
     targeting and to confirm that it is correct.
    </p><p>
     You can use the command below which will list all of your nodes with the
     <code class="literal">netboot-enabled: True</code> flag set:
    </p><div class="verbatim-wrap"><pre class="screen">sudo cobbler system find --netboot-enabled=1</pre></div></div></div></div><div class="sect1" id="sec.swift.config_processor"><div class="titlepage"><div><div><h2 class="title" id="sec.swift.config_processor"><span class="number">15.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Running the Configuration Processor</span> <a title="Permalink" class="permalink" href="#sec.swift.config_processor">#</a></h2></div></div></div><p>
   Once you have your configuration files setup, you need to run the
   configuration processor to complete your configuration.
  </p><p>
   When you run the configuration processor, you will be prompted for two
   passwords. Enter the first password to make the configuration processor
   encrypt its sensitive data, which consists of the random inter-service
   passwords that it generates and the ansible <code class="literal">group_vars</code>
   and <code class="literal">host_vars</code> that it produces for subsequent deploy
   runs. You will need this password for subsequent Ansible deploy and
   configuration processor runs. If you wish to change an encryption password
   that you have already used when running the configuration processor then
   enter the new password at the second prompt, otherwise just press
   <span class="keycap">Enter</span> to bypass this.
  </p><p>
   Run the configuration processor with this command:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
   For automated installation (for example CI), you can specify the required
   passwords on the ansible command line. For example, the command below will
   disable encryption by the configuration processor:
  </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost config-processor-run.yml \
  -e encrypt="" -e rekey=""</pre></div><p>
   If you receive an error during this step, there is probably an issue with
   one or more of your configuration files. Verify that all information in each
   of your configuration files is correct for your environment. Then commit
   those changes to Git using the instructions in the previous section before
   re-running the configuration processor again.
  </p><p>
   For any troubleshooting information regarding these steps, see
   <a class="xref" href="#sec.trouble-config_processor" title="19.2. Issues while Updating Configuration Files">Section 19.2, “Issues while Updating Configuration Files”</a>.
  </p></div><div class="sect1" id="sec.swift.deploy"><div class="titlepage"><div><div><h2 class="title" id="sec.swift.deploy"><span class="number">15.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying the Cloud</span> <a title="Permalink" class="permalink" href="#sec.swift.deploy">#</a></h2></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Use the playbook below to create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     [OPTIONAL] - Run the <code class="literal">wipe_disks.yml</code> playbook to ensure
     all of your partitions on your nodes are completely wiped before
     continuing with the installation. If you are using fresh machines this
     step may not be necessary.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts wipe_disks.yml</pre></div><p>
     If you have used an encryption password when running the configuration
     processor use the command below and enter the encryption password when
     prompted:
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts wipe_disks.yml --ask-vault-pass</pre></div></li><li class="step "><p>
     Run the <code class="literal">site.yml</code> playbook below:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts site.yml</pre></div><p>
     If you have used an encryption password when running the configuration
     processor use the command below and enter the encryption password when
     prompted:
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts site.yml --ask-vault-pass</pre></div><div id="idm139651560167904" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      The step above runs <code class="literal">osconfig</code> to configure the cloud
      and <code class="literal">ardana-deploy</code> to deploy the cloud. Therefore, this
      step may run for a while, perhaps 45 minutes or more, depending on the
      number of nodes in your environment.
     </p></div></li><li class="step "><p>
     Verify that the network is working correctly. Ping each IP in the
     <code class="literal">/etc/hosts</code> file from one of the controller nodes.
    </p></li></ol></div></div><p>
   For any troubleshooting information regarding these steps, see
   <a class="xref" href="#sec.trouble-deploy_cloud" title="19.3. Issues while Deploying the Cloud">Section 19.3, “Issues while Deploying the Cloud”</a>.
  </p></div><div class="sect1" id="sec.swift.post_installation"><div class="titlepage"><div><div><h2 class="title" id="sec.swift.post_installation"><span class="number">15.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Post-Installation Verification and Administration</span> <a title="Permalink" class="permalink" href="#sec.swift.post_installation">#</a></h2></div></div></div><p>
   We recommend verifying the installation using the instructions in
   <a class="xref" href="#cloud_verification" title="Chapter 22. Cloud Verification">Chapter 22, <em>Cloud Verification</em></a>.
  </p><p>
   There are also a list of other common post-installation administrative tasks
   listed in the <a class="xref" href="#postinstall_checklist" title="Chapter 28. Other Common Post-Installation Tasks">Chapter 28, <em>Other Common Post-Installation Tasks</em></a> list.
  </p></div></div><div class="chapter " id="install_sles_compute"><div class="titlepage"><div><div><h2 class="title"><span class="number">16 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing SLES Compute</span> <a title="Permalink" class="permalink" href="#install_sles_compute">#</a></h2></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#sles_overview"><span class="number">16.1 </span><span class="name">SLES Compute Node Installation Overview</span></a></span></dt><dt><span class="section"><a href="#sles_support"><span class="number">16.2 </span><span class="name">SLES Support</span></a></span></dt><dt><span class="section"><a href="#install_sles"><span class="number">16.3 </span><span class="name">Using the Cloud Lifecycle Manager to Deploy SLES Compute Nodes</span></a></span></dt><dt><span class="section"><a href="#provisioning_sles"><span class="number">16.4 </span><span class="name">Provisioning SLES Yourself</span></a></span></dt></dl></div></div><div class="sect1" id="sles_overview"><div class="titlepage"><div><div><h2 class="title" id="sles_overview"><span class="number">16.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SLES Compute Node Installation Overview</span> <a title="Permalink" class="permalink" href="#sles_overview">#</a></h2></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> supports SLES compute nodes, specifically SUSE Linux Enterprise Server 12 SP3. <span class="phrase"><span class="phrase">SUSE</span></span>
  does not ship a SLES ISO with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> so you will need to download a copy of
  the SLES ISO (<code class="filename">SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso</code>)
  and the SLES SDK ISO
  (<code class="filename">SLE-12-SP3-SDK-DVD-x86_64-GM-DVD1.iso</code>) from SUSE. You
  can use the following
  link to download the ISO. To do so, either log in or create a SUSE
  account before downloading:
  <a class="link" href="https://www.suse.com/products/server/download/" target="_blank">https://www.suse.com/products/server/download/</a>.
 </p><p>
  There are two approaches for deploying SLES compute nodes in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    Using the Cloud Lifecycle Manager to automatically deploy SLES Compute Nodes.
   </p></li><li class="listitem "><p>
    Provisioning SLES nodes yourself, either manually or using a third-party
    tool, and then providing the relevant information to the Cloud Lifecycle Manager.
   </p></li></ul></div><p>
  These two approaches can be used whether you are installing a cloud for the
  first time or adding a compute node to an existing cloud. Regardless of your
  approach, you should be certain to register your SLES compute nodes in order
  to get product updates as they come available. For more information, see
  <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 1 “Registering SLES”</span>.
 </p></div><div class="sect1" id="sles_support"><div class="titlepage"><div><div><h2 class="title" id="sles_support"><span class="number">16.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SLES Support</span> <a title="Permalink" class="permalink" href="#sles_support">#</a></h2></div></div></div><p>
  SUSE Linux Enterprise Server (SLES) Host OS KVM and/or supported SLES guests
  have been tested and qualified by <span class="phrase"><span class="phrase">SUSE</span></span> to run on <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
  
 </p></div><div class="sect1" id="install_sles"><div class="titlepage"><div><div><h2 class="title" id="install_sles"><span class="number">16.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Cloud Lifecycle Manager to Deploy SLES Compute Nodes</span> <a title="Permalink" class="permalink" href="#install_sles">#</a></h2></div></div></div><p>
  The method used for deploying SLES compute nodes using Cobbler on the
  Cloud Lifecycle Manager uses legacy BIOS.
 </p><div id="idm139651560131424" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   UEFI and Secure boot are not supported on SLES Compute.
  </p></div><div class="sect2" id="idm139651560130480"><div class="titlepage"><div><div><h3 class="title" id="idm139651560130480"><span class="number">16.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying legacy BIOS SLES Compute nodes</span> <a title="Permalink" class="permalink" href="#idm139651560130480">#</a></h3></div></div></div><p>
   The installation process for legacy BIOS SLES Compute nodes is similar to
   that described in <a class="xref" href="#install_kvm" title="Chapter 13. Installing Mid-scale and Entry-scale KVM">Chapter 13, <em>Installing Mid-scale and Entry-scale KVM</em></a> with some additional requirements:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     The standard SLES ISO (SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso) must be
     accessible via <code class="literal">/home/stack/sles12sp3.iso</code>. Rename the ISO
     or create a symbolic link:
    </p><div class="verbatim-wrap"><pre class="screen">mv SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso /home/stack/sles12sp3.iso</pre></div></li><li class="listitem "><p>
     You must identify the node(s) on which you want to install SLES, by adding
     the key/value pair <code class="literal">distro-id: sles12sp3-x86_64</code> to
     server details in <code class="literal">servers.yml</code>. You will also need to
     update <code class="literal">net_interfaces.yml</code>,
     <code class="literal">server_roles.yml</code>, <code class="literal">disk_compute.yml</code>
     and <code class="literal">control_plane.yml</code>. For more information on
     configuration of the Input Model for SLES, see
     <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 11 “Modifying Example Configurations for Compute Nodes”, Section 11.1 “SLES Compute Nodes”</span>.
    </p></li></ul></div></div><div class="sect2" id="sles_uefi_overview"><div class="titlepage"><div><div><h3 class="title" id="sles_uefi_overview"><span class="number">16.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploying UEFI SLES compute nodes</span> <a title="Permalink" class="permalink" href="#sles_uefi_overview">#</a></h3></div></div></div><p>
   Deplyoing UEFI nodes has been automated in the Cloud Lifecycle Manager and requires the
   following to be met:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     All of your nodes using SLES must already be installed, either manually
     or via Cobbler.
    </p></li><li class="listitem "><p>
     Your input model should be configured for your SLES nodes, per the
     instructions at <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 11 “Modifying Example Configurations for Compute Nodes”, Section 11.1 “SLES Compute Nodes”</span>.
    </p></li><li class="listitem "><p>
     You should have run the configuration processor and the
     <code class="filename">ready-deployment.yml</code> playbook.
    </p></li></ul></div><p>
   Execute the following steps to re-image one or more nodes after you have run
   the <code class="filename">ready-deployment.yml</code> playbook.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Run the following playbook, ensuring that you specify only your UEFI
     SLES nodes using the nodelist. This playbook will reconfigure Cobbler
     for the nodes listed.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook prepare-sles-grub2.yml -e nodelist=node1[,node2,node3]</pre></div></li><li class="step "><p>
     Re-image the node(s), ensuring that you only specify your UEFI SLES
     nodes using the nodelist.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml \
-e nodelist=node1[,node2,node3]</pre></div></li><li class="step "><p>
     Make backups of the <code class="filename">grub.cfg-*</code> and
     <code class="filename">grubx64.efi</code> files in
     <code class="filename">/srv/tftp/grub/</code> as they will be overwritten when
     running the cobbler-deploy playbook on the next step. You will need these
     files if you need to reimage the nodes in the future.
    </p></li><li class="step "><p>
     Run the <code class="filename">cobbler-deploy.yml</code> playbook, which will reset
     Cobbler back to the default values:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</pre></div></li></ol></div></div><div class="sect3" id="idm139651560106704"><div class="titlepage"><div><div><h4 class="title" id="idm139651560106704"><span class="number">16.3.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">UEFI Secure Boot</span> <a title="Permalink" class="permalink" href="#idm139651560106704">#</a></h4></div></div></div><p>
    Secure Boot is a method used to restrict binaries execution for booting the
    system. With this option enabled, system BIOS will only allow boot loaders
    with trusted cryptographic signatures to be executed, thus enable
    preventing malware from hiding embedded code in the boot chain. Each boot
    loader launched during the boot process is digitally signed and that
    signature is validated against a set of trusted certificates embedded in
    the UEFI BIOS.  Secure Boot is completely implemented in the BIOS and does
    not require special hardware.
   </p><p>Thus Secure Boot is:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      Intended to prevent boot-sector malware or kernel code injection.
     </p></li><li class="listitem "><p>Hardware-based code signing.</p></li><li class="listitem "><p>Extension of the UEFI BIOS architecture.</p></li><li class="listitem "><p>
      Optional with the ability to enable or disable it through the BIOS.
     </p></li></ul></div><p>
    In Boot Options of RBSU, <span class="guimenu">Boot Mode</span> needs to be set to
    <code class="literal">UEFI Mode</code> and <span class="guimenu">UEFI Optimized Boot</span>
    should be <code class="literal">Enabled</code>&gt;.
   </p><p>
    Secure Boot is enabled at
         <span class="guimenu">System Configuration</span> › <span class="guimenu">BIOS/Platform Configuration (RBSU)</span> › <span class="guimenu">Server
         Security</span> › <span class="guimenu">Secure Boot Configuration</span> › <span class="guimenu">Secure Boot Enforcement</span>.
   </p></div></div></div><div class="sect1" id="provisioning_sles"><div class="titlepage"><div><div><h2 class="title" id="provisioning_sles"><span class="number">16.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Provisioning SLES Yourself</span> <a title="Permalink" class="permalink" href="#provisioning_sles">#</a></h2></div></div></div><p>
  This section outlines the steps needed to manually provision a SLES node so
  that it can be added to a new or existing <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> cloud.
 </p><div class="sect2" id="idm139651560089264"><div class="titlepage"><div><div><h3 class="title" id="idm139651560089264"><span class="number">16.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure Cloud Lifecycle Manager to Enable SLES</span> <a title="Permalink" class="permalink" href="#idm139651560089264">#</a></h3></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Take note of the IP address of the Cloud Lifecycle Manager node. It will be used below
     during <a class="xref" href="#sec.provisioning_sles.add_zypper" title="16.4.6. Add zypper repository">Section 16.4.6, “Add zypper repository”</a>.
    </p></li><li class="step "><p>
     Mount or copy the contents of
     <code class="filename">SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso</code> to
     <code class="literal">/srv/www/suse-12.3/x86_64/repos/ardana/sles12/zypper/OS/</code>
    </p></li><li class="step "><p>
     Mount or copy the contents of
     <code class="filename">SLE-12-SP3-SDK-DVD-x86_64-GM-DVD1.iso</code> to
     <code class="literal">/srv/www/suse-12.3/x86_64/repos/ardana/sles12/zypper/SDK/</code>
    </p></li></ol></div></div><div id="idm139651560082688" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    If you choose to mount an ISO, we recommend creating an <code class="filename">/etc/fstab</code> entry to
    ensure the ISO is mounted after a reboot.
   </p></div></div><div class="sect2" id="idm139651560081088"><div class="titlepage"><div><div><h3 class="title" id="idm139651560081088"><span class="number">16.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Install SUSE Linux Enterprise Server 12 SP3</span> <a title="Permalink" class="permalink" href="#idm139651560081088">#</a></h3></div></div></div><p>
   Install SUSE Linux Enterprise Server 12 SP3 using the standard iso
   (<code class="filename">SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso</code>)
  </p></div><div class="sect2" id="idm139651560079168"><div class="titlepage"><div><div><h3 class="title" id="idm139651560079168"><span class="number">16.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Assign a static IP</span> <a title="Permalink" class="permalink" href="#idm139651560079168">#</a></h3></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Use the <code class="literal">ip addr</code> command to find out what network
     devices are on your system:
    </p><div class="verbatim-wrap"><pre class="screen">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: <span class="bold"><strong>eno1</strong></span>: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000
    link/ether <span class="bold"><strong>f0:92:1c:05:89:70</strong></span> brd ff:ff:ff:ff:ff:ff
    inet 10.13.111.178/26 brd 10.13.111.191 scope global eno1
       valid_lft forever preferred_lft forever
    inet6 fe80::f292:1cff:fe05:8970/64 scope link
       valid_lft forever preferred_lft forever
3: eno2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000
    link/ether f0:92:1c:05:89:74 brd ff:ff:ff:ff:ff:ff</pre></div></li><li class="step "><p>
     Identify the one that matches the MAC address of your server and edit the
     corresponding config file in
     <code class="literal">/etc/sysconfig/network-scripts</code>.
    </p><div class="verbatim-wrap"><pre class="screen">vi /etc/sysconfig/network-scripts/<span class="bold"><strong>ifcfg-eno1</strong></span></pre></div></li><li class="step "><p>
     Edit the <code class="literal">IPADDR</code> and <code class="literal">NETMASK</code> values
     to match your environment. Note that the <code class="literal">IPADDR</code> is used
     in the corresponding stanza in <code class="literal">servers.yml</code>. You may
     also need to set <code class="literal">BOOTPROTO</code> to <code class="literal">none</code>.
    </p><div class="verbatim-wrap"><pre class="screen">TYPE=Ethernet
<span class="bold"><strong>BOOTPROTO=none</strong></span>
DEFROUTE=yes
PEERDNS=yes
PEERROUTES=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_PEERDNS=yes
IPV6_PEERROUTES=yes
IPV6_FAILURE_FATAL=no
NAME=eno1
UUID=36060f7a-12da-469b-a1da-ee730a3b1d7c
DEVICE=eno1
ONBOOT=yes
<span class="bold"><strong>NETMASK=255.255.255.192</strong></span>
<span class="bold"><strong>IPADDR=10.13.111.14</strong></span></pre></div></li><li class="step "><p>
     [OPTIONAL] Reboot your SLES node and ensure that it can be accessed from
     the Cloud Lifecycle Manager.
    </p></li></ol></div></div></div><div class="sect2" id="idm139651560063552"><div class="titlepage"><div><div><h3 class="title" id="idm139651560063552"><span class="number">16.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Add <code class="literal">ardana</code> user and home directory</span> <a title="Permalink" class="permalink" href="#idm139651560063552">#</a></h3></div></div></div><div class="verbatim-wrap"><pre class="screen">useradd -m ardana
passwd ardana</pre></div></div><div class="sect2" id="idm139651560061728"><div class="titlepage"><div><div><h3 class="title" id="idm139651560061728"><span class="number">16.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Allow user <code class="literal">ardana</code> to <code class="literal">sudo</code> without password</span> <a title="Permalink" class="permalink" href="#idm139651560061728">#</a></h3></div></div></div><p>
    Setting up sudo on SLES is covered in the <em class="citetitle ">SLES Administration Guide</em> at
    <a class="link" href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/sec_sudo_conf.html" target="_blank">https://www.suse.com/documentation/sles-12/book_sle_admin/data/sec_sudo_conf.html</a>.
   </p><p>
    The recommendation is to create user specific <code class="command">sudo</code> config files under
    <code class="filename">/etc/sudoers.d</code>, therefore creating an <code class="filename">/etc/sudoers.d/ardana</code> config file with
    the following content will allow sudo commands without the requirement of a
    password.
   </p><div class="verbatim-wrap"><pre class="screen">ardana ALL=(ALL) NOPASSWD:ALL</pre></div></div><div class="sect2" id="sec.provisioning_sles.add_zypper"><div class="titlepage"><div><div><h3 class="title" id="sec.provisioning_sles.add_zypper"><span class="number">16.4.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Add zypper repository</span> <a title="Permalink" class="permalink" href="#sec.provisioning_sles.add_zypper">#</a></h3></div></div></div><p>
   Using the ISO-based repositories created above, add the zypper repositories.
  </p><p>
   Follow these steps. Update the value of deployer_ip as necessary.
  </p><div class="verbatim-wrap"><pre class="screen">deployer_ip=192.168.10.254
zypper addrepo --no-gpgcheck --refresh http://$deployer_ip:79/ardana/sles12/zypper/OS SLES-OS
zypper addrepo --no-gpgcheck --refresh http://$deployer_ip:79/ardana/sles12/zypper/SDK SLES-SDK</pre></div><p>
   To verify that the repositories have been added, run:
  </p><div class="verbatim-wrap"><pre class="screen">zypper repos --detail</pre></div><p>
   For more information about Zypper, see the
   <em class="citetitle ">SLES Administration Guide</em> at
   <a class="link" href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/sec_zypper.html" target="_blank">https://www.suse.com/documentation/sles-12/book_sle_admin/data/sec_zypper.html</a>.
  </p><div id="idm139651560050240" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
    If you intend on attaching encrypted volumes to any of your SLES
    Compute nodes, install the cryptographic libraries through cryptsetup on
    each node. Run the following command to install the necessary
    cryptographic libraries:
   </p><div class="verbatim-wrap"><pre class="screen">sudo zypper in cryptsetup</pre></div></div></div><div class="sect2" id="idm139651560048496"><div class="titlepage"><div><div><h3 class="title" id="idm139651560048496"><span class="number">16.4.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Add Required Packages</span> <a title="Permalink" class="permalink" href="#idm139651560048496">#</a></h3></div></div></div><p>
   As documented in <a class="xref" href="#sec.kvm.provision" title="13.4. Provisioning Your Baremetal Nodes">Section 13.4, “Provisioning Your Baremetal Nodes”</a>,
   you need to add extra packages.
   <span class="phrase">Ensure that <code class="literal">openssh-server</code>,
   <code class="literal">python</code>,
   and <code class="literal">rsync</code> are installed.</span>
  </p></div><div class="sect2" id="idm139651560044768"><div class="titlepage"><div><div><h3 class="title" id="idm139651560044768"><span class="number">16.4.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Set up passwordless SSH access</span> <a title="Permalink" class="permalink" href="#idm139651560044768">#</a></h3></div></div></div><p>
   Once you have started your installation using the Cloud Lifecycle Manager, or if
   you are adding a SLES node to an existing cloud, you need to copy the
   Cloud Lifecycle Manager public key to the SLES node. One way of doing this is to
   copy the <code class="literal">/home/ardana/.ssh/authorized_keys</code> from another
   node in the cloud to the same location on the SLES node. If you are
   installing a new cloud, this file will be available on the nodes after
   running the <code class="literal">bm-reimage.yml</code> playbook.
  </p><div id="idm139651560042256" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    Ensure that there is global read access to the file
    <code class="filename">/home/ardana/.ssh/authorized_keys</code>.
   </p></div><p>
   Now test passwordless SSH from the deployer and check your ability to
   remotely execute sudo commands:
  </p><div class="verbatim-wrap"><pre class="screen">ssh ardana@<em class="replaceable ">IP_OF_SLES_NODE</em> "sudo tail -5 /var/log/messages"</pre></div></div></div></div><div class="chapter " id="install_heat_templates"><div class="titlepage"><div><div><h2 class="title"><span class="number">17 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installation of SUSE CaaS Platform Heat Templates</span> <a title="Permalink" class="permalink" href="#install_heat_templates">#</a></h2></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#sec.heat.templates.install"><span class="number">17.1 </span><span class="name">SUSE CaaS Platform Heat Installation Procedure</span></a></span></dt></dl></div></div><p>
  This chapter describes how to install SUSE CaaS Platform Heat template on
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
 </p><div class="sect1" id="sec.heat.templates.install"><div class="titlepage"><div><div><h2 class="title" id="sec.heat.templates.install"><span class="number">17.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SUSE CaaS Platform Heat Installation Procedure</span> <a title="Permalink" class="permalink" href="#sec.heat.templates.install">#</a></h2></div></div></div><div class="procedure " id="idm139651560032976"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 17.1: </span><span class="name">Preparation </span><a title="Permalink" class="permalink" href="#idm139651560032976">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Download the latest SUSE CaaS Platform for <span class="productname">OpenStack</span> image (for example,
     <code class="filename">SUSE-CaaS-Platform-2.0-OpenStack-Cloud.x86_64-1.0.0-GM.qcow2</code>)
     from <a class="link" href="https://download.suse.com" target="_blank">https://download.suse.com</a>.
    </p></li><li class="step "><p>
     Upload the image to Glance:
    </p><div class="verbatim-wrap"><pre class="screen">openstack image create --public --disk-format qcow2 --container-format \
bare --file SUSE-CaaS-Platform-2.0-for-OpenStack-Cloud.x86_64-2.0.0-GM.qcow2 \
CaaSP-2</pre></div></li><li class="step "><p>
     Install the <span class="package">caasp-openstack-heat-templates</span> package on a
     machine with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> repositories:
    </p><div class="verbatim-wrap"><pre class="screen">zypper in caasp-openstack-heat-templates</pre></div><p>
     The installed templates are located in
     <code class="filename">/usr/share/caasp-openstack-heat-templates</code>.
    </p></li></ol></div></div><div class="procedure " id="idm139651560023504"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 17.2: </span><span class="name">Installing Templates via Horizon </span><a title="Permalink" class="permalink" href="#idm139651560023504">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     In Horizon, go to
     <span class="guimenu">Project</span> › <span class="guimenu">Stacks</span> › <span class="guimenu">Launch
     Stack</span>.
    </p></li><li class="step "><p>
     Select <span class="guimenu">File</span> from the <span class="guimenu">Template Source</span>
     drop-down box and upload the <code class="filename">caasp-stack.yaml</code> file.
    </p></li><li class="step "><p>
     In the <span class="guimenu">Launch Stack</span> dialog, provide the required
     information (stack name, password, flavor size, external network of your
     environment, etc.).
    </p></li><li class="step "><p>
     Click <span class="guimenu">Launch</span> to launch the stack. This creates all
     required resources for running SUSE CaaS Platform in an <span class="productname">OpenStack</span> environment. The
     stack creates one Admin Node, one Master Node, and server worker nodes as
     specified.
    </p></li></ol></div></div><div class="procedure " id="idm139651560014560"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 17.3: </span><span class="name">Install Templates from the Command Line </span><a title="Permalink" class="permalink" href="#idm139651560014560">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Specify the appropriate flavor and network settings in the
     <code class="filename">caasp-environment.yaml</code> file.
    </p></li><li class="step "><p>
     Create a stack in Heat by passing the template, environment file, and
     parameters:
    </p><div class="verbatim-wrap"><pre class="screen">openstack stack create -t caasp-stack.yaml -e caasp-environment.yaml \
--parameter image=CaaSP-2 caasp-stack</pre></div></li></ol></div></div><div class="procedure " id="idm139651560010736"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="number">Procedure 17.4: </span><span class="name">Accessing Velum SUSE CaaS Platform dashboard </span><a title="Permalink" class="permalink" href="#idm139651560010736">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     After the stack has been created, the Velum SUSE CaaS Platform dashboard runs on the Admin Node.
     You can access it using the Admin Node's floating IP address.
    </p></li><li class="step "><p>
     Create an account and follow the steps in the Velum SUSE CaaS Platform dashboard to complete the
     SUSE CaaS Platform installation.
    </p></li></ol></div></div><p>
   When you have successfully accessed the admin node web interface via the
   floating IP, follow the instructions at <a class="link" href="https://www.suse.com/documentation +    /suse-caasp-2/book_caasp_deployment/data/book_caasp_deployment.html" target="_blank">https://www.suse.com/documentation +    /suse-caasp-2/book_caasp_deployment/data/book_caasp_deployment.html</a> to
   continue the set up of SUSE CaaS Platform.
  </p></div></div><div class="chapter " id="integrations"><div class="titlepage"><div><div><h2 class="title"><span class="number">18 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Integrations</span> <a title="Permalink" class="permalink" href="#integrations">#</a></h2></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#config_3par"><span class="number">18.1 </span><span class="name">Configuring for 3PAR Block Storage Backend</span></a></span></dt><dt><span class="section"><a href="#ironic_oneview_integration"><span class="number">18.2 </span><span class="name">Ironic HPE OneView Integration</span></a></span></dt><dt><span class="section"><a href="#ses.integration"><span class="number">18.3 </span><span class="name">SUSE Enterprise Storage Integration</span></a></span></dt></dl></div></div><p>
  Once you have completed your cloud installation, these are some of the common
  integrations you may want to perform.
 </p><div class="sect1" id="config_3par"><div class="titlepage"><div><div><h2 class="title" id="config_3par"><span class="number">18.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring for 3PAR Block Storage Backend</span> <a title="Permalink" class="permalink" href="#config_3par">#</a></h2></div></div></div><p>
  This page describes how to configure your 3PAR backend for the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
  Entry-scale with KVM cloud model.
 </p><div class="sect2" id="idg-installation-installation-configure_3par-xml-7"><div class="titlepage"><div><div><h3 class="title" id="idg-installation-installation-configure_3par-xml-7"><span class="number">18.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#idg-installation-installation-configure_3par-xml-7">#</a></h3></div></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     You must have the license for the following software before you start your
     3PAR backend configuration for the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale with KVM cloud
     model:
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       Thin Provisioning
      </p></li><li class="listitem "><p>
       Virtual Copy
      </p></li><li class="listitem "><p>
       System Reporter
      </p></li><li class="listitem "><p>
       Dynamic Optimization
      </p></li><li class="listitem "><p>
       Priority Optimization
      </p></li></ul></div></li><li class="listitem "><p>
     Your <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Entry-scale KVM Cloud should be up and running.
     Installation steps can be found in
     <a class="xref" href="#install_kvm" title="Chapter 13. Installing Mid-scale and Entry-scale KVM">Chapter 13, <em>Installing Mid-scale and Entry-scale KVM</em></a>.
    </p></li><li class="listitem "><p>
     Your 3PAR Storage Array should be available in the cloud management
     network or routed to the cloud management network and the 3PAR FC and
     iSCSI ports configured.
    </p></li><li class="listitem "><p>
     The 3PAR management IP and iSCSI port IPs must have connectivity from the
     controller and compute nodes.
    </p></li><li class="listitem "><p>
     Please refer to the system requirements for 3PAR in the OpenStack
     configuration guide, which can be found here:
     <a class="link" href="http://docs.openstack.org/liberty/config-reference/content/hp-3par-sys-reqs.html" target="_blank">3PAR
     System Requirements</a>.
    </p></li></ul></div></div><div class="sect2" id="idg-installation-installation-configure_3par-xml-9"><div class="titlepage"><div><div><h3 class="title" id="idg-installation-installation-configure_3par-xml-9"><span class="number">18.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Notes</span> <a title="Permalink" class="permalink" href="#idg-installation-installation-configure_3par-xml-9">#</a></h3></div></div></div><p>
   <span class="bold"><strong>Encrypted 3Par Volume</strong></span>: Attaching an
   encrypted 3Par volume is possible after installation by setting
   <code class="literal">volume_use_multipath = true</code> under the libvirt stanza in
   the <code class="literal">nova/kvm-hypervisor.conf.j2</code> file and reconfigure
   nova.
  </p><p>
   <span class="bold"><strong>Concerning using multiple backends:</strong></span> If you
   are using multiple backend options, ensure that you specify each of the
   backends you are using when configuring your
   <code class="literal">cinder.conf.j2</code> file using a comma-delimited list.
   Also create multiple volume types so you can specify a backend to utilize
   when creating volumes. Instructions are included below.
   
   
  </p><p>
   <span class="bold"><strong>Concerning iSCSI and Fiber Channel:</strong></span> You
   should not configure cinder backends so that multipath volumes are exported
   over both iSCSI and Fiber Channel from a 3PAR backend to the same Nova
   compute server.
  </p><p>
   <span class="bold"><strong>3PAR driver has updated name:</strong></span> In the
   OpenStack Mitaka release, the 3PAR driver used for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> integration
   had its name updated from <code class="literal">HP3PARFCDriver</code> and
   <code class="literal">HP3PARISCSIDriver</code> to <code class="literal">HPE3PARFCDriver</code>
   and <code class="literal">HPE3PARISCSIDriver</code>. To prevent issues when upgrading
   from previous <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> releases, we left the names as-is in the release and
   provided a mapping so that the integration would continue to work. This will
   produce a warning in the <code class="literal">cinder-volume.log</code> file advising
   you of the deprecated name. The warning will look similar to this:
   <span class="quote">“<span class="quote">Option "hp3par_api_url" from group
   "&lt;<em class="replaceable ">YOUR_SECTION</em>&gt;" is
   deprecated. Use option "hpe3par_api_url" from group
   "&lt;<em class="replaceable ">YOUR_SECTION</em>&gt;"'</span>”</span>.
  </p><p>
   These are just warnings and can be ignored.
  </p></div><div class="sect2" id="sec.3par-multipath"><div class="titlepage"><div><div><h3 class="title" id="sec.3par-multipath"><span class="number">18.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Multipath Support</span> <a title="Permalink" class="permalink" href="#sec.3par-multipath">#</a></h3></div></div></div><p>
   If you want to enable multipath for Cinder volumes carved from 3PAR FC/iSCSI
   storage system, please go through the
   <code class="filename">~/openstack/ardana/ansible/roles/multipath/README.md</code>
   file on the Cloud Lifecycle Manager. The <code class="filename">README.md</code> file contains
   detailed procedures for configuring multipath for 3PAR FC/iSCSI Cinder
   volumes.
  </p><p>
   We have also included an additional set of steps needed if you are using
   3PAR FC/iSCSI multipath which is included below.
  </p><p>
   If you are using 3PAR FC/iSCSI multipath, an additional configuration is
   required:
  </p><p>
   If you are planning on attaching an encrypted 3PAR volume after
   installation, ensure that you <code class="literal">volume_use_multipath =
   true</code> under the libvirt section in the
   <code class="literal">nova/kvm-hypervisor.conf.j2</code> file before configuring
   Cinder.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Edit the
     <code class="literal">~/openstack/my_cloud/config/nova/kvm-hypervisor.conf.j2</code>
     file add this line under the <code class="literal">[libvirt]</code> section:
    </p><p>
     Example:
    </p><div class="verbatim-wrap"><pre class="screen">[libvirt]
...
iscsi_use_multipath=true</pre></div></li><li class="step "><p>
     Edit the file
     <code class="literal">~/openstack/my_cloud/config/cinder/cinder.conf.j2</code>
     and add this line under the <code class="literal">[DEFAULT]</code> section:
    </p><p>
     Example:
    </p><div class="verbatim-wrap"><pre class="screen">[DEFAULT]
...
use_multipath_for_image_xfer=true</pre></div></li><li class="step "><p>
     Commit your configuration to the local git repo
     (<a class="xref" href="#using_git" title="Chapter 12. Using Git for Configuration Management">Chapter 12, <em>Using Git for Configuration Management</em></a>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Use the playbook below to create a deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the Nova reconfigure playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts nova-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect2" id="config_fc"><div class="titlepage"><div><div><h3 class="title" id="config_fc"><span class="number">18.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure 3PAR FC as a Cinder Backend</span> <a title="Permalink" class="permalink" href="#config_fc">#</a></h3></div></div></div><p>
   You must modify the <code class="literal">cinder.conf.j2</code> to configure the FC
   details.
  </p><p>
   Perform the following steps to configure 3PAR FC as Cinder backend:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Make the following changes to the
     <code class="literal">~/openstack/my_cloud/config/cinder/cinder.conf.j2</code> file:
    </p><ol type="a" class="substeps "><li class="step "><p>
       Add your 3PAR backend to the <code class="literal">enabled_backends</code>
       section:
      </p><div class="verbatim-wrap"><pre class="screen"># Configure the enabled backends
enabled_backends=3par_FC</pre></div><div id="idm139651559941200" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
        If you are using multiple backend types, you can use a comma-delimited
        list here.
       </p></div></li><li class="step "><p>
       <code class="literal">[OPTIONAL]</code> If you want your volumes to use a default
       volume type, then enter the name of the volume type in the
       <code class="literal">[DEFAULT]</code> section with the syntax below.
       <span class="bold"><strong>Remember this value for when you
       create your volume type in the next section.</strong></span>
      </p><div id="idm139651559937520" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
        If you do not specify a default type then your volumes will default to
        a non-redundant RAID configuration. It is recommended that you create a
        volume type and specify it here that meets your environments needs.
       </p></div><div class="verbatim-wrap"><pre class="screen">[DEFAULT]
# Set the default volume type
default_volume_type = &lt;your new volume type&gt;</pre></div></li><li class="step "><p>
       Uncomment the <code class="literal">StoreServ (3par) iscsi cluster</code> section
       and fill the values per your cluster information. Here is an example:
      </p><div class="verbatim-wrap"><pre class="screen">[3par_FC]
san_ip: &lt;3par-san-ipaddr&gt;
san_login: &lt;3par-san-username&gt;
san_password: &lt;3par-san-password&gt;
hp3par_username: &lt;3par-username&gt;
hp3par_password: &lt;hp3par_password&gt;
hp3par_api_url: https://&lt;3par-san-ipaddr&gt;:8080/api/v1
hp3par_cpg: &lt;3par-cpg-name-1&gt;[,&lt;3par-cpg-name-2&gt;, ...]
volume_backend_name: &lt;3par-backend-name&gt;
volume_driver: cinder.volume.drivers.san.hp.hp_3par_fc.HP3PARFCDriver</pre></div></li></ol><div id="idm139651559933312" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      Do not use <code class="literal">backend_host</code> variable in
      <code class="literal">cinder.conf</code> file. If <code class="literal">backend_host</code>
      is set, it will override the [DEFAULT]/host value which <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>
      is dependent on.
     </p></div></li><li class="step "><p>
     Commit your configuration to the local git repo
     (<a class="xref" href="#using_git" title="Chapter 12. Using Git for Configuration Management">Chapter 12, <em>Using Git for Configuration Management</em></a>), as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the following playbook to complete the configuration:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect2" id="config_iscsi"><div class="titlepage"><div><div><h3 class="title" id="config_iscsi"><span class="number">18.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure 3PAR iSCSI as Cinder backend</span> <a title="Permalink" class="permalink" href="#config_iscsi">#</a></h3></div></div></div><p>
   You must modify the <code class="literal">cinder.conf.j2</code> to configure the iSCSI
   details.
  </p><p>
   Perform the following steps to configure 3PAR iSCSI as Cinder backend:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Make the following changes to the
     <code class="literal">~/openstack/my_cloud/config/cinder/cinder.conf.j2</code> file:
    </p><ol type="a" class="substeps "><li class="step "><p>
       Add your 3PAR backend to the <code class="literal">enabled_backends</code>
       section:
      </p><div class="verbatim-wrap"><pre class="screen"># Configure the enabled backends
enabled_backends=3par_iSCSI</pre></div></li><li class="step "><p>
       Uncomment the <code class="literal">StoreServ (3par) iscsi cluster</code> section
       and fill the values per your cluster information. Here is an example:
      </p><div class="verbatim-wrap"><pre class="screen">[3par_iSCSI]
san_ip: &lt;3par-san-ipaddr&gt;
san_login: &lt;3par-san-username&gt;
san_password: &lt;3par-san-password&gt;
hp3par_username: &lt;3par-username&gt;
hp3par_password: &lt;hp3par_password&gt;
hp3par_api_url: https://&lt;3par-san-ipaddr&gt;:8080/api/v1
hp3par_cpg: &lt;3par-cpg-name-1&gt;[,&lt;3par-cpg-name-2&gt;, ...]
volume_backend_name: &lt;3par-backend-name&gt;
volume_driver: cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver
hp3par_iscsi_ips: &lt;3par-ip-address-1&gt;[,&lt;3par-ip-address-2&gt;,&lt;3par-ip-address-3&gt;, ...]
hp3par_iscsi_chap_enabled=true</pre></div><div id="idm139651559912000" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
        Do not use <code class="literal">backend_host</code> variable in
        <code class="literal">cinder.conf</code> file. If <code class="literal">backend_host</code>
        is set, it will override the [DEFAULT]/host value which <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>
        is dependent on.
       </p></div></li></ol></li><li class="step "><p>
     Commit your configuration your local git repository:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "&lt;commit message&gt;"</pre></div></li><li class="step "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
     When you run the configuration processor you will be prompted for two
     passwords. Enter the first password to make the configuration processor
     encrypt its sensitive data, which consists of the random inter-service
     passwords that it generates and the Ansible group_vars and host_vars that
     it produces for subsequent deploy runs. You will need this key for
     subsequent Ansible deploy runs and subsequent configuration processor
     runs. If you wish to change an encryption password that you have already
     used when running the configuration processor then enter the new password
     at the second prompt, otherwise press <span class="keycap">Enter</span>.
    </p><p>
     For CI purposes you can specify the required passwords on the ansible
     command line. For example, the command below will disable encryption by
     the configuration processor
    </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/localhost config-processor-run.yml \
  -e encrypt="" -e rekey=""</pre></div><p>
     If you receive an error during either of these steps then there is an
     issue with one or more of your configuration files. We recommend that you
     verify that all of the information in each of your configuration files is
     correct for your environment and then commit those changes to git using
     the instructions above.
    </p></li><li class="step "><p>
     Run the following command to create a deployment directory.
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Run the following command to complete the configuration:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</pre></div></li></ol></div></div></div><div class="sect2" id="idg-installation-installation-configure_3par-xml-16"><div class="titlepage"><div><div><h3 class="title" id="idg-installation-installation-configure_3par-xml-16"><span class="number">18.1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Post-Installation Tasks</span> <a title="Permalink" class="permalink" href="#idg-installation-installation-configure_3par-xml-16">#</a></h3></div></div></div><p>
   After configuring 3PAR as your Block Storage backend, perform the
   following tasks:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="intraxref">Book “Operations Guide”, Chapter 7 “Managing Block Storage”, Section 7.1 “Managing Block Storage using Cinder”, Section 7.1.2 “Creating a Volume Type for your Volumes”</span>
    </p></li><li class="listitem "><p>
     <a class="xref" href="#sec.verify_block_storage.volume" title="23.1. Verifying Your Block Storage Backend">Section 23.1, “Verifying Your Block Storage Backend”</a>
    </p></li></ul></div></div></div><div class="sect1" id="ironic_oneview_integration"><div class="titlepage"><div><div><h2 class="title" id="ironic_oneview_integration"><span class="number">18.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ironic HPE OneView Integration</span> <a title="Permalink" class="permalink" href="#ironic_oneview_integration">#</a></h2></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> supports integration of Ironic (Baremetal) service with
  HPE OneView using <span class="emphasis"><em>agent_pxe_oneview</em></span> driver. Please refer to
  <a class="link" href="https://docs.openstack.org/developer/ironic/drivers/oneview.html" target="_blank">OpenStack
  Documentation</a> for more information.
 </p><div class="sect2" id="idm139651559884944"><div class="titlepage"><div><div><h3 class="title" id="idm139651559884944"><span class="number">18.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#idm139651559884944">#</a></h3></div></div></div><div class="orderedlist " id="prereq_list"><ol class="orderedlist" type="1"><li class="listitem "><p>
     Installed <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> with entry-scale-ironic-flat-network or
     entry-scale-ironic-multi-tenancy model.
    </p></li><li class="listitem "><p>
     HPE OneView 3.0 instance is running and connected to management network.
    </p></li><li class="listitem "><p>
     HPE OneView configuration is set into
     <code class="literal">definition/data/ironic/ironic_config.yml</code> (and
     <code class="literal">ironic-reconfigure.yml</code> playbook ran if needed). This
     should enable <span class="emphasis"><em>agent_pxe_oneview</em></span> driver in ironic
     conductor.
    </p></li><li class="listitem "><p>
     Managed node(s) should support PXE booting in legacy BIOS mode.
    </p></li><li class="listitem "><p>
     Managed node(s) should have PXE boot NIC listed first. That is, embedded
     1Gb NIC must be disabled (otherwise it always goes first).
    </p></li></ol></div></div><div class="sect2" id="idm139651559874480"><div class="titlepage"><div><div><h3 class="title" id="idm139651559874480"><span class="number">18.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Integrating with HPE OneView</span> <a title="Permalink" class="permalink" href="#idm139651559874480">#</a></h3></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     On the Cloud Lifecycle Manager, open the file
     <code class="literal">~/openstack/my_cloud/definition/data/ironic/ironic_config.yml</code>
    </p><div class="verbatim-wrap"><pre class="screen">~$ cd ~/openstack
vi my_cloud/definition/data/ironic/ironic_config.yml</pre></div></li><li class="step "><p>
     Modify the settings listed below:
    </p><ol type="a" class="substeps "><li class="step "><p>
       <code class="literal">enable_oneview</code>: should be set to "true" for HPE OneView
       integration
      </p></li><li class="step "><p>
       <code class="literal">oneview_manager_url</code>: HTTPS endpoint of HPE OneView
       management interface, for example:
       <span class="bold"><strong>https://10.0.0.10/</strong></span>
      </p></li><li class="step "><p>
       <code class="literal">oneview_username</code>: HPE OneView username, for example:
       <span class="bold"><strong>Administrator</strong></span>
      </p></li><li class="step "><p>
       <code class="literal">oneview_encrypted_password</code>: HPE OneView password in
       encrypted or clear text form. The encrypted form is distinguished by
       presence of <code class="literal">@ardana@</code> at the beginning of the
       string. The encrypted form can be created by running the
       <code class="command">ardanaencrypt.py</code>
       program. This program is shipped as part of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> and can be found in
       <code class="filename">~/openstack/ardana/ansible</code> directory on Cloud Lifecycle Manager.
      </p></li><li class="step "><p>
       <code class="literal">oneview_allow_insecure_connections</code>: should be set to
       "true" if HPE OneView is using self-generated certificate.
      </p></li></ol></li><li class="step "><p>
     Once you have saved your changes and exited the editor, add files, commit
     changes to local git repository, and run
     <code class="literal">config-processor-run.yml</code> and
     <code class="literal">ready-deployment.yml</code> playbooks, as described in
     <a class="xref" href="#using_git" title="Chapter 12. Using Git for Configuration Management">Chapter 12, <em>Using Git for Configuration Management</em></a>.
    </p><div class="verbatim-wrap"><pre class="screen">~/openstack$ git add my_cloud/definition/data/ironic/ironic_config.yml
~/openstack$ cd ardana/ansible
~/openstack/ardana/ansible$ ansible-playbook -i hosts/localhost \
  config-processor-run.yml
...
~/openstack/ardana/ansible$ ansible-playbook -i hosts/localhost \
  ready-deployment.yml</pre></div></li><li class="step "><p>
     Run ironic-reconfigure.yml playbook.
    </p><div class="verbatim-wrap"><pre class="screen">$ cd ~/scratch/ansible/next/ardana/ansible/

# This is needed if password was encrypted in ironic_config.yml file
~/scratch/ansible/next/ardana/ansible$ export ARDANA_USER_PASSWORD_ENCRYPT_KEY=your_password_encrypt_key
~/scratch/ansible/next/ardana/ansible$ ansible-playbook -i hosts/verb_hosts ironic-reconfigure.yml
...</pre></div></li></ol></div></div></div><div class="sect2" id="idm139651559852976"><div class="titlepage"><div><div><h3 class="title" id="idm139651559852976"><span class="number">18.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Registering Node in HPE OneView</span> <a title="Permalink" class="permalink" href="#idm139651559852976">#</a></h3></div></div></div><p>
   In the HPE OneView web interface:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Navigate to
     <span class="guimenu">Menu</span> › <span class="guimenu">Server Hardware</span>.
     Add new <span class="guimenu">Server Hardware</span> item, using
     managed node IPMI IP and credentials. If this is the first node of this
     type being added, corresponding
     <span class="guimenu">Server Hardware Type</span> will be created automatically.
    </p></li><li class="step "><p>
     Navigate to
     <span class="guimenu">Menu</span> › <span class="guimenu">Server Profile Template</span>.
     Add <span class="guimenu">Server Profile Template</span>. Use
     <span class="guimenu">Server Hardware Type</span> corresponding to node being
     registered. In <span class="guimenu">BIOS Settings</span> section, set
     <span class="guimenu">Manage Boot Mode</span> and <span class="guimenu">Manage Boot
     Order</span> options must be turned on:
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-ironic-OneViewWebRegister.png"><img src="images/media-ironic-OneViewWebRegister.png" width="" /></a></div></div></li><li class="step "><p>
     Verify that node is powered off. Power the node off if needed.
    </p></li></ol></div></div></div><div class="sect2" id="idm139651559839312"><div class="titlepage"><div><div><h3 class="title" id="idm139651559839312"><span class="number">18.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Provisioning Ironic Node</span> <a title="Permalink" class="permalink" href="#idm139651559839312">#</a></h3></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Login to the Cloud Lifecycle Manager and source respective credentials file
     (for example <code class="filename">service.osrc</code> for admin account).
    </p></li><li class="step "><p>
     Review glance images with <code class="literal">glance image-list</code>
    </p><div class="verbatim-wrap"><pre class="screen">$ glance image-list
+--------------------------------------+--------------------------+
| ID                                   | Name                     |
+--------------------------------------+--------------------------+
| c61da588-622c-4285-878f-7b86d87772da | cirros-0.3.4-x86_64      |
+--------------------------------------+--------------------------+</pre></div><p>
     Ironic deploy images (boot image,
     <code class="literal">ir-deploy-kernel</code>, <code class="literal">ir-deploy-ramdisk</code>,
     <code class="literal">ir-deploy-iso</code>) are created automatically. The
     <code class="systemitem">agent_pxe_oneview</code> Ironic driver requires
     <code class="systemitem">ir-deploy-kernel</code> and
     <code class="systemitem">ir-deploy-ramdisk</code> images.
    </p></li><li class="step "><p>
     Create node using <code class="literal">agent_pxe_oneview</code> driver.
    </p><div class="verbatim-wrap"><pre class="screen">$ ironic --ironic-api-version 1.22 node-create -d agent_pxe_oneview --name test-node-1 \
  --network-interface neutron -p memory_mb=131072 -p cpu_arch=x86_64 -p local_gb=80 -p cpus=2 \
  -p 'capabilities=boot_mode:bios,boot_option:local,server_hardware_type_uri:\
     /rest/server-hardware-types/E5366BF8-7CBF-48DF-A752-8670CF780BB2,server_profile_template_uri:\
     /rest/server-profile-templates/00614918-77f8-4146-a8b8-9fc276cd6ab2' \
  -i 'server_hardware_uri=/rest/server-hardware/32353537-3835-584D-5135-313930373046' \
  -i dynamic_allocation=True \
  -i deploy_kernel=633d379d-e076-47e6-b56d-582b5b977683 \
  -i deploy_ramdisk=d5828785-edf2-49fa-8de2-3ddb7f3270d5

+-------------------+--------------------------------------------------------------------------+
| Property          | Value                                                                    |
+-------------------+--------------------------------------------------------------------------+
| chassis_uuid      |                                                                          |
| driver            | agent_pxe_oneview                                                        |
| driver_info       | {u'server_hardware_uri': u'/rest/server-                                 |
|                   | hardware/32353537-3835-584D-5135-313930373046', u'dynamic_allocation':   |
|                   | u'True', u'deploy_ramdisk': u'd5828785-edf2-49fa-8de2-3ddb7f3270d5',     |
|                   | u'deploy_kernel': u'633d379d-e076-47e6-b56d-582b5b977683'}               |
| extra             | {}                                                                       |
| name              | test-node-1                                                              |
| network_interface | neutron                                                                  |
| properties        | {u'memory_mb': 131072, u'cpu_arch': u'x86_64', u'local_gb': 80, u'cpus': |
|                   | 2, u'capabilities':                                                      |
|                   | u'boot_mode:bios,boot_option:local,server_hardware_type_uri:/rest        |
|                   | /server-hardware-types/E5366BF8-7CBF-                                    |
|                   | 48DF-A752-8670CF780BB2,server_profile_template_uri:/rest/server-profile- |
|                   | templates/00614918-77f8-4146-a8b8-9fc276cd6ab2'}                         |
| resource_class    | None                                                                     |
| uuid              | c202309c-97e2-4c90-8ae3-d4c95afdaf06                                     |
+-------------------+--------------------------------------------------------------------------+</pre></div><div id="idm139651559826816" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
        For deployments created via Ironic/HPE OneView integration,
        <code class="literal">memory_mb</code> property must reflect physical amount of
        RAM installed in the managed node. That is, for a server with 128 Gb of RAM
        it works out to 132*1024=13072.
       </p></li><li class="listitem "><p>
        Boot mode in capabilities property must reflect boot mode used by the
        server, that is 'bios' for Legacy BIOS and 'uefi' for UEFI.
       </p></li><li class="listitem "><p>
        Values for <code class="literal">server_hardware_type_uri</code>,
        <code class="literal">server_profile_template_uri</code> and
        <code class="literal">server_hardware_uri</code> can be grabbed from browser URL
        field while navigating to respective objects in HPE OneView UI. URI
        corresponds to the part of URL which starts form the token
        <code class="literal">/rest</code>.
        That is, the URL
        <code class="literal">https://oneview.mycorp.net/#/profile-templates/show/overview/r/rest/server-profile-templates/12345678-90ab-cdef-0123-012345678901</code>
        corresponds to the URI
        <code class="literal">/rest/server-profile-templates/12345678-90ab-cdef-0123-012345678901</code>.
       </p></li><li class="listitem "><p>
        Grab IDs of <code class="literal">deploy_kernel</code> and
        <code class="literal">deploy_ramdisk</code> from <span class="bold"><strong>glance
        image-list</strong></span> output above.
       </p></li></ul></div></div></li><li class="step "><p>
     Create port.
    </p><div class="verbatim-wrap"><pre class="screen">$ ironic --ironic-api-version 1.22 port-create \
  --address aa:bb:cc:dd:ee:ff \
  --node c202309c-97e2-4c90-8ae3-d4c95afdaf06 \
  -l switch_id=ff:ee:dd:cc:bb:aa \
  -l switch_info=MY_SWITCH \
  -l port_id="Ten-GigabitEthernet 1/0/1" \
  --pxe-enabled true
+-----------------------+----------------------------------------------------------------+
| Property              | Value                                                          |
+-----------------------+----------------------------------------------------------------+
| address               | 8c:dc:d4:b5:7d:1c                                              |
| extra                 | {}                                                             |
| local_link_connection | {u'switch_info': u'C20DATA', u'port_id': u'Ten-GigabitEthernet |
|                       | 1/0/1',    u'switch_id': u'ff:ee:dd:cc:bb:aa'}                 |
| node_uuid             | c202309c-97e2-4c90-8ae3-d4c95afdaf06                           |
| pxe_enabled           | True                                                           |
| uuid                  | 75b150ef-8220-4e97-ac62-d15548dc8ebe                           |
+-----------------------+----------------------------------------------------------------+</pre></div><div id="idm139651559813872" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
      Ironic Multi-Tenancy networking model is used in this example.
      Therefore, ironic port-create command contains information about the
      physical switch. HPE OneView integration can also be performed using the
      Ironic Flat Networking model. For more information, see
      <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 10 “Example Configurations”, Section 10.6 “Ironic Examples”</span>.
     </p></div></li><li class="step "><p>
     Move node to manageable provisioning state. The connectivity between
     Ironic and HPE OneView will be verified, Server Hardware Template settings
     validated, and Server Hardware power status retrieved from HPE OneView and set
     into the Ironic node.
    </p><div class="verbatim-wrap"><pre class="screen">$ ironic node-set-provision-state test-node-1 manage</pre></div></li><li class="step "><p>
     Verify that node power status is populated.
    </p><div class="verbatim-wrap"><pre class="screen">$ ironic node-show test-node-1
+-----------------------+-----------------------------------------------------------------------+
| Property              | Value                                                                 |
+-----------------------+-----------------------------------------------------------------------+
| chassis_uuid          |                                                                       |
| clean_step            | {}                                                                    |
| console_enabled       | False                                                                 |
| created_at            | 2017-06-30T21:00:26+00:00                                             |
| driver                | agent_pxe_oneview                                                     |
| driver_info           | {u'server_hardware_uri': u'/rest/server-                              |
|                       | hardware/32353537-3835-584D-5135-313930373046', u'dynamic_allocation':|
|                       | u'True', u'deploy_ramdisk': u'd5828785-edf2-49fa-8de2-3ddb7f3270d5',  |
|                       | u'deploy_kernel': u'633d379d-e076-47e6-b56d-582b5b977683'}            |
| driver_internal_info  | {}                                                                    |
| extra                 | {}                                                                    |
| inspection_finished_at| None                                                                  |
| inspection_started_at | None                                                                  |
| instance_info         | {}                                                                    |
| instance_uuid         | None                                                                  |
| last_error            | None                                                                  |
| maintenance           | False                                                                 |
| maintenance_reason    | None                                                                  |
| name                  | test-node-1                                                           |
| network_interface     |                                                                       |
| power_state           | power off                                                             |
| properties            | {u'memory_mb': 131072, u'cpu_arch': u'x86_64', u'local_gb': 80,       |
|                       | u'cpus': 2, u'capabilities':                                          |
|                       | u'boot_mode:bios,boot_option:local,server_hardware_type_uri:/rest     |
|                       | /server-hardware-types/E5366BF8-7CBF-                                 |
|                       | 48DF-A752-86...BB2,server_profile_template_uri:/rest/server-profile-  |
|                       | templates/00614918-77f8-4146-a8b8-9fc276cd6ab2'}                      |
| provision_state       | manageable                                                            |
| provision_updated_at  | 2017-06-30T21:04:43+00:00                                             |
| raid_config           |                                                                       |
| reservation           | None                                                                  |
| resource_class        |                                                                       |
| target_power_state    | None                                                                  |
| target_provision_state| None                                                                  |
| target_raid_config    |                                                                       |
| updated_at            | 2017-06-30T21:04:43+00:00                                             |
| uuid                  | c202309c-97e2-4c90-8ae3-d4c95afdaf06                                  |
+-----------------------+-----------------------------------------------------------------------+</pre></div></li><li class="step "><p>
     Move node to available provisioning state. The Ironic node will be
     reported to Nova as available.
    </p><div class="verbatim-wrap"><pre class="screen">$ ironic node-set-provision-state test-node-1 provide</pre></div></li><li class="step "><p>
     Verify that node resources were added to Nova hypervisor stats.
    </p><div class="verbatim-wrap"><pre class="screen">$ nova hypervisor-stats
+----------------------+--------+
| Property             | Value  |
+----------------------+--------+
| count                | 1      |
| current_workload     | 0      |
| disk_available_least | 80     |
| free_disk_gb         | 80     |
| free_ram_mb          | 131072 |
| local_gb             | 80     |
| local_gb_used        | 0      |
| memory_mb            | 131072 |
| memory_mb_used       | 0      |
| running_vms          | 0      |
| vcpus                | 2      |
| vcpus_used           | 0      |
+----------------------+--------+</pre></div></li><li class="step "><p>
     Create Nova flavor.
    </p><div class="verbatim-wrap"><pre class="screen">$ nova flavor-create m1.ironic auto 131072 80 2
+-------------+-----------+--------+------+-----------+------+-------+-------------+-----------+
| ID          | Name      | Mem_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public |
+-------------+-----------+--------+------+-----------+------+-------+-------------+-----------+
| 33c8...f8d8 | m1.ironic | 131072 | 80   | 0         |      | 2     | 1.0         | True      |
+-------------+-----------+--------+------+-----------+------+-------+-------------+-----------+
$ nova flavor-key m1.ironic set capabilities:boot_mode="bios"
$ nova flavor-key m1.ironic set capabilities:boot_option="local"
$ nova flavor-key m1.ironic set cpu_arch=x86_64</pre></div><div id="idm139651559798992" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
      All parameters (specifically, amount of RAM and boot mode) must
      correspond to ironic node parameters.
     </p></div></li><li class="step "><p>
     Create Nova keypair if needed.
    </p><div class="verbatim-wrap"><pre class="screen">$ nova keypair-add ironic_kp --pub-key ~/.ssh/id_rsa.pub</pre></div></li><li class="step "><p>
     Boot Nova instance.
    </p><div class="verbatim-wrap"><pre class="screen">$ nova boot --flavor m1.ironic --image d6b5...e942 --key-name ironic_kp \
  --nic net-id=5f36...dcf3 test-node-1
+-------------------------------+-----------------------------------------------------+
| Property                      | Value                                               |
+-------------------------------+-----------------------------------------------------+
| OS-DCF:diskConfig             | MANUAL                                              |
| OS-EXT-AZ:availability_zone   |                                                     |
| OS-EXT-SRV-ATTR:host          | -                                                   |
| OS-EXT-SRV-ATTR:              |                                                     |
|       hypervisor_hostname     | -                                                   |
| OS-EXT-SRV-ATTR:instance_name |                                                     |
| OS-EXT-STS:power_state        | 0                                                   |
| OS-EXT-STS:task_state         | scheduling                                          |
| OS-EXT-STS:vm_state           | building                                            |
| OS-SRV-USG:launched_at        | -                                                   |
| OS-SRV-USG:terminated_at      | -                                                   |
| accessIPv4                    |                                                     |
| accessIPv6                    |                                                     |
| adminPass                     | pE3m7wRACvYy                                        |
| config_drive                  |                                                     |
| created                       | 2017-06-30T21:08:42Z                                |
| flavor                        | m1.ironic (33c81884-b8aa-46...3b72f8d8)             |
| hostId                        |                                                     |
| id                            | b47c9f2a-e88e-411a-abcd-6172aea45397                |
| image                         | Ubuntu Trusty 14.04 BIOS (d6b5d971-42...5f2d88e942) |
| key_name                      | ironic_kp                                           |
| metadata                      | {}                                                  |
| name                          | test-node-1                                         |
| os-extended-volumes:          |                                                     |
|       volumes_attached        | []                                                  |
| progress                      | 0                                                   |
| security_groups               | default                                             |
| status                        | BUILD                                               |
| tenant_id                     | c8573f7026d24093b40c769ca238fddc                    |
| updated                       | 2017-06-30T21:08:42Z                                |
| user_id                       | 2eae99221545466d8f175eeb566cc1b4                    |
+-------------------------------+-----------------------------------------------------+</pre></div><p>
     During nova instance boot, the following operations will be performed by
     Ironic via HPE OneView REST API.
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       In HPE OneView, new Server Profile is generated for specified Server
       Hardware, using specified Server Profile Template. Boot order in Server
       Profile is set to list PXE as the first boot source.
      </p></li><li class="listitem "><p>
       The managed node is powered on and boots IPA image from PXE.
      </p></li><li class="listitem "><p>
       IPA image writes user image onto disk and reports success back to
       Ironic.
      </p></li><li class="listitem "><p>
       Ironic modifies Server Profile in HPE OneView to list 'Disk' as default boot
       option.
      </p></li><li class="listitem "><p>
       Ironic reboots the node (via HPE OneView REST API call).
      </p></li></ul></div></li></ol></div></div></div></div><div class="sect1" id="ses.integration"><div class="titlepage"><div><div><h2 class="title" id="ses.integration"><span class="number">18.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SUSE Enterprise Storage Integration</span> <a title="Permalink" class="permalink" href="#ses.integration">#</a></h2></div></div></div><p>
  The current version of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> supports integration with
  SUSE Enterprise Storage. Integrating SUSE Enterprise Storage enables Ceph block storage as well as object
  and image storage services in <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
 </p><div class="sect2" id="ses.installation"><div class="titlepage"><div><div><h3 class="title" id="ses.installation"><span class="number">18.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enabling SUSE Enterprise Storage Integration</span> <a title="Permalink" class="permalink" href="#ses.installation">#</a></h3></div></div></div><p>
   The SUSE Enterprise Storage integration is provided through the
   <span class="package">ardana-ses</span> RPM package. As this package is
   included in the <code class="systemitem">patterns-cloud-ardana</code> pattern, its
   installation is covered in <a class="xref" href="#cha.depl.dep_inst" title="Chapter 3. Installing the Cloud Lifecycle Manager server">Chapter 3, <em>Installing the Cloud Lifecycle Manager server</em></a>.
  </p></div><div class="sect2" id="ses.config"><div class="titlepage"><div><div><h3 class="title" id="ses.config"><span class="number">18.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuration</span> <a title="Permalink" class="permalink" href="#ses.config">#</a></h3></div></div></div><p>
   After the SUSE Enterprise Storage integration package has been installed, it must be
   configured in order to work. Files that contain relevant SUSE Enterprise Storage/Ceph
   deployment information must be placed into a directory on the deployer
   node. This includes the configuration file that describes various aspects of
   the Ceph environment as well as keyrings for each user and pool created in
   the Ceph environment. In addition to that, you need to edit the
   <code class="filename">settings.yml</code> file to enable the SUSE Enterprise Storage integration to
   run and update all of the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> service configuration files.
  </p><p>
   The <code class="filename">settings.yml</code> file must reside in the
  <code class="filename">~/openstack/my_config/confg/ses/</code> directory. Open the
  file for editing, uncomment the <code class="literal">ses_config_path:</code>
  parameter, and specify the location on the deployer host containing the
  <code class="filename">ses_config.yml</code> and keyring files as the parameter's
  value. After you have done that, the <code class="filename">site.yml</code> and
  <code class="filename">ardana-reconfigure.yml</code> playbooks activates the SUSE Enterprise Storage
  integration and configures the Cinder, Glance, and Nova services.
  </p><p>
    For Ceph, it is necessary to create pools and users to allow the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
    services to use the SUSE Enterprise Storage/Ceph cluster. Pools and users must be created
    for Cinder, Cinder backup, Nova and Glance. After the
    required pools and users are set up on the SUSE Enterprise Storage/Ceph cluster, you have to
    create a <code class="filename">ses_config.yml</code> configuration file (see the
    example below). This file
    is used during deployment to configure all of the services. The
    <code class="filename">ses_config.yml</code> and the keyring files should be placed
    in a separate directory. The path to this directory must be specified in the
    <code class="filename">settings.yml</code> file. 
   </p><div class="example" id="idm139651559764048"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 18.1: </span><span class="name">ses_config.yml Example </span><a title="Permalink" class="permalink" href="#idm139651559764048">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">ses_cluster_configuration:
    ses_cluster_name: ceph
    ses_radosgw_url: "https://192.168.56.8:8080/swift/v1"

    conf_options:
        ses_fsid: d5d7c7cb-5858-3218-a36f-d028df7b1111
        ses_mon_initial_members: ses-osd2, ses-osd3, ses-osd1
        ses_mon_host: 192.168.56.8, 192.168.56.9, 192.168.56.7
        ses_public_network: 192.168.56.0/21
        ses_cluster_network: 192.168.56.0/21

    cinder:
        rbd_store_pool: cinder
        rbd_store_pool_user: cinder
        keyring_file_name: ceph.client.cinder.keyring

    cinder-backup:
        rbd_store_pool: backups
        rbd_store_pool_user: cinder_backup
        keyring_file_name: ceph.client.cinder-backup.keyring

    # Nova uses the cinder user to access the nova pool, cinder pool
    # So all we need here is the nova pool name.
    nova:
        rbd_store_pool: nova

    glance:
        rbd_store_pool: glance
        rbd_store_pool_user: glance
        keyring_file_name: ceph.client.glance.keyring</pre></div></div></div></div></div></div><div class="chapter " id="troubleshooting_installation"><div class="titlepage"><div><div><h2 class="title"><span class="number">19 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting the Installation</span> <a title="Permalink" class="permalink" href="#troubleshooting_installation">#</a></h2></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#sec.trouble-deployer_setup"><span class="number">19.1 </span><span class="name">Issues during Cloud Lifecycle Manager Setup</span></a></span></dt><dt><span class="section"><a href="#sec.trouble-config_processor"><span class="number">19.2 </span><span class="name">Issues while Updating Configuration Files</span></a></span></dt><dt><span class="section"><a href="#sec.trouble-deploy_cloud"><span class="number">19.3 </span><span class="name">Issues while Deploying the Cloud</span></a></span></dt></dl></div></div><p>
  We have gathered some of the common issues that occur during installation and
  organized them by when they occur during the installation. These sections
  will coincide with the steps labeled in the installation instructions.
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <a class="xref" href="#sec.trouble-deployer_setup" title="19.1. Issues during Cloud Lifecycle Manager Setup">Section 19.1, “Issues during Cloud Lifecycle Manager Setup”</a>
   </p></li><li class="listitem "><p>
     <a class="xref" href="#sec.trouble-config_processor" title="19.2. Issues while Updating Configuration Files">Section 19.2, “Issues while Updating Configuration Files”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec.trouble-deploy_cloud" title="19.3. Issues while Deploying the Cloud">Section 19.3, “Issues while Deploying the Cloud”</a>
   </p></li></ul></div><div class="sect1" id="sec.trouble-deployer_setup"><div class="titlepage"><div><div><h2 class="title" id="sec.trouble-deployer_setup"><span class="number">19.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Issues during Cloud Lifecycle Manager Setup</span> <a title="Permalink" class="permalink" href="#sec.trouble-deployer_setup">#</a></h2></div></div></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="idm139651559751872"><span class="name">Issue: Running the ardana-init.bash script when configuring your Cloud Lifecycle Manager does not complete</span><a title="Permalink" class="permalink" href="#idm139651559751872">#</a></h5></div><p>
   Part of what the <code class="literal">ardana-init.bash</code> script does is
   install Git. So if your DNS server(s) is/are not specified in your
   <code class="filename">/etc/resolv.conf</code> file, is not valid, or is not
   functioning properly on your Cloud Lifecycle Manager, it will not be able to
   complete.
  </p><p>
   To resolve this issue, double check your nameserver in your
   <code class="filename">/etc/resolv.conf</code> file and then re-run the script.
  </p></div><div class="sect1" id="sec.trouble-config_processor"><div class="titlepage"><div><div><h2 class="title" id="sec.trouble-config_processor"><span class="number">19.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Issues while Updating Configuration Files</span> <a title="Permalink" class="permalink" href="#sec.trouble-config_processor">#</a></h2></div></div></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="idm139651559747168"><span class="name">Configuration Processor Fails Due to Wrong yml Format</span><a title="Permalink" class="permalink" href="#idm139651559747168">#</a></h5></div><p>
   If you receive the error below when running the configuration processor then
   you may have a formatting error:
  </p><div class="verbatim-wrap"><pre class="screen">TASK: [fail msg="Configuration processor run failed, see log output above for
details"]</pre></div><p>
   First you should check the Ansible log in the location below for more
   details on which yml file in your input model has the error:
  </p><div class="verbatim-wrap"><pre class="screen">~/.ansible/ansible.log</pre></div><p>
   Check the configuration file to locate and fix the error, keeping in mind
   the following tips below.
  </p><p>
   Check your files to ensure that they don't contain the following:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     Non-ascii characters
    </p></li><li class="listitem "><p>
     Unneeded spaces
    </p></li></ul></div><p>
   Once you have fixed the formatting error in your files, commit the changes
   with these steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Commit your changes to Git:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "My config or other commit message"</pre></div></li><li class="step "><p>
     Re-run the configuration processor playbook and confirm the error is not
     received again.
    </p></li></ol></div></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="idm139651559737632"><span class="name">Configuration processor fails with provider network OCTAVIA-MGMT-NET error</span><a title="Permalink" class="permalink" href="#idm139651559737632">#</a></h5></div><p>
   If you receive the error below when running the configuration processor then
   you have not correctly configured your VLAN settings for Octavia.
  </p><div class="verbatim-wrap"><pre class="screen">"################################################################################",
"# The configuration processor failed.  ",
"#   config-data-2.0           ERR: Provider network OCTAVIA-MGMT-NET host_routes:
"# destination '192.168.10.0/24' is not defined as a Network in the input model.
""# Add 'external: True' to this host_route if this is for an external network.",
"################################################################################"</pre></div><p>
   To resolve the issue, ensure that your settings in
   <code class="literal">~/openstack/my_cloud/definition/data/neutron/neutron_config.yml</code>
   are correct for the VLAN setup for Octavia.
  </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="idm139651559734016"><span class="name">Changes Made to your Configuration Files</span><a title="Permalink" class="permalink" href="#idm139651559734016">#</a></h5></div><p>
   If you have made corrections to your configuration files and need to re-run
   the Configuration Processor, the only thing you need to do is commit your
   changes to your local Git repository:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "commit message"</pre></div><p>
   You can then re-run the configuration processor:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="idm139651559731120"><span class="name">Configuration Processor Fails Because Encryption Key Does Not Meet Requirements</span><a title="Permalink" class="permalink" href="#idm139651559731120">#</a></h5></div><p>
   If you choose to set an encryption password when running the configuration
   processor, you may receive the following error if the chosen password does
   not meet the complexity requirements:
  </p><div class="verbatim-wrap"><pre class="screen">################################################################################
# The configuration processor failed.
#   encryption-key ERR: The Encryption Key does not meet the following requirement(s):
#       The Encryption Key must be at least 12 characters
#       The Encryption Key must contain at least 3 of following classes of characters:
#                           Uppercase Letters, Lowercase Letters, Digits, Punctuation
################################################################################</pre></div><p>
   If you receive the above error, simply run the configuration processor again
   and select a password that meets the complexity requirements detailed in the
   error message:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></div><div class="sect1" id="sec.trouble-deploy_cloud"><div class="titlepage"><div><div><h2 class="title" id="sec.trouble-deploy_cloud"><span class="number">19.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Issues while Deploying the Cloud</span> <a title="Permalink" class="permalink" href="#sec.trouble-deploy_cloud">#</a></h2></div></div></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="idm139651559725920"><span class="name">Issue: If the site.yml playbook fails, you can query the log for the reason</span><a title="Permalink" class="permalink" href="#idm139651559725920">#</a></h5></div><p>
   Ansible is good about outputting the errors into the command line output,
   however if you'd like to view the full log for any reason the location is:
  </p><div class="verbatim-wrap"><pre class="screen">~/.ansible/ansible.log</pre></div><p>
   This log is updated real time as you run Ansible playbooks.
  </p><div id="idm139651559723600" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>
    Use grep to parse through the log. Usage: <code class="literal">grep &lt;text&gt;
    ~/.ansible/ansible.log</code>
   </p></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="idm139651559722208"><span class="name">Issue: How to Wipe the Disks of your Machines</span><a title="Permalink" class="permalink" href="#idm139651559722208">#</a></h5></div><p>
   If you have re-run the <code class="literal">site.yml</code> playbook, you may need to
   wipe the disks of your nodes
  </p><p>
   You would generally run the playbook below after re-running the
   <code class="literal">bm-reimage.yml</code> playbook but before you re-run the
   <code class="literal">site.yml</code> playbook.
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts wipe_disks.yml</pre></div><p>
   The playbook will show you the disks to be wiped in the output and allow you
   to confirm that you want to complete this action or abort it if you do not
   want to proceed. You can optionally use the <code class="literal">--limit
   &lt;NODE_NAME&gt;</code> switch on this playbook to restrict it to
   specific nodes.
  </p><p>
   If you receive an error stating that <code class="literal">osconfig</code> has already
   run on your nodes then you will need to remove the
   <code class="literal">/etc/ardana/osconfig-ran</code> file on each of the nodes you want
   to wipe with this command:
  </p><div class="verbatim-wrap"><pre class="screen">sudo rm /etc/ardana/osconfig-ran</pre></div><p>
   That will clear this flag and allow the disk to be wiped.
  </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="idm139651559714976"><span class="name">Issue: Freezer installation fails if an independent network is used for the External_API</span><a title="Permalink" class="permalink" href="#idm139651559714976">#</a></h5></div><p>
   The Freezer installation fails if an independent network is used
   for the External_API. If you intend to deploy the External API
   on an independent network, the following changes need to be made:
  </p><p>
   In <code class="literal">roles/freezer-agent/defaults/main.yml</code> add the
   following line:
  </p><div class="verbatim-wrap"><pre class="screen">backup_freezer_api_url: "{{ FRE_API | item('advertises.vips.private[0].url', default=' ') }}"</pre></div><p>
   In <code class="literal">roles/freezer-agent/templates/backup.osrc.j2</code> add the
   following line:
  </p><div class="verbatim-wrap"><pre class="screen">export OS_FREEZER_URL={{ backup_freezer_api_url }}</pre></div><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="idm139651559710496"><span class="name">Error Received if Root Logical Volume is Too Small</span><a title="Permalink" class="permalink" href="#idm139651559710496">#</a></h5></div><p>
   When running the <code class="literal">site.yml</code> playbook, you may receive a
   message that includes the error below if your root logical volume is too
   small. This error needs to be parsed out and resolved.
  </p><div class="verbatim-wrap"><pre class="screen">2015-09-29 15:54:03,022 p=26345 u=stack | stderr: New size given (7128 extents)
not larger than existing size (7629 extents)</pre></div><p>
   The error message may also reference the root volume:
  </p><div class="verbatim-wrap"><pre class="screen">"name": "root", "size": "10%"</pre></div><p>
   The problem here is that the root logical volume, as specified in the
   <code class="literal">disks_controller.yml</code> file, is set to
   <code class="literal">10%</code> of the overall physical volume and this value is too
   small.
  </p><p>
   To resolve this issue you need to ensure that the percentage is set properly
   for the size of your logical-volume. The default values in the configuration
   files is based on a 500 GB disk, so if your logical volumes are smaller you
   may need to increase the percentage so there is enough room.
  </p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect4 bridgehead"><h5 class="title" id="idm139651559704992"><span class="name">Multiple Keystone Failures Received during site.yml</span><a title="Permalink" class="permalink" href="#idm139651559704992">#</a></h5></div><p>
   If you receive the Keystone error below during your
   <code class="literal">site.yml</code> run then follow these steps:
  </p><div class="verbatim-wrap"><pre class="screen">TASK: [OPS-MON | _keystone_conf | Create Ops Console service in Keystone] *****
failed:
[...]
msg: An unexpected error prevented the server from fulfilling your request.
(HTTP 500) (Request-ID: req-23a09c72-5991-4685-b09f-df242028d742), failed

FATAL: all hosts have already failed -- aborting</pre></div><p>
   The most likely cause of this error is that the virtual IP address is having
   issues and the Keystone API communication through the virtual IP address is
   not working properly. You will want to check the Keystone log on the
   controller where you will likely see authorization failure errors.
  </p><p>
   Verify that your virtual IP address is active and listening on the proper
   port on all of your controllers using this command:
  </p><div class="verbatim-wrap"><pre class="screen">netstat -tplan | grep 35357</pre></div><p>
   Ensure that your Cloud Lifecycle Manager did not pick the wrong (unusable) IP
   address from the list of IP addresses assigned to your Management network.
  </p><p>
   The Cloud Lifecycle Manager will take the first available IP address after the
   <code class="literal">gateway-ip</code> defined in your
   <code class="filename">~/openstack/my_cloud/definition/data/networks.yml</code> file.
   This IP will be used as the virtual IP address for that particular network.
   If this IP address is used and reserved for another purpose outside of your
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> deployment then you will receive the error above.
  </p><p>
   To resolve this issue we recommend that you utilize the
   <code class="literal">start-address</code> and possibly the
   <code class="literal">end-address</code> (if needed) options in your
   <code class="filename">networks.yml</code> file to further define which IP addresses
   you want your cloud deployment to use. For more information, see
   <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 7 “Configuration Objects”, Section 7.14 “Networks”</span>.
  </p><p>
   After you have made changes to your <code class="filename">networks.yml</code> file,
   follow these steps to commit the changes:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Ensuring that you stay within the <code class="filename">~/openstack</code> directory,
     commit the changes you just made:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack
git commit -a -m "commit message"</pre></div></li><li class="step "><p>
     Run the configuration processor:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div></li><li class="step "><p>
     Update your deployment directory:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
     Re-run the site.yml playbook:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts site.yml</pre></div></li></ol></div></div></div></div><div class="chapter " id="esx_troubleshooting_installation"><div class="titlepage"><div><div><h2 class="title"><span class="number">20 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Troubleshooting the ESX</span> <a title="Permalink" class="permalink" href="#esx_troubleshooting_installation">#</a></h2></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#idm139651559680896"><span class="number">20.1 </span><span class="name">Issue: Ardana-ux-services.service is not running</span></a></span></dt><dt><span class="section"><a href="#idm139651559674672"><span class="number">20.2 </span><span class="name">Issue: ESX Cluster shows UNKNOWN in Operations Console</span></a></span></dt><dt><span class="section"><a href="#idm139651559669456"><span class="number">20.3 </span><span class="name">Issue: Unable to view the VM console in Horizon UI</span></a></span></dt></dl></div></div><p>
  This section contains troubleshooting tasks for your <span class="phrase"><span class="phrase">SUSE® <span class="productname">OpenStack</span> Cloud</span></span>
  <span class="phrase"><span class="phrase">8</span></span> for ESX.
 </p><div class="sect1" id="idm139651559680896"><div class="titlepage"><div><div><h2 class="title" id="idm139651559680896"><span class="number">20.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Issue: Ardana-ux-services.service is not running</span> <a title="Permalink" class="permalink" href="#idm139651559680896">#</a></h2></div></div></div><p>
   If you perform any maintenance work or reboot the Cloud Lifecycle Manager/deployer
   node, make sure to restart the Cloud Lifecycle Manager UX service for standalone deployer node
   and shared Cloud Lifecycle Manager/controller node based on your environment.
  </p><p>
   For standalone deployer node, execute <code class="literal">ardana-start.yml</code>
   playbook to restart the Cloud Lifecycle Manager UX services on the deployer node after a reboot.
  </p><p>
   For shared deployer/controller node, execute
   <code class="literal">ardana-start.yml</code> playbook on all the controllers to
   restart Cloud Lifecycle Manager UX services.
  </p><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-start.yml --limit <em class="replaceable ">HOST_NAME</em></pre></div><p>
   Replace <em class="replaceable ">HOST_NAME</em> with the host name of the Cloud Lifecycle Manager
   node or the Cloud Lifecycle Manager Node/Shared Controller.
  </p></div><div class="sect1" id="idm139651559674672"><div class="titlepage"><div><div><h2 class="title" id="idm139651559674672"><span class="number">20.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Issue: ESX Cluster shows UNKNOWN in Operations Console</span> <a title="Permalink" class="permalink" href="#idm139651559674672">#</a></h2></div></div></div><p>
   In the Operations Console Alarms dashboard, if all the alarms for ESX cluster are
   showing UNKNOWN then restart the <code class="literal">openstack-monasca-agent</code> running in
   ESX compute proxy.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     SSH to the respective compute proxy. You can find the hostname of the
     proxy from the dimensions list shown against the respective alarm.
    </p></li><li class="step "><p>
     Restart the <code class="literal">openstack-monasca-agent</code> service.
    </p><div class="verbatim-wrap"><pre class="screen">sudo systemctl restart openstack-monasca-agent</pre></div></li></ol></div></div></div><div class="sect1" id="idm139651559669456"><div class="titlepage"><div><div><h2 class="title" id="idm139651559669456"><span class="number">20.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Issue: Unable to view the VM console in Horizon UI</span> <a title="Permalink" class="permalink" href="#idm139651559669456">#</a></h2></div></div></div><p>
   By default the gdbserver firewall is disabled in ESXi host which results in
   a Handshake error when accessing the VM instance console in the Horizon UI.
  </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-esx-gdbserver.png"><img src="images/media-esx-gdbserver.png" width="" /></a></div></div><p>
   <span class="bold"><strong>Procedure to enable gdbserver</strong></span>
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Login to vSphere Client.
    </p></li><li class="step "><p>
     Select the ESXi Host and click
     <span class="guimenu">Configuration</span> tab in the menu bar. You
     must perform the following actions on all the ESXi hosts in the compute
     clusters.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-esx-1.png"><img src="images/media-esx-1.png" width="" /></a></div></div></li><li class="step "><p>
     On the left hand side select <span class="bold"><strong>Security
     Profile</strong></span> from the list of
     <span class="bold"><strong>Software</strong></span>. Click
     <span class="bold"><strong>Properties</strong></span> on the right hand side.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-esx-2.png"><img src="images/media-esx-2.png" width="" /></a></div></div><p>
     Firewall Properties box displays.
    </p></li><li class="step "><p>
     Select <span class="bold"><strong>gdbserver</strong></span> checkbox and click
     <span class="bold"><strong>OK</strong></span>.
    </p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/media-esx-3.png"><img src="images/media-esx-3.png" width="" /></a></div></div></li></ol></div></div></div></div></div><div class="part" id="post_install"><div class="titlepage"><div><div><h1 class="title"><span class="number">Part III </span><span class="name">Post-Installation </span><a title="Permalink" class="permalink" href="#post_install">#</a></h1></div></div></div><div class="toc"><dl><dt><span class="chapter"><a href="#post_install_overview"><span class="number">21 </span><span class="name">Overview</span></a></span></dt><dd class="toc-abstract"><p>
   Once you have completed your cloud deployment, these are some of the common
   post-installation tasks you may need to perform. Take a look at the
   descriptions below to determine which of these you need to do.
  </p></dd><dt><span class="chapter"><a href="#cloud_verification"><span class="number">22 </span><span class="name">Cloud Verification</span></a></span></dt><dd class="toc-abstract"><p>
  Once you have completed your cloud deployment, these are some of the common
  post-installation tasks you may need to perform to verify your cloud
  installation.
 </p></dd><dt><span class="chapter"><a href="#ui_verification"><span class="number">23 </span><span class="name">UI Verification</span></a></span></dt><dd class="toc-abstract"><p>
  Once you have completed your cloud deployment, these are some of the common
  post-installation tasks you may need to perform to verify your cloud
  installation.
 </p></dd><dt><span class="chapter"><a href="#install_openstack_clients"><span class="number">24 </span><span class="name">Installing OpenStack Clients</span></a></span></dt><dd class="toc-abstract"><p>
  If you have a standalone deployer, the OpenStack CLI and other clients will
	not be installed automatically on that node. If you require access to these
	clients, you will need to follow the procedure below to add the appropriate
	software.
 </p></dd><dt><span class="chapter"><a href="#tls30"><span class="number">25 </span><span class="name">Configuring Transport Layer Security (TLS)</span></a></span></dt><dd class="toc-abstract"><p>
    TLS is enabled by default during the installation of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> and
    additional configuration options are available to secure your environment,
    as described below.
   </p></dd><dt><span class="chapter"><a href="#config_availability_zones"><span class="number">26 </span><span class="name">Configuring Availability Zones</span></a></span></dt><dd class="toc-abstract"><p>
  The Cloud Lifecycle Manager only creates a default availability zone during
  installation. If your system has multiple failure/availability zones defined
  in your input model, these zones will not get created automatically.
 </p></dd><dt><span class="chapter"><a href="#OctaviaInstall"><span class="number">27 </span><span class="name">Configuring Load Balancer as a Service</span></a></span></dt><dd class="toc-abstract"><p>
    The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Neutron LBaaS service supports several load balancing
    providers. By default, both Octavia and the namespace HAProxy driver are
    configured to be used. We describe this in more detail here.
   </p></dd><dt><span class="chapter"><a href="#postinstall_checklist"><span class="number">28 </span><span class="name">Other Common Post-Installation Tasks</span></a></span></dt></dl></div><div class="chapter " id="post_install_overview"><div class="titlepage"><div><div><h2 class="title"><span class="number">21 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Overview</span> <a title="Permalink" class="permalink" href="#post_install_overview">#</a></h2></div></div></div><div class="line"></div><p>
   Once you have completed your cloud deployment, these are some of the common
   post-installation tasks you may need to perform. Take a look at the
   descriptions below to determine which of these you need to do.
  </p></div><div class="chapter " id="cloud_verification"><div class="titlepage"><div><div><h2 class="title"><span class="number">22 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cloud Verification</span> <a title="Permalink" class="permalink" href="#cloud_verification">#</a></h2></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#api_verification"><span class="number">22.1 </span><span class="name">API Verification</span></a></span></dt></dl></div></div><p>
  Once you have completed your cloud deployment, these are some of the common
  post-installation tasks you may need to perform to verify your cloud
  installation.
 </p><div class="sect1" id="api_verification"><div class="titlepage"><div><div><h2 class="title" id="api_verification"><span class="number">22.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">API Verification</span> <a title="Permalink" class="permalink" href="#api_verification">#</a></h2></div></div></div><p>
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> provides a tool, Tempest, that you can use to verify that
  your cloud deployment completed successfully:
 </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
    <a class="xref" href="#sec.api_verification.prereq" title="22.1.1. Prerequisites">Section 22.1.1, “Prerequisites”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec.api_verification.tempest" title="22.1.2. Tempest Integration Tests">Section 22.1.2, “Tempest Integration Tests”</a>
   </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
      <a class="xref" href="#sec.api_verification.running" title="22.1.3. Running the Tests">Section 22.1.3, “Running the Tests”</a>
     </p></li><li class="listitem "><p>
      <a class="xref" href="#sec.api_verification.result" title="22.1.4. Viewing Test Results">Section 22.1.4, “Viewing Test Results”</a>
     </p></li><li class="listitem "><p>
      <a class="xref" href="#sec.api_verification.custom" title="22.1.5. Customizing the Test Run">Section 22.1.5, “Customizing the Test Run”</a>
     </p></li></ul></div></li><li class="listitem "><p>
    <a class="xref" href="#sec.verify_block_storage.volume" title="23.1. Verifying Your Block Storage Backend">Section 23.1, “Verifying Your Block Storage Backend”</a>
   </p></li><li class="listitem "><p>
    <a class="xref" href="#sec.verify_block_storage.swift" title="23.2. Verify the Object Storage (Swift) Operations">Section 23.2, “Verify the Object Storage (Swift) Operations”</a>
   </p></li></ul></div><div class="sect2" id="sec.api_verification.prereq"><div class="titlepage"><div><div><h3 class="title" id="sec.api_verification.prereq"><span class="number">22.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#sec.api_verification.prereq">#</a></h3></div></div></div><p>
   The verification tests rely on you having an external network setup and a
   cloud image in your image (Glance) repository. Run the following playbook to
   configure your cloud:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts ardana-cloud-configure.yml</pre></div><div id="idm139651559615568" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, the EXT_NET_CIDR setting for the external network is
    now specified in the input model - see
    <span class="intraxref">Book “<em class="citetitle ">Planning an Installation with Cloud Lifecycle Manager</em>”, Chapter 7 “Configuration Objects”, Section 7.16 “Configuration Data”, Section 7.16.2 “Neutron Configuration Data”, Section 7.16.2.2 “neutron-external-networks”</span>.
   </p></div></div><div class="sect2" id="sec.api_verification.tempest"><div class="titlepage"><div><div><h3 class="title" id="sec.api_verification.tempest"><span class="number">22.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Tempest Integration Tests</span> <a title="Permalink" class="permalink" href="#sec.api_verification.tempest">#</a></h3></div></div></div><p>
   Tempest is a set of integration tests for OpenStack API validation,
   scenarios, and other specific tests to be run against a live OpenStack
   cluster. In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, Tempest has been modeled as a service and this
   gives you the ability to locate Tempest anywhere in the cloud. It is
   recommended that you install Tempest on your Cloud Lifecycle Manager node - that
   is where it resides by default in a new installation.
  </p><p>
   A version of the upstream
   <a class="link" href="http://docs.openstack.org/developer/tempest/" target="_blank">Tempest</a>
   integration tests is pre-deployed on the Cloud Lifecycle Manager node.
   For details on what Tempest is testing, you can check the contents of this
   file on your Cloud Lifecycle Manager:
  </p><div class="verbatim-wrap"><pre class="screen">/opt/stack/tempest/run_filters/ci.txt</pre></div><p>
   You can use these embedded tests to verify if the deployed cloud is
   functional.
  </p><p>
   For more information on running Tempest tests, see
   <a class="link" href="https://git.openstack.org/cgit/openstack/tempest/tree/README.rst" target="_blank">Tempest
   - The OpenStack Integration Test Suite</a>.
  </p><div id="idm139651559603328" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    Running these tests requires access to the deployed cloud's identity admin
    credentials
   </p></div><p>
   Tempest creates and deletes test accounts and test resources for test
   purposes.
  </p><p>
   In certain cases Tempest might fail to clean-up some of test resources after
   a test is complete, for example in case of failed tests.
  </p></div><div class="sect2" id="sec.api_verification.running"><div class="titlepage"><div><div><h3 class="title" id="sec.api_verification.running"><span class="number">22.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Running the Tests</span> <a title="Permalink" class="permalink" href="#sec.api_verification.running">#</a></h3></div></div></div><p>
   To run the default set of Tempest tests:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Ensure you can access your cloud:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts cloud-client-setup.yml
source /etc/environment</pre></div></li><li class="step "><p>
     Run the tests:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts tempest-run.yml</pre></div></li></ol></div></div><p>
   Optionally, you can <a class="xref" href="#sec.api_verification.custom" title="22.1.5. Customizing the Test Run">Section 22.1.5, “Customizing the Test Run”</a>.
  </p></div><div class="sect2" id="sec.api_verification.result"><div class="titlepage"><div><div><h3 class="title" id="sec.api_verification.result"><span class="number">22.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Viewing Test Results</span> <a title="Permalink" class="permalink" href="#sec.api_verification.result">#</a></h3></div></div></div><p>
   Tempest is deployed under <code class="literal">/opt/stack/tempest</code>. Test
   results are written in a log file in the following directory:
  </p><div class="verbatim-wrap"><pre class="screen">/opt/stack/tempest/logs</pre></div><p>
   A detailed log file is written to:
  </p><div class="verbatim-wrap"><pre class="screen">/opt/stack/tempest/tempest.log</pre></div><p>
   The results are also stored in the <code class="literal">testrepository</code>
   database.
  </p><p>
   To access the results after the run:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Change to the <code class="literal">tempest</code> directory and list the test
     results:
    </p><div class="verbatim-wrap"><pre class="screen">cd /opt/stack/tempest
./venv/bin/testr last</pre></div></li></ol></div></div><div id="idm139651559585776" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    If you encounter an error saying "local variable 'run_subunit_content'
    referenced before assignment", you may need to log in as the
    <code class="literal">tempest</code> user to run this command. This is due to a known
    issue reported at
    <a class="link" href="https://bugs.launchpad.net/testrepository/+bug/1348970" target="_blank">https://bugs.launchpad.net/testrepository/+bug/1348970</a>.
   </p></div><p>
   See
   <a class="link" href="https://testrepository.readthedocs.org/en/latest/" target="_blank">Test
   Repository Users Manual</a> for more details on how to manage the test
   result repository.
  </p></div><div class="sect2" id="sec.api_verification.custom"><div class="titlepage"><div><div><h3 class="title" id="sec.api_verification.custom"><span class="number">22.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Customizing the Test Run</span> <a title="Permalink" class="permalink" href="#sec.api_verification.custom">#</a></h3></div></div></div><p>
   There are several ways available to customize which tests will be executed.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <a class="xref" href="#sec.api_verification.service" title="22.1.6. Run Tests for Specific Services and Exclude Specific Features">Section 22.1.6, “Run Tests for Specific Services and Exclude Specific Features”</a>
    </p></li><li class="listitem "><p>
     <a class="xref" href="#sec.api_verification.list" title="22.1.7. Run Tests Matching a Series of White and Blacklists">Section 22.1.7, “Run Tests Matching a Series of White and Blacklists”</a>
    </p></li></ul></div></div><div class="sect2" id="sec.api_verification.service"><div class="titlepage"><div><div><h3 class="title" id="sec.api_verification.service"><span class="number">22.1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Run Tests for Specific Services and Exclude Specific Features</span> <a title="Permalink" class="permalink" href="#sec.api_verification.service">#</a></h3></div></div></div><p>
   Tempest allows you to test specific services and features using the
   <code class="literal">tempest.conf</code> configuration file.
  </p><p>
   A working configuration file with inline documentation is deployed under
   <code class="literal">/opt/stack/tempest/etc/</code>.
  </p><p>
   To use this, follow these steps:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Edit the
     <code class="literal">/opt/stack/tempest/configs/tempest_region1.conf</code> file.
    </p></li><li class="step "><p>
     To test specific service, edit the <code class="literal">[service_available]</code>
     section and clear the comment character <code class="literal">#</code> and set a
     line to <code class="literal">true</code> to test that service or
     <code class="literal">false</code> to not test that service.
    </p><div class="verbatim-wrap"><pre class="screen">cinder = true
neutron = false</pre></div></li><li class="step "><p>
     To test specific features, edit any of the
     <code class="literal">*_feature_enabled</code> sections to enable or disable tests
     on specific features of a service.
    </p><div class="verbatim-wrap"><pre class="screen">[volume-feature-enabled]
[compute-feature-enabled]
[identity-feature-enabled]
[image-feature-enabled]
[network-feature-enabled]
[object-storage-feature-enabled]</pre></div><div class="verbatim-wrap"><pre class="screen">#Is the v2 identity API enabled (boolean value)
api_v2 = true
#Is the v3 identity API enabled (boolean value)
api_v3 = false</pre></div></li><li class="step "><p>
     Then run tests normally
    </p></li></ol></div></div></div><div class="sect2" id="sec.api_verification.list"><div class="titlepage"><div><div><h3 class="title" id="sec.api_verification.list"><span class="number">22.1.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Run Tests Matching a Series of White and Blacklists</span> <a title="Permalink" class="permalink" href="#sec.api_verification.list">#</a></h3></div></div></div><p>
   You can run tests against specific scenarios by editing or creating a run
   filter file.
  </p><p>
   Run filter files are deployed under
   <code class="literal">/opt/stack/tempest/run_filters</code>.
  </p><p>
   Use run filters to whitelist or blacklist specific tests or groups of tests:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     lines starting with # or empty are ignored
    </p></li><li class="listitem "><p>
     lines starting with <code class="literal">+</code> are whitelisted
    </p></li><li class="listitem "><p>
     lines starting with <code class="literal">-</code> are blacklisted
    </p></li><li class="listitem "><p>
     lines not matching any of the above conditions are blacklisted
    </p></li></ul></div><p>
   If whitelist is empty, all available tests are fed to blacklist. If
   blacklist is empty, all tests from whitelist are returned.
  </p><p>
   Whitelist is applied first. The blacklist is executed against the set of
   tests returned by the whitelist.
  </p><p>
   To run whitelist and blacklist tests:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Make sure you can access the cloud:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts cloud-client-setup.yml
source /etc/environment</pre></div></li><li class="step "><p>
     Run the tests:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts tempest-run.yml  -e run_filter &lt;run_filter_name&gt;</pre></div></li></ol></div></div><p>
   Note that the run_filter_name is the name of the run_filter file except for
   the extension. For instance, to run using the filter from the file
   /opt/stack/tempest/run_filters/ci.txt, use the following:
  </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts tempest-run.yml -e run_filter=ci</pre></div><p>
   Documentation on the format of white and black-lists is available at:
  </p><div class="verbatim-wrap"><pre class="screen">/opt/stack/tempest/tests2skip.py</pre></div><p>
   Example:
  </p><p>
   The following entries run API tests, exclude tests that are less relevant
   for deployment validation, such as negative, admin, cli and third-party (EC2)
   tests:
  </p><div class="verbatim-wrap"><pre class="screen">+tempest\.api\.*
*[Aa]dmin.*
*[Nn]egative.*
- tempest\.cli.*
- tempest\.thirdparty\.*</pre></div></div></div></div><div class="chapter " id="ui_verification"><div class="titlepage"><div><div><h2 class="title"><span class="number">23 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">UI Verification</span> <a title="Permalink" class="permalink" href="#ui_verification">#</a></h2></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#sec.verify_block_storage.volume"><span class="number">23.1 </span><span class="name">Verifying Your Block Storage Backend</span></a></span></dt><dt><span class="section"><a href="#sec.verify_block_storage.swift"><span class="number">23.2 </span><span class="name">Verify the Object Storage (Swift) Operations</span></a></span></dt><dt><span class="section"><a href="#upload_image"><span class="number">23.3 </span><span class="name">Uploading an Image for Use</span></a></span></dt><dt><span class="section"><a href="#create_extnet"><span class="number">23.4 </span><span class="name">Creating an External Network</span></a></span></dt></dl></div></div><p>
  Once you have completed your cloud deployment, these are some of the common
  post-installation tasks you may need to perform to verify your cloud
  installation.
 </p><div class="sect1" id="sec.verify_block_storage.volume"><div class="titlepage"><div><div><h2 class="title" id="sec.verify_block_storage.volume"><span class="number">23.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Verifying Your Block Storage Backend</span> <a title="Permalink" class="permalink" href="#sec.verify_block_storage.volume">#</a></h2></div></div></div><p>
  The sections below will show you the steps to verify that your Block Storage
  backend was setup properly.
 </p><div class="sect2" id="idm139651559539152"><div class="titlepage"><div><div><h3 class="title" id="idm139651559539152"><span class="number">23.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create a Volume</span> <a title="Permalink" class="permalink" href="#idm139651559539152">#</a></h3></div></div></div><p>
   Perform the following steps to create a volume using Horizon dashboard.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log into the Horizon dashboard. For more information, see
     <span class="intraxref">Book “User Guide Overview”, Chapter 3 “Cloud Admin Actions with the Dashboard”</span>.
    </p></li><li class="step "><p>
     Choose <span class="guimenu">Project</span> › <span class="guimenu">Compute</span> › <span class="guimenu">Volumes</span>.
    </p></li><li class="step "><p>
     On the <span class="guimenu">Volumes</span> tabs, click the
     <span class="guimenu">Create Volume</span> button to create a volume.
    </p></li><li class="step "><p>
     In the <span class="guimenu">Create Volume</span> options, enter the
     required details into the fields and then click the
     <span class="guimenu">Create Volume</span> button:
    </p><ol type="a" class="substeps "><li class="step "><p>
       Volume Name - This is the name you specify for your volume.
      </p></li><li class="step "><p>
       Description (optional) - This is an optional description for the volume.
      </p></li><li class="step "><p>
       Type - Select the volume type you have created for your volumes from the
       drop down.
      </p></li><li class="step "><p>
       Size (GB) - Enter the size, in GB, you would like the volume to be.
      </p></li><li class="step "><p>
       Availability Zone - You can either leave this at the default option of
       <span class="guimenu">Any Availability Zone</span> or select a
       specific zone from the drop-down box.
      </p></li></ol></li></ol></div></div><p>
   The dashboard will then show the volume you have just created.
  </p></div><div class="sect2" id="idm139651559523648"><div class="titlepage"><div><div><h3 class="title" id="idm139651559523648"><span class="number">23.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Attach Volume to an Instance</span> <a title="Permalink" class="permalink" href="#idm139651559523648">#</a></h3></div></div></div><p>
   Perform the following steps to attach a volume to an instance:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log into the Horizon dashboard. For more information, see
     <span class="intraxref">Book “User Guide Overview”, Chapter 3 “Cloud Admin Actions with the Dashboard”</span>.
    </p></li><li class="step "><p>
     Choose <span class="guimenu">Project</span> › <span class="guimenu">Compute</span> › <span class="guimenu">Instances</span>.
    </p></li><li class="step "><p>
     In the <span class="guimenu">Action</span> column, choose the
     <span class="guimenu">Edit Attachments</span> in the drop-down
     box next to the instance you want to attach the volume to.
    </p></li><li class="step "><p>
     In the <span class="guimenu">Attach To Instance</span> drop-down,
     select the volume that you want to attach.
    </p></li><li class="step "><p>
     Edit the <span class="guimenu">Device Name</span> if necessary.
    </p></li><li class="step "><p>
     Click <span class="guimenu">Attach Volume</span> to complete the
     action.
    </p></li><li class="step "><p>
     On the <span class="guimenu">Volumes</span> screen,
     verify that the volume you attached is displayed in the
     <span class="guimenu">Attached To</span> columns.
    </p></li></ol></div></div></div><div class="sect2" id="idm139651559510256"><div class="titlepage"><div><div><h3 class="title" id="idm139651559510256"><span class="number">23.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Detach Volume from Instance</span> <a title="Permalink" class="permalink" href="#idm139651559510256">#</a></h3></div></div></div><p>
   Perform the following steps to detach the volume from instance:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log into the Horizon dashboard. For more information, see
     <span class="intraxref">Book “User Guide Overview”, Chapter 3 “Cloud Admin Actions with the Dashboard”</span>.
    </p></li><li class="step "><p>
     Choose <span class="guimenu">Project</span> › <span class="guimenu">Compute</span> › <span class="guimenu">Instances</span>.
    </p></li><li class="step "><p>
     Click the check box next to the name of the volume you want to detach.
    </p></li><li class="step "><p>
     In the <span class="guimenu">Action</span> column, choose the
     <span class="guimenu">Edit Attachments</span> in the drop-down
     box next to the instance you want to attach the volume to.
    </p></li><li class="step "><p>
     Click <span class="guimenu">Detach Attachment</span>. A confirmation
     dialog box appears.
    </p></li><li class="step "><p>
     Click <span class="guimenu">Detach Attachment</span> to confirm the
     detachment of the volume from the associated instance.
    </p></li></ol></div></div></div><div class="sect2" id="idm139651559499072"><div class="titlepage"><div><div><h3 class="title" id="idm139651559499072"><span class="number">23.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Delete Volume</span> <a title="Permalink" class="permalink" href="#idm139651559499072">#</a></h3></div></div></div><p>
   Perform the following steps to delete a volume using Horizon dashboard:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log into the Horizon dashboard. For more information, see
     <span class="intraxref">Book “User Guide Overview”, Chapter 3 “Cloud Admin Actions with the Dashboard”</span>.
    </p></li><li class="step "><p>
     Choose <span class="guimenu">Project</span> › <span class="guimenu">Compute</span> › <span class="guimenu">Volumes</span>.
    </p></li><li class="step "><p>
     In the <span class="guimenu">Actions</span> column, click
     <span class="guimenu">Delete Volume</span> next to the volume you
     would like to delete.
    </p></li><li class="step "><p>
     To confirm and delete the volume, click <span class="guimenu">Delete Volume</span>
     again.
    </p></li><li class="step "><p>
     Verify that the volume was removed from the
     <span class="guimenu">Volumes</span> screen.
    </p></li></ol></div></div></div><div class="sect2" id="idm139651559488928"><div class="titlepage"><div><div><h3 class="title" id="idm139651559488928"><span class="number">23.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Verifying Your Object Storage (Swift)</span> <a title="Permalink" class="permalink" href="#idm139651559488928">#</a></h3></div></div></div><p>
   The following procedure shows how to validate that all servers have been
   added to the Swift rings:
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Run the swift-compare-model-rings.yml playbook as follows:
    </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts swift-compare-model-rings.yml</pre></div></li><li class="step "><p>
     Search for output similar to the following. Specifically, look at the
     number of drives that are proposed to be added.
    </p><div class="verbatim-wrap"><pre class="screen">TASK: [swiftlm-ring-supervisor | validate-input-model | Print report] *********
ok: [ardana-cp1-c1-m1-mgmt] =&gt; {
    "var": {
        "report.stdout_lines": [
            "Rings:",
            "  ACCOUNT:",
            "    ring exists",
            "    no device changes",
            "    ring will be rebalanced",
            "  CONTAINER:",
            "    ring exists",
            "    no device changes",
            "    ring will be rebalanced",
            "  OBJECT-0:",
            "    ring exists",
            "    no device changes",
            "    ring will be rebalanced"
        ]
    }
}</pre></div></li><li class="step "><p>
     If the text contains "no device changes" then the deploy was successful
     and no further action is needed.
    </p></li><li class="step "><p>
     If more drives need to be added, it indicates that the deploy
     failed on some nodes and that you restarted the deploy to include those
     nodes. However, the nodes are not in the Swift rings because enough time
     has not elapsed to allow the rings to be rebuilt. You have two options to
     continue:
    </p><ol type="a" class="substeps "><li class="step "><p>
       Repeat the deploy. There are two steps:
      </p><ol type="i" class="substeps "><li class="step "><p>
         Delete the ring builder files as described in
         <span class="intraxref">Book “Operations Guide”, Chapter 15 “Troubleshooting Issues”, Section 15.6 “Storage Troubleshooting”, Section 15.6.2 “Swift Storage Troubleshooting”, Section 15.6.2.8 “Restarting the Object Storage Deployment”</span>.
        </p></li><li class="step "><p>
         Repeat the installation process starting by running the
         <code class="filename">site.yml</code> playbook as described in
         <a class="xref" href="#sec.kvm.deploy" title="13.7. Deploying the Cloud">Section 13.7, “Deploying the Cloud”</a>.
        </p></li></ol></li><li class="step "><p>
       Rebalance the rings several times until all drives are incorporated in
       the rings. This process may take several hours to complete (because you
       need to wait one hour between each rebalance). The steps are as follows:
      </p><ol type="i" class="substeps "><li class="step "><p>
         Change the min-part-hours to 1 hour. See
         <span class="intraxref">Book “Operations Guide”, Chapter 8 “Managing Object Storage”, Section 8.5 “Managing Swift Rings”, Section 8.5.7 “Changing min-part-hours in Swift”</span>.
        </p></li><li class="step "><p>
         Use the "First phase of ring rebalance" and "Final rebalance phase" as
         described in <span class="intraxref">Book “Operations Guide”, Chapter 8 “Managing Object Storage”, Section 8.5 “Managing Swift Rings”, Section 8.5.5 “Applying Input Model Changes to Existing Rings”</span>.
         The <span class="quote">“<span class="quote">Weight change phase of ring rebalance</span>”</span> does not
         apply because you have not set the weight-step attribute at this
         stage.
        </p></li><li class="step "><p>
         Set the min-part-hours to the recommended 16 hours as described in
         <span class="intraxref">Book “Operations Guide”, Chapter 8 “Managing Object Storage”, Section 8.5 “Managing Swift Rings”, Section 8.5.7 “Changing min-part-hours in Swift”</span>.
        </p></li></ol></li></ol></li></ol></div></div><p>
   If you receive errors during the validation, see
   <span class="intraxref">Book “Operations Guide”, Chapter 15 “Troubleshooting Issues”, Section 15.6 “Storage Troubleshooting”, Section 15.6.2 “Swift Storage Troubleshooting”, Section 15.6.2.3 “Interpreting Swift Input Model Validation Errors”</span>.
  </p></div></div><div class="sect1" id="sec.verify_block_storage.swift"><div class="titlepage"><div><div><h2 class="title" id="sec.verify_block_storage.swift"><span class="number">23.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Verify the Object Storage (Swift) Operations</span> <a title="Permalink" class="permalink" href="#sec.verify_block_storage.swift">#</a></h2></div></div></div><p>
   For information about verifying the operations, see
   <span class="intraxref">Book “Operations Guide”, Chapter 8 “Managing Object Storage”, Section 8.1 “Running the Swift Dispersion Report”</span>.
  </p></div><div class="sect1" id="upload_image"><div class="titlepage"><div><div><h2 class="title" id="upload_image"><span class="number">23.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Uploading an Image for Use</span> <a title="Permalink" class="permalink" href="#upload_image">#</a></h2></div></div></div><p>
  To create a Compute instance, you need to obtain an image that you can use.
  The Cloud Lifecycle Manager provides an Ansible playbook that will
  download a CirrOS Linux image, and then upload it as a public image to your
  image repository for use across your projects.
 </p><div class="sect2" id="idm139651559461248"><div class="titlepage"><div><div><h3 class="title" id="idm139651559461248"><span class="number">23.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Running the Playbook</span> <a title="Permalink" class="permalink" href="#idm139651559461248">#</a></h3></div></div></div><p>
   Use the following command to run this playbook:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts glance-cloud-configure.yml -e proxy=&lt;PROXY&gt;</pre></div><p>
   The table below shows the optional switch that you can use as part of this
   playbook to specify environment-specific information:
  </p><div class="informaltable"><table border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Switch</th><th>Description</th></tr></thead><tbody><tr><td>
       <p>
        <code class="literal">-e proxy="&lt;proxy_address:port&gt;"</code>
       </p>
      </td><td>
       <p>
        Optional. If your environment requires a proxy for the internet, use
        this switch to specify the proxy information.
       </p>
      </td></tr></tbody></table></div></div><div class="sect2" id="idm139651559449424"><div class="titlepage"><div><div><h3 class="title" id="idm139651559449424"><span class="number">23.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How to Curate Your Own Images</span> <a title="Permalink" class="permalink" href="#idm139651559449424">#</a></h3></div></div></div><p>
   OpenStack has created a guide to show you how to obtain, create, and modify
   images that will be compatible with your cloud:
  </p><p>
   <a class="link" href="http://docs.openstack.org/image-guide/content/" target="_blank">OpenStack
   Virtual Machine Image Guide</a>
  </p></div><div class="sect2" id="idm139651559446720"><div class="titlepage"><div><div><h3 class="title" id="idm139651559446720"><span class="number">23.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the GlanceClient CLI to Create Images</span> <a title="Permalink" class="permalink" href="#idm139651559446720">#</a></h3></div></div></div><p>
   You can use the GlanceClient on a machine accessible to your cloud or it's
   also installed automatically on your Cloud Lifecycle Manager.
  </p><p>
   The GlanceClient allows you to create, update, list, and delete images as
   well as manage your image member lists, which allows you to share access to
   images across multiple tenants. As with most of the OpenStack CLI tools, you
   can use the <code class="literal">glance help</code> command to get a full list of
   commands as well as their syntax.
  </p><p>
   If you would like to use the <code class="literal">--copy-from</code> option when
   creating an image, you will need to have your Administrator enable the http
   store in your environment using the instructions outlined at
   <span class="intraxref">Book “Operations Guide”, Chapter 5 “Managing Compute”, Section 5.6 “Configuring the Image Service”, Section 5.6.2 “Allowing the Glance copy-from option in your environment”</span>.
  </p></div></div><div class="sect1" id="create_extnet"><div class="titlepage"><div><div><h2 class="title" id="create_extnet"><span class="number">23.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Creating an External Network</span> <a title="Permalink" class="permalink" href="#create_extnet">#</a></h2></div></div></div><p>
  You must have an external network set up to allow your Compute instances to
  reach the internet. There are multiple methods you can use to create this
  external network and we provide two of them here. The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> installer
  provides an Ansible playbook that will create this network for use across
  your projects. We also show you how to create this network via the command
  line tool from your Cloud Lifecycle Manager.
 </p><div class="sect2" id="idm139651559437440"><div class="titlepage"><div><div><h3 class="title" id="idm139651559437440"><span class="number">23.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Notes</span> <a title="Permalink" class="permalink" href="#idm139651559437440">#</a></h3></div></div></div><p>
   If you have multiple regions in your cloud environment, it is important that
   when you set up your external networks in each region that you allocate
   unique IP ranges for each. If you have overlapping CIDRs then you risk
   having the same floating IP being allocated to two different virtual
   machines which will cause a conflict.
  </p></div><div class="sect2" id="sec.create_extnet-playbook"><div class="titlepage"><div><div><h3 class="title" id="sec.create_extnet-playbook"><span class="number">23.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the Ansible Playbook</span> <a title="Permalink" class="permalink" href="#sec.create_extnet-playbook">#</a></h3></div></div></div><p>
   This playbook will query the Networking service for an existing external
   network, and then create a new one if you do not already have one. The
   resulting external network will have the name <code class="literal">ext-net</code>
   with a subnet matching the CIDR you specify in the command below.
  </p><p>
   If you need to specify more granularity, for example specifying an
   allocation pool for the subnet then you should utilize the
   <a class="xref" href="#sec.create_extnet-cli" title="23.4.3. Using the NeutronClient CLI">Section 23.4.3, “Using the NeutronClient CLI”</a>.
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts neutron-cloud-configure.yml -e EXT_NET_CIDR=&lt;CIDR&gt;</pre></div><p>
   The table below shows the optional switch that you can use as part of this
   playbook to specify environment-specific information:
  </p><div class="informaltable"><table border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Switch</th><th>Description</th></tr></thead><tbody><tr><td>
       <p>
        <code class="literal">-e EXT_NET_CIDR=&lt;CIDR&gt;</code>
       </p>
      </td><td>
       <p>
        Optional. You can use this switch to specify the external network CIDR.
        If you choose not to use this switch, or use a wrong value, the VMs
        will not be accessible over the network.
       </p>
       <p>
        This CIDR will be from the <code class="literal">EXTERNAL VM</code> network.
       </p>
       <div id="idm139651559421648" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
         If this option is not defined the default value is "172.31.0.0/16"
        </p></div>
      </td></tr></tbody></table></div></div><div class="sect2" id="sec.create_extnet-cli"><div class="titlepage"><div><div><h3 class="title" id="sec.create_extnet-cli"><span class="number">23.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using the NeutronClient CLI</span> <a title="Permalink" class="permalink" href="#sec.create_extnet-cli">#</a></h3></div></div></div><p>
   For more granularity you can utilize the Neutron command line tool to create
   your external network.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to the Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Source the Admin credentials:
    </p><div class="verbatim-wrap"><pre class="screen">source ~/service.osrc</pre></div></li><li class="step "><p>
     Create the external network and then the subnet using these commands
     below.
    </p><p>
     Creating the network:
    </p><div class="verbatim-wrap"><pre class="screen">neutron net-create --router:external &lt;external-network-name&gt;</pre></div><p>
     Creating the subnet:
    </p><div class="verbatim-wrap"><pre class="screen">neutron subnet-create &lt;external-network-name&gt; &lt;CIDR&gt; --gateway &lt;gateway&gt; \
--allocation-pool start=&lt;IP_start&gt;,end=&lt;IP_end&gt; [--disable-dhcp]</pre></div><p>
     Where:
    </p><div class="informaltable"><table border="1"><colgroup><col class="c1" /><col class="c2" /></colgroup><thead><tr><th>Value</th><th>Description</th></tr></thead><tbody><tr><td>external-network-name</td><td>
         <p>
          This is the name given to your external network. This is a unique
          value that you will choose. The value <code class="literal">ext-net</code> is
          usually used.
         </p>
        </td></tr><tr><td>CIDR</td><td>
         <p>
          You can use this switch to specify the external network CIDR. If you
          choose not to use this switch, or use a wrong value, the VMs will not
          be accessible over the network.
         </p>
         <p>
          This CIDR will be from the EXTERNAL VM network.
         </p>
        </td></tr><tr><td>--gateway</td><td>
         <p>
          Optional switch to specify the gateway IP for your subnet. If this
          is not included then it will choose the first available IP.
         </p>
        </td></tr><tr><td>
         --allocation-pool start end
        </td><td>
         <p>
          Optional switch to specify a start and end IP address to use as the
          allocation pool for this subnet.
         </p>
        </td></tr><tr><td>--disable-dhcp</td><td>
         <p>
          Optional switch if you want to disable DHCP on this subnet. If this
          is not specified then DHCP will be enabled.
         </p>
        </td></tr></tbody></table></div></li></ol></div></div></div><div class="sect2" id="idm139651559394544"><div class="titlepage"><div><div><h3 class="title" id="idm139651559394544"><span class="number">23.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Next Steps</span> <a title="Permalink" class="permalink" href="#idm139651559394544">#</a></h3></div></div></div><p>
   Once the external network is created, users can create a Private Network to
   complete their networking setup. For instructions, see
   <span class="intraxref">Book “User Guide Overview”, Chapter 8 “Creating a Private Network”</span>.
  </p></div></div></div><div class="chapter " id="install_openstack_clients"><div class="titlepage"><div><div><h2 class="title"><span class="number">24 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing OpenStack Clients</span> <a title="Permalink" class="permalink" href="#install_openstack_clients">#</a></h2></div></div></div><div class="line"></div><p>
  If you have a standalone deployer, the OpenStack CLI and other clients will
	not be installed automatically on that node. If you require access to these
	clients, you will need to follow the procedure below to add the appropriate
	software.
 </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
    [OPTIONAL] Connect to your standalone deployer and try to use the OpenStack
    CLI:
   </p><div class="verbatim-wrap"><pre class="screen">source ~/keystone.osrc
<span class="bold"><strong>openstack project list</strong></span>

-bash: openstack: command not found</pre></div></li><li class="step "><p>
    Edit the configuration file containing details of your Control Plane,
    typically
    <code class="literal">~/openstack/my_cloud/definition/data/control_plane</code>.
   </p></li><li class="step "><p>
    Locate the stanza for the cluster where you want to install the client(s).
    For a standalone deployer, this will look like the following extract:
   </p><div class="verbatim-wrap"><pre class="screen">      clusters:
        - name: cluster0
          cluster-prefix: c0
          server-role: LIFECYCLE-MANAGER-ROLE
          member-count: 1
          allocation-policy: strict
          service-components:
            - ntp-server
            - lifecycle-manager</pre></div></li><li class="step "><p>
    Choose the client(s) you wish to install from the following list of
    available clients:
   </p><div class="verbatim-wrap"><pre class="screen"> - openstack-client
 - ceilometer-client
 - cinder-client
 - designate-client
 - glance-client
 - heat-client
 - ironic-client
 - keystone-client
 - neutron-client
 - nova-client
 - swift-client
 - monasca-client
 - barbican-client</pre></div></li><li class="step "><p>
    Add the client(s) to the list of <code class="literal">service-components</code> - in
    this example, we add the <code class="literal">openstack-client</code> to the
    standalone deployer:
   </p><div class="verbatim-wrap"><pre class="screen">      clusters:
        - name: cluster0
          cluster-prefix: c0
          server-role: LIFECYCLE-MANAGER-ROLE
          member-count: 1
          allocation-policy: strict
          service-components:
            - ntp-server
            - lifecycle-manager
            <span class="bold"><strong>- openstack-client
            - ceilometer-client
            - cinder-client
            - designate-client
            - glance-client
            - heat-client
            - ironic-client
            - keystone-client
            - neutron-client
            - nova-client
            - swift-client
            - monasca-client
            - barbican-client
</strong></span></pre></div></li><li class="step "><p>
    Commit the configuration changes:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
git add -A
git commit -m "Add explicit client service deployment"</pre></div></li><li class="step "><p>
    Run the configuration processor, followed by the
    <code class="literal">ready-deployment</code> playbook:
   </p><div class="verbatim-wrap"><pre class="screen">cd ~/openstack/ardana/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml -e encrypt="" \
  -e rekey=""
ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div></li><li class="step "><p>
    Add the software for the clients using the following command:
   </p><div class="verbatim-wrap"><pre class="screen">cd /home/stack/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts clients-upgrade.yml</pre></div></li><li class="step "><p>
    Check that the software has been installed correctly. In this instance,
    connect to your standalone deployer and try to use the OpenStack CLI:
   </p><div class="verbatim-wrap"><pre class="screen">source ~/keystone.osrc
openstack project list</pre></div><p>
    You should now see a list of projects returned:
   </p><div class="verbatim-wrap"><pre class="screen">stack@ardana-cp1-c0-m1-mgmt:~$ <span class="bold"><strong>openstack project list</strong></span>

+----------------------------------+------------------+
| ID                               | Name             |
+----------------------------------+------------------+
| 076b6e879f324183bbd28b46a7ee7826 | kronos           |
| 0b81c3a9e59c47cab0e208ea1bb7f827 | backup           |
| 143891c2a6094e2988358afc99043643 | octavia          |
| 1d3972a674434f3c95a1d5ed19e0008f | glance-swift     |
| 2e372dc57cac4915bf06bbee059fc547 | glance-check     |
| 383abda56aa2482b95fb9da0b9dd91f4 | monitor          |
| 606dd3b1fa6146668d468713413fb9a6 | swift-monitor    |
| 87db9d1b30044ea199f0293f63d84652 | admin            |
| 9fbb7494956a483ca731748126f50919 | demo             |
| a59d0c682474434a9ddc240ddfe71871 | services         |
| a69398f0f66a41b2872bcf45d55311a7 | swift-dispersion |
| f5ec48d0328d400992c1c5fb44ec238f | cinderinternal   |
+----------------------------------+------------------+</pre></div></li></ol></div></div></div><div class="chapter " id="tls30"><div class="titlepage"><div><div><h2 class="title"><span class="number">25 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Transport Layer Security (TLS)</span> <a title="Permalink" class="permalink" href="#tls30">#</a></h2></div><div><div class="abstract"><div class="abstract-title-wrap"><h6 class="abstract-title">Abstract<a title="Permalink" class="permalink" href="#idm139651559365936">#</a></h6></div><p>
    TLS is enabled by default during the installation of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> and
    additional configuration options are available to secure your environment,
    as described below.
   </p></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#idm139651559329872"><span class="number">25.1 </span><span class="name">Configuring TLS in the input model</span></a></span></dt><dt><span class="section"><a href="#idm139651559305808"><span class="number">25.2 </span><span class="name">User-provided certificates and trust chains</span></a></span></dt><dt><span class="section"><a href="#idm139651559292624"><span class="number">25.3 </span><span class="name">Edit the input model to include your certificate files</span></a></span></dt><dt><span class="section"><a href="#sec.generate-certificate"><span class="number">25.4 </span><span class="name">Generate a self-signed CA</span></a></span></dt><dt><span class="section"><a href="#idm139651559263120"><span class="number">25.5 </span><span class="name">Generate a certificate signing request</span></a></span></dt><dt><span class="section"><a href="#idm139651559257920"><span class="number">25.6 </span><span class="name">Generate a server certificate</span></a></span></dt><dt><span class="section"><a href="#sec.upload-toclm"><span class="number">25.7 </span><span class="name">Upload to the Cloud Lifecycle Manager</span></a></span></dt><dt><span class="section"><a href="#idm139651559221264"><span class="number">25.8 </span><span class="name">Configuring the cipher suite</span></a></span></dt><dt><span class="section"><a href="#idm139651559216528"><span class="number">25.9 </span><span class="name">Testing</span></a></span></dt><dt><span class="section"><a href="#idm139651559211632"><span class="number">25.10 </span><span class="name">Verifying that the trust chain is correctly deployed</span></a></span></dt><dt><span class="section"><a href="#idm139651559208208"><span class="number">25.11 </span><span class="name">Turning TLS on or off</span></a></span></dt></dl></div></div><p>
  In <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>, you can provide your own certificate authority and
  certificates for internal and public virtual IP addresses (VIPs), and you
  should do so for any production cloud. The certificates automatically
  generated by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> are useful for testing and setup, but you should always
  install your own for production use. Certificate installation is discussed
  below.
 </p><p>
  Please read the following if you're using the default <code class="literal">cert-name:
  my-public-cert</code> in your model.
 </p><p>
  The bundled test certificate for public endpoints, located at
  <code class="filename">~/openstack/my_cloud/config/tls/certs/my-public-cert</code>, is
  now expired but was left in the product in case you changed the content with
  your valid certificate. Please verify if the certificate is expired and
  generate your own, as described in
  <a class="xref" href="#sec.generate-certificate" title="25.4. Generate a self-signed CA">Section 25.4, “Generate a self-signed CA”</a>.
 </p><p>
  You can verify the expiry date by running this command:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openssl x509 -in ~/openstack/my_cloud/config/tls/certs/my-public-cert \
-noout -enddate
notAfter=Oct  8 09:01:58 2016 GMT</pre></div><p>
  Before you begin, the following list of terms will be helpful when generating
  and installing certificates.
 </p><div class="variablelist "><dl class="variablelist"><dt id="idm139651559352688"><span class="term "><span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>-generated public CA</span></dt><dd><p>
     A <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>-generated public CA
     (<code class="filename">openstack_frontend_cacert.crt</code>) is available for you
     to use in <code class="filename">/etc/pki/trust/anchors/ca-certificates</code>.
    </p></dd><dt id="idm139651559346960"><span class="term ">Fully qualified domain name (FQDN) of the public VIP</span></dt><dd><p>
     The registered domain name. A FQDN is not mandatory. It is perfectly valid
     to have no FQDN and use IP addresses instead. Note that you can use FQDNs
     on public endpoints, and you may change them whenever the need arises.
    </p></dd><dt id="idm139651559344912"><span class="term ">Certificate authority (CA) certificate</span></dt><dd><p>
     Your certificates must be signed by a CA, such as your internal IT
     department or a public certificate authority. For this example we will use
     a self-signed certificate.
    </p></dd><dt id="idm139651559342928"><span class="term ">Server certificate</span></dt><dd><p>
     It is easy to confuse server certificates and CA certificates. Server
     certificates reside on the server and CA certificates reside on the
     client. A server certificate affirms that the server that sent it serves a
     set of IP addresses, domain names, and set of services. A CA certificate
     is used by the client to authenticate this claim.
    </p></dd><dt id="idm139651559340784"><span class="term ">SAN (subject-alt-name)</span></dt><dd><p>
     The set of IP addresses and domain names in a server certificate request:
     A template for a server certificate.
    </p></dd><dt id="idm139651559338880"><span class="term ">Certificate signing request (CSR)</span></dt><dd><p>
     A blob of data generated from a certificate request and sent to a CA,
     which would then sign it, produce a server certificate, and send it back.
    </p></dd><dt id="idm139651559336928"><span class="term ">External VIP</span></dt><dd><p>
     External virtual IP address
    </p></dd><dt id="idm139651559335120"><span class="term ">Internal VIP</span></dt><dd><p>
     Internal virtual IP address
    </p></dd></dl></div><p>
  The major difference between an external VIP certificate and an internal VIP
  certificate is that the internal VIP has approximately 40 domain names in the
  SAN. This is because each service has a different domain name in
  <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>. So it is unlikely that you can create an internal server
  certificate before running the configuration processor. But after a
  configuration processor run, a certificate request would be created for each
  of your cert-names.
 </p><div class="sect1" id="idm139651559329872"><div class="titlepage"><div><div><h2 class="title" id="idm139651559329872"><span class="number">25.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring TLS in the input model</span> <a title="Permalink" class="permalink" href="#idm139651559329872">#</a></h2></div></div></div><p>
   For this example certificate configuration, let's assume there's no FQDN for
   the external VIP and that you're going to use the default IP address
   provided by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span>. Let's also assume that for the internal VIP you
   will use the defaults as well. If you were to call your certificate
   authority "example-CA," the CA certificate would then be called
   "example-CA.crt" and the key would be called "example-CA.key." In the
   following examples, the external VIP certificate will be named
   "example-public-cert" and the internal VIP certificate will be named
   "example-internal-cert."
  </p><div id="idm139651559325680" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    Cautions:
   </p></div><p>
   Any time you make a cert change when using your own CA:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     You should use a distinct name from those already existing in
     <code class="filename">config/tls/cacerts</code>. This also means that you should
     not <span class="emphasis"><em>reuse</em></span> your CA names (and use unique and
     distinguishable names such as MyCompanyXYZ_PrivateRootCA.crt). A new name
     is what indicates that a file is new or changed, so reusing a name means
     that the file is not considered changed even its contents have changed.
    </p></li><li class="listitem "><p>
     You should not remove any existing CA files from
     <code class="filename">config/tls/cacerts</code>.
    </p></li><li class="listitem "><p>
     If you want to remove an existing CA you must
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       First remove the file.
      </p></li><li class="step "><p>
       Then run:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible -i hosts/verb_hosts FND-STN -a 'sudo keytool -delete -alias \
debian:&lt;filename to remove&gt; \
-keystore /usr/lib/jvm/java-7-openjdk-amd64/jre/lib/security/cacerts \
-storepass changeit'</pre></div></li></ol></div></div></li></ul></div><div id="idm139651559315856" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
    Be sure to install your own certificate for all production clouds after
    installing and testing your cloud. If you ever want to test or troubleshoot
    later, you will be able to revert to the sample certificate to get back to
    a stable state for testing.
   </p></div><div id="idm139651559314704" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    Unless this is a new deployment, do not update both the certificate and the
    CA together. Add the CA first and then run a site deploy. Then update the
    certificate and run tls-reconfigure, FND-CLU-stop, FND-CLU-start and then
    ardana-reconfigure. If a playbook has failed, rerun it with -vv to get
    detailed error information. The configure, HAproxy restart, and reconfigure
    steps are included below. If this is a new deployment and you are adding
    your own certs/CA before running site.yml this caveat does not apply.
   </p></div><p>
   You can add your own certificate by following the instructions below. All
   changes must go into the file
   <code class="filename">~/openstack/my_cloud/definition/data/network_groups.yml</code>.
  </p><p>
   Below are the entries for TLS for the internal and admin load balancers:
  </p><div class="verbatim-wrap"><pre class="screen">- provider: ip-cluster
        name: lb
        tls-components:
        - default
        components:
        # These services do not currently support TLS so they are not listed
        # under tls-components
        - nova-metadata
        roles:
        - internal
        - admin
        cert-file: openstack-internal-cert
        # The openstack-internal-cert is a reserved name and
        # this certificate will be autogenerated. You
        # can bring in your own certificate with a different name

        # cert-file: customer-provided-internal-cert
        # replace this with name of file in "config/tls/certs/"</pre></div><p>
   The configuration processor will also create a request template for each
   named certificate under <code class="literal">info/cert_reqs/</code> This will be of
   the form:
  </p><div class="verbatim-wrap"><pre class="screen">info/cert_reqs/customer-provided-internal-cert</pre></div><p>
   These request templates contain the subject <code class="literal">Alt-names</code>
   that the certificates need. You can add to this template before generating
   your certificate signing request .
  </p><p>
   You would then send the CSR to your CA to be signed, and once you receive
   the certificate, place it in <code class="filename">config/tls/certs</code>.
  </p><p>
   When you bring in your own certificate, you may want to bring in the trust
   chains (or CA certificate) for this certificate. This is usually not
   required if the CA is a public signer that is typically bundled with the
   operating system. However, we suggest you include it anyway by copying the
   file into the directory <code class="filename">config/cacerts/</code>.
  </p></div><div class="sect1" id="idm139651559305808"><div class="titlepage"><div><div><h2 class="title" id="idm139651559305808"><span class="number">25.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">User-provided certificates and trust chains</span> <a title="Permalink" class="permalink" href="#idm139651559305808">#</a></h2></div></div></div><p>
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> generates its own internal certificates but is designed to allow
   you to bring in your own certificates for the VIPs. Here is the general
   process.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     You must have a server certificate and a CA certificate to go with it
     (unless the signer is a public CA and it's already bundled with most
     distributions).
    </p></li><li class="step "><p>
     You must decide the names of the server certificates and configure the
     <code class="literal">network_groups.yml</code> file in the input model such that
     each load balancer provider has at least one cert-name associated with it.
    </p></li><li class="step "><p>
     Run the configuration processor. Note that you may or may not have the
     certificate file at this point. The configuration processor would create
     certificate request file artifacts under
     <code class="literal">info/cert_reqs/</code> for each of the cert-name(s) in the
     <code class="literal">network_groups.yml</code> file. While there's no special
     reason to use the request file created for an external endpoint VIP
     certificate, it is important to use the request files created for internal
     certificates since the canonical names for the internal VIP can be many
     and service specific and each of these need to be in the Subject Alt Names
     attribute of the certificate.
    </p></li><li class="step "><p>
     Create a certificate signing request for this request file and send it to
     your internal CA or a public CA to get it certified and issued with a
     certificate. You will now have a server certificate and possibly a trust
     chain or CA certificate.
    </p></li><li class="step "><p>
     Next, upload it to the Cloud Lifecycle Manager. Server certificates should be added to
     <code class="filename">config/tls/certs</code> and CA certificates should be added
     to <code class="filename">config/tls/cacerts</code>. The file extension should be
     CRT file for the CA certificate to be processed by <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>. Detailed
     steps are next.
    </p></li></ol></div></div></div><div class="sect1" id="idm139651559292624"><div class="titlepage"><div><div><h2 class="title" id="idm139651559292624"><span class="number">25.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Edit the input model to include your certificate files</span> <a title="Permalink" class="permalink" href="#idm139651559292624">#</a></h2></div></div></div><p>
   Edit the load balancer configuration in
   <code class="literal">~/openstack/my_cloud/definition/data/network_groups.yml</code>:
  </p><div class="verbatim-wrap"><pre class="screen">load-balancers:
 - provider: ip-cluster
 name: lb
 tls-components:
 - default
 components:
 - nova-metadata
 roles:
 - internal
 - admin
 cert-file: example-internal-cert #&lt;&lt;&lt;---- Certificate name for the internal VIP

- provider: ip-cluster
 name: extlb
 external-name: myardana.test #&lt;&lt;&lt;--- Use just IP for the external VIP in this example
 tls-components:
 - default
 roles:
 - public
 cert-file: example-public-cert #&lt;&lt;&lt;---- Certificate name for the external VIP</pre></div><p>
   Commit your changes to the local git repository and run the configuration
   processor:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "changed VIP certificates"
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml</pre></div><p>
   Verify that certificate requests have been generated by the configuration
   processor for every certificate file configured in the
   <code class="literal">networks_groups.yml</code> file. In this example, there are two
   files, as shown from the list command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ls ~/openstack/my_cloud/info/cert_reqs
example-internal-cert
example-public-cert</pre></div></div><div class="sect1" id="sec.generate-certificate"><div class="titlepage"><div><div><h2 class="title" id="sec.generate-certificate"><span class="number">25.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Generate a self-signed CA</span> <a title="Permalink" class="permalink" href="#sec.generate-certificate">#</a></h2></div></div></div><div id="idm139651559283872" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    In a production setting you will not perform this step. You will use your
    company's CA or a valid public CA.
   </p></div><p>
   This section demonstrates to how you can create your own self-signed CA and
   then use this CA to sign server certificates. This CA can be your
   organization's IT internal CA that is self-signed and whose CA certificates
   are deployed on your organization's machines. This way the server
   certificate becomes legitimate.
  </p><div id="idm139651559282096" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    Please use a unique CN for your example Certificate Authority and do not
    install multiple CA certificates with the same CN into your cloud.
   </p></div><p>
   Copy the commands below to the command line and execute. This will cause the
   two files, <code class="literal">example-CA.key</code> and
   <code class="literal">example-CA.crt</code> to be created:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>export EXAMPLE_CA_KEY_FILE='example-CA.key'
<code class="prompt user">ardana &gt; </code>export EXAMPLE_CA_CERT_FILE='example-CA.crt'
<code class="prompt user">ardana &gt; </code>openssl req -x509 -batch -newkey rsa:2048 -nodes -out "${EXAMPLE_CA_CERT_FILE}" \
-keyout "${EXAMPLE_CA_KEY_FILE}" \
-subj "/C=UK/O=hp/CN=YourOwnUniqueCertAuthorityName" \
-days 365</pre></div><p>
   You can tweak the subj and days settings above to meet your needs, or to
   test. For instance, if you want to test what happens when a CA expires, you
   can set 'days' to a very low value. Grab the configuration
   processor-generated request file from <code class="literal">info/cert_reqs/</code>:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cat ~/openstack/my_cloud/info/cert_reqs/example-internal-cert</pre></div><p>
   Now, copy this file to your working directory and append a
   <code class="literal">.req</code> extension to it.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cp ~/openstack/my_cloud/info/cert_reqs/example-internal-cert \
example-internal-cert.req</pre></div><div class="example" id="sec.tls.private_metadata"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 25.1: </span><span class="name">Certificate request file </span><a title="Permalink" class="permalink" href="#sec.tls.private_metadata">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">[req]
distinguished_name = req_distinguished_name
req_extensions = v3_req
prompt = no

[ req_distinguished_name ]
CN = "openstack-vip"

[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names

[ alt_names ]
DNS.1 = "deployerincloud-ccp-c0-m1-mgmt"
DNS.2 = "deployerincloud-ccp-vip-CEI-API-mgmt"
DNS.3 = "deployerincloud-ccp-vip-CND-API-mgmt"
DNS.4 = "deployerincloud-ccp-vip-DES-API-mgmt"
DNS.5 = "deployerincloud-ccp-vip-FND-MDB-mgmt"
DNS.6 = "deployerincloud-ccp-vip-FND-RMQ-mgmt"
DNS.7 = "deployerincloud-ccp-vip-FND-VDB-mgmt"
DNS.8 = "deployerincloud-ccp-vip-FRE-API-mgmt"
DNS.9 = "deployerincloud-ccp-vip-GLA-API-mgmt"
DNS.10 = "deployerincloud-ccp-vip-GLA-REG-mgmt"
DNS.11 = "deployerincloud-ccp-vip-HEA-ACF-mgmt"
DNS.12 = "deployerincloud-ccp-vip-HEA-ACW-mgmt"
DNS.13 = "deployerincloud-ccp-vip-HEA-API-mgmt"
DNS.14 = "deployerincloud-ccp-vip-HUX-SVC-mgmt"
DNS.15 = "deployerincloud-ccp-vip-HZN-WEB-mgmt"
DNS.16 = "deployerincloud-ccp-vip-KEY-API-mgmt"
DNS.17 = "deployerincloud-ccp-vip-KEYMGR-API-mgmt"
DNS.18 = "deployerincloud-ccp-vip-LOG-API-mgmt"
DNS.19 = "deployerincloud-ccp-vip-LOG-SVR-mgmt"
DNS.20 = "deployerincloud-ccp-vip-MON-API-mgmt"
DNS.21 = "deployerincloud-ccp-vip-NEU-SVR-mgmt"
DNS.22 = "deployerincloud-ccp-vip-NOV-API-mgmt"
DNS.23 = "deployerincloud-ccp-vip-NOV-MTD-mgmt"
DNS.24 = "deployerincloud-ccp-vip-OCT-API-mgmt"
DNS.25 = "deployerincloud-ccp-vip-OPS-WEB-mgmt"
DNS.26 = "deployerincloud-ccp-vip-SHP-API-mgmt"
DNS.27 = "deployerincloud-ccp-vip-SWF-PRX-mgmt"
DNS.28 = "deployerincloud-ccp-vip-admin-CEI-API-mgmt"
DNS.29 = "deployerincloud-ccp-vip-admin-CND-API-mgmt"
DNS.30 = "deployerincloud-ccp-vip-admin-DES-API-mgmt"
DNS.31 = "deployerincloud-ccp-vip-admin-FND-MDB-mgmt"
DNS.32 = "deployerincloud-ccp-vip-admin-FRE-API-mgmt"
DNS.33 = "deployerincloud-ccp-vip-admin-GLA-API-mgmt"
DNS.34 = "deployerincloud-ccp-vip-admin-HEA-ACF-mgmt"
DNS.35 = "deployerincloud-ccp-vip-admin-HEA-ACW-mgmt"
DNS.36 = "deployerincloud-ccp-vip-admin-HEA-API-mgmt"
DNS.37 = "deployerincloud-ccp-vip-admin-HUX-SVC-mgmt"
DNS.38 = "deployerincloud-ccp-vip-admin-HZN-WEB-mgmt"
DNS.39 = "deployerincloud-ccp-vip-admin-KEY-API-mgmt"
DNS.40 = "deployerincloud-ccp-vip-admin-KEYMGR-API-mgmt"
DNS.41 = "deployerincloud-ccp-vip-admin-MON-API-mgmt"
DNS.42 = "deployerincloud-ccp-vip-admin-NEU-SVR-mgmt"
DNS.43 = "deployerincloud-ccp-vip-admin-NOV-API-mgmt"
DNS.44 = "deployerincloud-ccp-vip-admin-OPS-WEB-mgmt"
DNS.45 = "deployerincloud-ccp-vip-admin-SHP-API-mgmt"
DNS.46 = "deployerincloud-ccp-vip-admin-SWF-PRX-mgmt"
DNS.47 = "192.168.245.5"
IP.1 = "192.168.245.5"

=============end of certificate request file.</pre></div></div></div><div id="idm139651559269344" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    In the case of a public VIP certificate, please add all the FQDNs you want
    it to support Currently <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> does not add the hostname for the
    external-name specified in <code class="literal">network_groups.yml</code> to the
    certificate request file . However, you can add it to the certificate
    request file manually. Here we assume that <code class="literal">myardana.test</code>
    is your external-name. In that case you would add this line (to the
    certificate request file that is shown above in
    <a class="xref" href="#sec.tls.private_metadata" title="Certificate request file">Example 25.1, “Certificate request file”</a>):
   </p><div class="verbatim-wrap"><pre class="screen">DNS.48 = "myardana.test"</pre></div></div><div id="idm139651559264576" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    Any attempt to use IP addresses rather than FQDNs in certificates must use
    subject alternate name entries that list both the IP address (needed for
    Google) and DNS with an IP (needed for a Python bug workaround). Failure to
    create the certificates in this manner will cause future installations of
    Go-based tools (such as Cloud Foundry, Stackato and other PaaS components)
    to fail.
   </p></div></div><div class="sect1" id="idm139651559263120"><div class="titlepage"><div><div><h2 class="title" id="idm139651559263120"><span class="number">25.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Generate a certificate signing request</span> <a title="Permalink" class="permalink" href="#idm139651559263120">#</a></h2></div></div></div><p>
   Now that you have a CA and a certificate request file, it's time to generate
   a CSR.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>export EXAMPLE_SERVER_KEY_FILE='example-internal-cert.key'
<code class="prompt user">ardana &gt; </code>export EXAMPLE_SERVER_CSR_FILE='example-internal-cert.csr'
<code class="prompt user">ardana &gt; </code>export EXAMPLE_SERVER_REQ_FILE=example-internal-cert.req
<code class="prompt user">ardana &gt; </code>openssl req -newkey rsa:2048 -nodes -keyout "$EXAMPLE_SERVER_KEY_FILE" \
-out "$EXAMPLE_SERVER_CSR_FILE" -extensions v3_req -config "$EXAMPLE_SERVER_REQ_FILE"</pre></div><p>
   Note that in production you would usually send the generated
   <code class="literal">example-internal-cert.csr</code> file to your IT department. But
   in this example you are your own CA, so sign and generate a server
   certificate.
  </p></div><div class="sect1" id="idm139651559257920"><div class="titlepage"><div><div><h2 class="title" id="idm139651559257920"><span class="number">25.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Generate a server certificate</span> <a title="Permalink" class="permalink" href="#idm139651559257920">#</a></h2></div></div></div><div id="idm139651559257168" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    In a production setting you will not perform this step. You will send the
    CSR created in the previous section to your company CA or a to a valid
    public CA and have them sign and send you back the certificate.
   </p></div><p>
   This section demonstrates how you would use your own self-signed CA that
   your created earlier to sign and generate a server certificate. A server
   certificate is essentially a signed public key, the signer being a CA and
   trusted by a client. When you install this the signing CA's certificate
   (called CA certificate or trust chain) on the client machine, you are
   telling the client to trust this CA, and thereby implicitly trusting any
   server certificates that are signed by this CA, thus creating a trust
   anchor.
  </p><p>
   <span class="bold"><strong>CA configuration file</strong></span>
  </p><p>
   When the CA signs the certificate, it uses a configuration file that tells
   it to verify the CSR. Note that in a production scenario the CA takes care
   of this for you.
  </p><p>
   Create a file called <code class="literal">openssl.cnf</code> and add the following
   contents to it.
  </p><div class="verbatim-wrap"><pre class="screen"># Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
#...

# OpenSSL configuration file.
#

# Establish working directory.

dir = .

[ ca ]
default_ca = CA_default

[ CA_default ]
serial = $dir/serial
database = $dir/index.txt
new_certs_dir = $dir/
certificate = $dir/cacert.pem
private_key = $dir/cakey.pem
unique_subject = no
default_crl_days = 365
default_days = 365
default_md = md5
preserve = no
email_in_dn = no
nameopt = default_ca
certopt = default_ca
policy = policy_match
copy_extensions = copy


[ policy_match ]
countryName = optional
stateOrProvinceName = optional
organizationName = optional
organizationalUnitName = optional
commonName = supplied
emailAddress = optional

[ req ]
default_bits = 1024 # Size of keys
default_keyfile = key.pem # name of generated keys
default_md = md5 # message digest algorithm
string_mask = nombstr # permitted characters
distinguished_name = req_distinguished_name
req_extensions = v3_req
x509_extensions = v3_ca

[ req_distinguished_name ]
# Variable name Prompt string
#---------------------- ----------------------------------
0.organizationName = Organization Name (company)
organizationalUnitName = Organizational Unit Name (department, division)
emailAddress = Email Address
emailAddress_max = 40
localityName = Locality Name (city, district)
stateOrProvinceName = State or Province Name (full name)
countryName = Country Name (2 letter code)
countryName_min = 2
countryName_max = 2
commonName = Common Name (hostname, IP, or your name)
commonName_max = 64

# Default values for the above, for consistency and less typing.
# Variable name Value
#------------------------------ ------------------------------
0.organizationName_default = Exampleco PLC
localityName_default = Anytown
stateOrProvinceName_default = Anycounty
countryName_default = UK
commonName_default = my-CA

[ v3_ca ]
basicConstraints = CA:TRUE
subjectKeyIdentifier = hash
authorityKeyIdentifier = keyid:always,issuer:always
subjectAltName = @alt_names

[ v3_req ]
basicConstraints = CA:FALSE
subjectKeyIdentifier = hash

[ alt_names ]

######### end of openssl.cnf #########</pre></div><p>
   <span class="bold"><strong>Sign and create a server certificate</strong></span>
  </p><p>
   Now you can sign the server certificate with your CA. Copy the commands
   below to the command line and execute. This will cause the one file,
   example-internal-cert.crt, to be created:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>export EXAMPLE_SERVER_CERT_FILE='example-internal-cert.crt'
<code class="prompt user">ardana &gt; </code>export EXAMPLE_SERVER_CSR_FILE='example-internal-cert.csr'
<code class="prompt user">ardana &gt; </code>export EXAMPLE_CA_KEY_FILE='example-CA.key'
<code class="prompt user">ardana &gt; </code>export EXAMPLE_CA_CERT_FILE='example-CA.crt'
<code class="prompt user">ardana &gt; </code>touch index.txt
<code class="prompt user">ardana &gt; </code>openssl rand -hex -out serial 6
<code class="prompt user">ardana &gt; </code>openssl ca -batch -notext -md sha256 -in "$EXAMPLE_SERVER_CSR_FILE" \
-cert "$EXAMPLE_CA_CERT_FILE" \
-keyfile "$EXAMPLE_CA_KEY_FILE" \
-out "$EXAMPLE_SERVER_CERT_FILE" \
-config openssl.cnf -extensions v3_req</pre></div><p>
   Finally, concatenate both the server key and certificate in preparation for
   uploading to the Cloud Lifecycle Manager.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cat example-internal-cert.key example-internal-cert.crt &gt; example-internal-cert</pre></div><p>
   Note that you have only created the internal-cert in this example. Repeat
   the above sequence for example-public-cert. Make sure you use the
   appropriate certificate request generated by the configuration processor.
  </p></div><div class="sect1" id="sec.upload-toclm"><div class="titlepage"><div><div><h2 class="title" id="sec.upload-toclm"><span class="number">25.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Upload to the Cloud Lifecycle Manager</span> <a title="Permalink" class="permalink" href="#sec.upload-toclm">#</a></h2></div></div></div><p>
   The following two files created from the example run above will need to be
   uploaded to the Cloud Lifecycle Manager and copied into <code class="filename">config/tls</code>.
  </p><div class="itemizedlist " id="ul_zcc_v1c_5v"><ul class="itemizedlist"><li class="listitem "><p>
     example-internal-cert
    </p></li><li class="listitem "><p>
     example-CA.crt
    </p></li></ul></div><p>
   Once on the Cloud Lifecycle Manager, execute the following two copy commands to copy to their
   respective directories. Note if you had created an external cert, you can
   copy that in a similar manner, specifying its name using the copy command as
   well.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cp example-internal-cert ~/openstack/my_cloud/config/tls/certs/
<code class="prompt user">ardana &gt; </code>cp example-CA.crt ~/openstack/my_cloud/config/tls/cacerts/</pre></div><p>
   <span class="bold"><strong>Continue with the deployment</strong></span>
  </p><p>
   Next, log into the Cloud Lifecycle Manager node, and save and commit the changes to the local
   git repository:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "updated certificate and CA"</pre></div><p>
   Next, rerun the <code class="literal">config-processor-run</code> playbook, and run
   <code class="literal">ready-deployment.yml</code>:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div><p>
   If you receive any prompts, enter the required information.
  </p><div id="idm139651559227808" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    For automated installation (for example CI) you can specify the required
    passwords on the Ansible command line. For example, the command below will
    disable encryption by the configuration processor:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml -e encrypt="" -e rekey=""</pre></div></div><p>
   Run this series of runbooks to complete the deployment:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts tls-reconfigure.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts FND-CLU-stop.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts FND-CLU-start.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-stop.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts monasca-start.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml</pre></div></div><div class="sect1" id="idm139651559221264"><div class="titlepage"><div><div><h2 class="title" id="idm139651559221264"><span class="number">25.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring the cipher suite</span> <a title="Permalink" class="permalink" href="#idm139651559221264">#</a></h2></div></div></div><p>
   By default, the cipher suite is set to:
   <code class="literal">HIGH:!aNULL:!eNULL:!DES:!3DES</code>. This setting is
   recommended in the
   <a class="link" href="http://docs.openstack.org/security-guide/secure-communication/introduction-to-ssl-and-tls.html" target="_blank">OpenStack
   documentation site</a>. You may override this. To do so, open
   <code class="filename">config/haproxy/defaults.yml</code> and edit it. The parameters
   can be found under the <code class="literal">haproxy_globals</code> list.
  </p><div class="verbatim-wrap"><pre class="screen">- "ssl-default-bind-ciphers HIGH:!aNULL:!eNULL:!DES:!3DES"
- "ssl-default-server-ciphers HIGH:!aNULL:!eNULL:!DES:!3DES"</pre></div><p>
   Make the changes as needed. It's best to keep the two options identical.
  </p></div><div class="sect1" id="idm139651559216528"><div class="titlepage"><div><div><h2 class="title" id="idm139651559216528"><span class="number">25.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Testing</span> <a title="Permalink" class="permalink" href="#idm139651559216528">#</a></h2></div></div></div><p>
   You can easily determine if an endpoint is behind TLS. To do so, run the
   following command, which probes a Keystone identity service endpoint that's
   behind TLS:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>echo | openssl s_client -connect 192.168.245.5:5000 | openssl x509 -fingerprint -noout
depth=0 CN = openstack-vip
verify error:num=20:unable to get local issuer certificate
verify return:1
depth=0 CN = openstack-vip
verify error:num=27:certificate not trusted
verify return:1
depth=0 CN = openstack-vip
verify error:num=21:unable to verify the first certificate
verify return:1
DONE
SHA1 Fingerprint=C6:46:1E:59:C6:11:BF:72:5E:DD:FC:FF:B0:66:A7:A2:CC:32:1C:B8</pre></div><p>
   The next command probes a MariaDB endpoint that is not behind TLS:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>echo | openssl s_client -connect 192.168.245.5:3306 | openssl x509 -fingerprint -noout
140448358213264:error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown protocol:s23_clnt.c:795:
unable to load certificate
140454148159120:error:0906D06C:PEM routines:PEM_read_bio:no start line:pem_lib.c:703
:Expecting: TRUSTED CERTIFICATE</pre></div></div><div class="sect1" id="idm139651559211632"><div class="titlepage"><div><div><h2 class="title" id="idm139651559211632"><span class="number">25.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Verifying that the trust chain is correctly deployed</span> <a title="Permalink" class="permalink" href="#idm139651559211632">#</a></h2></div></div></div><p>
   You can determine if the trust chain is correctly deployed by running the
   following commands:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>echo | openssl s_client -connect 192.168.245.9:5000 2&gt;/dev/null | grep code
Verify return code: 21 (unable to verify the first certificate)
echo | openssl s_client -connect 192.168.245.9:5000 \
-CAfile /etc/pki/trust/anchors/ca-certificates/openstack_frontend_cacert.crt 2&gt;/dev/null | grep code
Verify return code: 0 (ok)</pre></div><p>
   Here, the first command produces error 21, which is then fixed by providing
   the CA certificate file. This verifies that the CA certificate matches the
   server certificate.
  </p></div><div class="sect1" id="idm139651559208208"><div class="titlepage"><div><div><h2 class="title" id="idm139651559208208"><span class="number">25.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Turning TLS on or off</span> <a title="Permalink" class="permalink" href="#idm139651559208208">#</a></h2></div></div></div><p>
   You should leave TLS enabled in production. However, if you need to disable
   it for any reason, you must change "tls-components" to "components" in
   <code class="literal">network_groups.yml</code> (as shown earlier) and comment out the
   cert-file. Additionally, if you have a <code class="literal">network_groups.yml</code>
   file from a previous installation, you won't have TLS enabled unless you
   change "components" to "tls-components" in that file. By default, Horizon is
   configured with TLS in the input model. Note that you should not disable TLS
   in the input model for Horizon as that is a public endpoint and is required.
   Additionally, you should keep all services behind TLS, but using the input
   model file <code class="literal">network_groups.yml</code> you may turn TLS off for a
   service for troubleshooting or debugging. TLS should always be enabled for
   production environments.
  </p><p>
   If you are using an example input model on a clean install, all supported
   TLS services will be enabled before deployment of your cloud. If you want to
   change this setting later, for example, when upgrading, you can change the
   input model and reconfigure the system. The process is as follows:
  </p><p>
   Edit the input model <code class="literal">network_groups.yml</code> file
   appropriately, as described above. Then, commit the changes to the git
   repository:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/openstack/ardana/ansible/
<code class="prompt user">ardana &gt; </code>git add -A
<code class="prompt user">ardana &gt; </code>git commit -m "TLS change"</pre></div><p>
   Change directories again and run the configuration processor and ready
   deployment playbooks:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost config-processor-run.yml
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/localhost ready-deployment.yml</pre></div><p>
   Change directories again and run the reconfigure playbook:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts ardana-reconfigure.yml</pre></div></div></div><div class="chapter " id="config_availability_zones"><div class="titlepage"><div><div><h2 class="title"><span class="number">26 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Availability Zones</span> <a title="Permalink" class="permalink" href="#config_availability_zones">#</a></h2></div></div></div><div class="line"></div><p>
  The Cloud Lifecycle Manager only creates a default availability zone during
  installation. If your system has multiple failure/availability zones defined
  in your input model, these zones will not get created automatically.
 </p><p>
  Once the installation has finished, you can run the
  <code class="literal">nova-cloud-configure.yml</code> playbook to configure
  availability zones and assign compute nodes to those zones based on the
  configuration specified in the model.
 </p><p>
  You can run the playbook <code class="literal">nova-cloud-configure.yml</code> any time
  you make changes to the configuration of availability zones in your input
  model. Alternatively, you can use Horizon or the command line to perform the
  configuration.
 </p><p>
  
  For more details, see the OpenStack Availability Zone documentation at
 <a class="link" href="https://docs.openstack.org/nova/pike/user/aggregates.html" target="_blank">https://docs.openstack.org/nova/pike/user/aggregates.html</a>.
 </p></div><div class="chapter " id="OctaviaInstall"><div class="titlepage"><div><div><h2 class="title"><span class="number">27 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Load Balancer as a Service</span> <a title="Permalink" class="permalink" href="#OctaviaInstall">#</a></h2></div><div><div class="abstract"><div class="abstract-title-wrap"><h6 class="abstract-title">Abstract<a title="Permalink" class="permalink" href="#idm139651559188464">#</a></h6></div><p>
    The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Neutron LBaaS service supports several load balancing
    providers. By default, both Octavia and the namespace HAProxy driver are
    configured to be used. We describe this in more detail here.
   </p></div></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#idm139651559174752"><span class="number">27.1 </span><span class="name">Prerequisites</span></a></span></dt><dt><span class="section"><a href="#idm139651559171152"><span class="number">27.2 </span><span class="name">Octavia Load Balancing Provider</span></a></span></dt><dt><span class="section"><a href="#idm139651559154416"><span class="number">27.3 </span><span class="name">Setup of prerequisites</span></a></span></dt><dt><span class="section"><a href="#idm139651559055216"><span class="number">27.4 </span><span class="name">Create Load Balancers</span></a></span></dt><dt><span class="section"><a href="#idm139651559026112"><span class="number">27.5 </span><span class="name">Create Floating IPs for Load Balancer</span></a></span></dt><dt><span class="section"><a href="#idm139651559017792"><span class="number">27.6 </span><span class="name">Testing the Octavia Load Balancer</span></a></span></dt></dl></div></div><p>
  The <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> Neutron LBaaS service supports several load balancing
  providers. By default, both Octavia and the namespace HAProxy driver are
  configured to be used. A user can specify which provider to use with the
  <code class="option">--provider</code> flag upon load balancer creation.
 </p><p>
  Example:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron lbaas-loadbalancer-create --name <em class="replaceable ">NAME</em> --provider \
  [octavia|haproxy] <em class="replaceable ">SUBNET</em></pre></div><p>
  If you do not specify the <code class="literal">--provider</code> option it will
  default to Octavia. The Octavia driver provides more functionality than the
  HAProxy namespace driver which is deprecated. The HAProxy namespace driver
  will be retired in a future version of <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>.
 </p><p>
  There are additional drivers for third-party hardware load balancers. Please
  refer to the vendor directly. The <code class="literal">neutron
  service-provider-list</code> command displays not only the currently
  installed load balancer drivers but also other installed services such as
  VPN. You can see a list of available services as follows:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron service-provider-list
+----------------+----------+---------+
| service_type   | name     | default |
+----------------+----------+---------+
| LOADBALANCERV2 | octavia  | True    |
| VPN            | openswan | True    |
| LOADBALANCERV2 | haproxy  | False   |
| LOADBALANCERV2 | octavia  | True    |
| VPN            | openswan | True    |
| LOADBALANCERV2 | haproxy  | False   |
+----------------+----------+---------+</pre></div><div id="idm139651559175696" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
   The Octavia load balancer provider is listed as the default.
  </p></div><div class="sect1" id="idm139651559174752"><div class="titlepage"><div><div><h2 class="title" id="idm139651559174752"><span class="number">27.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prerequisites</span> <a title="Permalink" class="permalink" href="#idm139651559174752">#</a></h2></div></div></div><p>
   You will need to create an external network and create an image to test
   LBaaS functionality. If you have already created an external network and
   registered an image, this step can be skipped.
  </p><p>
   Creating an external network: <a class="xref" href="#create_extnet" title="23.4. Creating an External Network">Section 23.4, “Creating an External Network”</a>.
  </p><p>
   Creating and uploading a Glance image: <span class="intraxref">Book “User Guide Overview”, Chapter 10 “Creating and Uploading a Glance Image”</span>.
  </p></div><div class="sect1" id="idm139651559171152"><div class="titlepage"><div><div><h2 class="title" id="idm139651559171152"><span class="number">27.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Octavia Load Balancing Provider</span> <a title="Permalink" class="permalink" href="#idm139651559171152">#</a></h2></div></div></div><p>
   The Octavia Load balancing provider bundled with <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> is an
   operator grade load balancer for <span class="productname">OpenStack</span>. It is based on the
   <span class="productname">OpenStack</span> Pike version of Octavia. It differs from the namespace driver by
   starting a new Nova virtual machine to house the HAProxy load balancer
   software, called an <span class="emphasis"><em>amphora</em></span>, that provides the load
   balancer function. A virtual machine for each load balancer requested
   provides a better separation of load balancers between tenants and makes it
   easier to grow load balancing capacity alongside compute node growth.
   Additionally, if the virtual machine fails for any reason Octavia will
   replace it with a replacement VM from a pool of spare VMs, assuming that the
   feature is configured.
  </p><div id="idm139651559165008" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    The Health Monitor will not create or replace failed amphorae. If the pool
    of spare VMs is exhausted there will be no additional virtual machines to
    handle load balancing requests.
   </p></div><p>
   Octavia uses two-way SSL encryption to communicate with the amphora. There
   are demo Certificate Authority (CA) certificates included with
   <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> in
   <code class="filename">~/scratch/ansible/next/ardana/ansible/roles/octavia-common/files</code>
   on the Cloud Lifecycle Manager. For additional security in production deployments, all
   certificate authorities should be replaced with ones you generated yourself
   by running the following commands:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>openssl genrsa -passout pass:foobar -des3 -out cakey.pem 2048
<code class="prompt user">ardana &gt; </code>openssl req -x509 -passin pass:foobar -new -nodes -key cakey.pem -out ca_01.pem
<code class="prompt user">ardana &gt; </code>openssl genrsa -passout pass:foobar -des3 -out servercakey.pem 2048
<code class="prompt user">ardana &gt; </code>openssl req -x509 -passin pass:foobar -new -nodes -key cakey.pem -out serverca_01.pem</pre></div><p>
   For more details refer to the
   <a class="link" href="https://www.openssl.org/docs/manmaster/apps/openssl.html" target="_blank">openssl
   man page</a>.
  </p><div id="idm139651559156400" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    If you change the certificate authority and have amphora running with an
    old CA you won’t be able to control the amphora. The amphora's will need
    to be failed over so they can utilize the new certificate. If you change
    the CA password for the server certificate you need to change that in the
    Octavia configuration files as well. For more information, see
    <span class="intraxref">Book “Operations Guide”, Chapter 9 “Managing Networking”, Section 9.3 “Networking Service Overview”, Section 9.3.9 “Load Balancer: Octavia Driver Administration”, Section 9.3.9.2 “Tuning Octavia Installation”</span>.
   </p></div></div><div class="sect1" id="idm139651559154416"><div class="titlepage"><div><div><h2 class="title" id="idm139651559154416"><span class="number">27.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Setup of prerequisites</span> <a title="Permalink" class="permalink" href="#idm139651559154416">#</a></h2></div></div></div><p>
   <span class="bold"><strong>Octavia Network and Management Network
   Ports</strong></span>
  </p><p>
   The Octavia management network and Management network must have access to
   each other. If you have a configured firewall between the Octavia management
   network and Management network, you must open up the following ports to
   allow network traffic between the networks.
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     From Management network to Octavia network
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       TCP 9443 (amphora API)
      </p></li></ul></div></li><li class="listitem "><p>
     From Octavia network to Management network
    </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
       TCP 9876 (Octavia API)
      </p></li><li class="listitem "><p>
       UDP 5555 (Octavia Health Manager)
      </p></li></ul></div></li></ul></div><p>
   <span class="bold"><strong>Installing the Amphora Image</strong></span>
  </p><p>
   Octavia uses Nova VMs for its load balancing function and <span class="phrase"><span class="phrase">SUSE</span></span>
   provides images used to boot those VMs called
   <code class="literal">octavia-amphora</code>.
  </p><div id="idm139651559142800" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>
    Without these images the Octavia load balancer will not work.
   </p></div><p>
   <span class="bold"><strong>Register the image.</strong></span> The <span class="productname">OpenStack</span> load
   balancing service (Octavia) does not automatically register the Amphora
   guest image.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     The full path and name for the Amphora image is
     <code class="filename">/srv/www/suse-12.3/x86_64/repos/Cloud/suse/noarch/openstack-octavia-amphora-image.rpm</code>
    </p><p>
     Switch to the ansible directory and register the image by giving the full
     path and name for the Amphora image as an argument to service_package:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>cd ~/scratch/ansible/next/ardana/ansible/
<code class="prompt user">ardana &gt; </code>ansible-playbook -i hosts/verb_hosts service-guest-image.yml \
-e service_package=\
/srv/www/suse-12.3/x86_64/repos/Cloud/suse/noarch/openstack-octavia-amphora-image.rpm</pre></div></li><li class="step "><p>
     Source the service user (this can be done on a different computer)
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>source service.osrc</pre></div></li><li class="step "><p>
     Verify that the image was registered (this can be done on a computer with
     access to the Glance CLI client)
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack image-list
+--------------------------------------+------------------------+---------
| ID                                   | Name                   | Status |
+--------------------------------------+------------------------+--------+
...
| 1d4dd309-8670-46b6-801d-3d6af849b6a9 | octavia-amphora-x86_64 | active |
...</pre></div><div id="idm139651559132256" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>
      In the example above, the status of the
      <code class="literal">octavia-amphora-x86_64</code> image is
      <span class="emphasis"><em>active</em></span> which means the image was successfully
      registered. If a status of the images is <span class="emphasis"><em>queued</em></span>, you
      need to run the image registration again.
     </p><p>
      If you run the registration by accident, the system will only upload a
      new image if the underlying image has been changed.
     </p></div><p>
     Please be aware that if you have already created load balancers they will
     not receive the new image. Only load balancers created after the image has
     been successfully installed will use the new image. If existing load
     balancers need to be switched to the new image please follow the
     instructions in <span class="intraxref">Book “Operations Guide”, Chapter 9 “Managing Networking”, Section 9.3 “Networking Service Overview”, Section 9.3.9 “Load Balancer: Octavia Driver Administration”, Section 9.3.9.2 “Tuning Octavia Installation”</span>.
    </p></li></ol></div></div><p>
   <span class="bold"><strong>Setup network, subnet, router, security and
   IP's</strong></span>
  </p><p>
   If you have already created a network, subnet, router, security settings and
   IPs you can skip the following steps and go directly to creating the load
   balancers.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Create a network.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron network create lb_net1
+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | True                                 |
| id                        | 71a1ac88-30a3-48a3-a18b-d98509fbef5c |
| mtu                       | 0                                    |
| name                      | lb_net1                              |
| provider:network_type     | vxlan                                |
| provider:physical_network |                                      |
| provider:segmentation_id  | 1061                                 |
| router:external           | False                                |
| shared                    | False                                |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tenant_id                 | 4b31d0508f83437e83d8f4d520cda22f     |
+---------------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     Create a subnet.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron subnet create --name lb_subnet1 lb_net1 10.247.94.128/26 \
  --gateway 10.247.94.129
+-------------------+----------------------------------------------------+
| Field             | Value                                              |
+-------------------+----------------------------------------------------+
| allocation_pools  | {"start": "10.247.94.130", "end": "10.247.94.190"} |
| cidr              | 10.247.94.128/26                                   |
| dns_nameservers   |                                                    |
| enable_dhcp       | True                                               |
| gateway_ip        | 10.247.94.129                                      |
| host_routes       |                                                    |
| id                | 6fc2572c-53b3-41d0-ab63-342d9515f514               |
| ip_version        | 4                                                  |
| ipv6_address_mode |                                                    |
| ipv6_ra_mode      |                                                    |
| name              | lb_subnet1                                         |
| network_id        | 71a1ac88-30a3-48a3-a18b-d98509fbef5c               |
| subnetpool_id     |                                                    |
| tenant_id         | 4b31d0508f83437e83d8f4d520cda22f                   |
+-------------------+----------------------------------------------------+</pre></div></li><li class="step "><p>
     Create a router.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron router create --distributed False lb_router1
+-----------------------+--------------------------------------+
| Field                 | Value                                |
+-----------------------+--------------------------------------+
| admin_state_up        | True                                 |
| distributed           | False                                |
| external_gateway_info |                                      |
| ha                    | False                                |
| id                    | 6aafc9a9-93f6-4d7e-94f2-3068b034b823 |
| name                  | lb_router1                           |
| routes                |                                      |
| status                | ACTIVE                               |
| tenant_id             | 4b31d0508f83437e83d8f4d520cda22f     |
+-----------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     Add interface to router. In the following example, the interface
     <code class="literal">426c5898-f851-4f49-b01f-7a6fe490410c</code> will be added to
     the router <code class="literal">lb_router1</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron router add subnet lb_router1 lb_subnet1</pre></div></li><li class="step "><p>
     Set gateway for router.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron router set lb_router1 ext-net</pre></div></li><li class="step "><p>
     Check networks.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron network list
+-----------------------------+------------------+-----------------------------+
| id                          | name             | subnets                     |
+-----------------------------+------------------+-----------------------------+
| d3cb12a6-a000-4e3e-         | ext-net          | f4152001-2500-4ebe-ba9d-    |
| 82c4-ee04aa169291           |                  | a8d6149a50df 10.247.96.0/23 |
| 8306282a-3627-445a-a588-c18 | OCTAVIA-MGMT-NET | f00299f8-3403-45ae-ac4b-    |
| 8b6a13163                   |                  | 58af41d57bdc                |
|                             |                  | 10.247.94.128/26            |
| 71a1ac88-30a3-48a3-a18b-    | lb_net1          | 6fc2572c-                   |
| d98509fbef5c                |                  | 53b3-41d0-ab63-342d9515f514 |
|                             |                  | 10.247.94.128/26            |
+-----------------------------+------------------+-----------------------------+</pre></div></li><li class="step "><p>
     Create security group.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron security group create lb_secgroup1
+----------------+---------------------------------------------------------------------------+
| Field          | Value                                                                     |
+----------------+---------------------------------------------------------------------------+
| description    |                                                                           |
| id             | 75343a54-83c3-464c-8773-802598afaee9                                      |
| name           | lb_secgroup1                                                              |
| security group | {"remote_group_id": null, "direction": "egress", "remote_ip_prefix": null,|
|    rules       | "protocol": null, "tenant_id": "4b31d...da22f", "port_range_max": null,   |
|                | "security_group_id": "75343a54-83c3-464c-8773-802598afaee9",              |
|                | "port_range_min": null, "ethertype": "IPv4", "id": "20ae3...97a7a"}       |
|                | {"remote_group_id": null, "direction": "egress",                          |
|                | "remote_ip_prefix": null, "protocol": null, "tenant_id": "4b31...a22f",   |
|                | "port_range_max": null, "security_group_id": "7534...98afaee9",           |
|                | "port_range_min": null, "ethertype": "IPv6", "id": "563c5c...aaef9"}      |
| tenant_id      | 4b31d0508f83437e83d8f4d520cda22f                                          |
+----------------+---------------------------------------------------------------------------+</pre></div></li><li class="step "><p>
     Create icmp security group rule.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron security group rule create lb_secgroup1 --protocol icmp
+-------------------+--------------------------------------+
| Field             | Value                                |
+-------------------+--------------------------------------+
| direction         | ingress                              |
| ethertype         | IPv4                                 |
| id                | 16d74150-a5b2-4cf6-82eb-a6c49a972d93 |
| port_range_max    |                                      |
| port_range_min    |                                      |
| protocol          | icmp                                 |
| remote_group_id   |                                      |
| remote_ip_prefix  |                                      |
| security_group_id | 75343a54-83c3-464c-8773-802598afaee9 |
| tenant_id         | 4b31d0508f83437e83d8f4d520cda22f     |
+-------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     Create TCP port 22 rule.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron security group rule create lb_secgroup1 --protocol tcp \
  --port-range-min 22 --port-range-max 22
+-------------------+--------------------------------------+
| Field             | Value                                |
+-------------------+--------------------------------------+
| direction         | ingress                              |
| ethertype         | IPv4                                 |
| id                | 472d3c8f-c50f-4ad2-97a1-148778e73af5 |
| port_range_max    | 22                                   |
| port_range_min    | 22                                   |
| protocol          | tcp                                  |
| remote_group_id   |                                      |
| remote_ip_prefix  |                                      |
| security_group_id | 75343a54-83c3-464c-8773-802598afaee9 |
| tenant_id         | 4b31d0508f83437e83d8f4d520cda22f     |
+-------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     Create TCP port 80 rule.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron security group rule create lb_secgroup1 --protocol tcp \
  --port-range-min 80 --port-range-max 80
+-------------------+--------------------------------------+
| Field             | Value                                |
+-------------------+--------------------------------------+
| direction         | ingress                              |
| ethertype         | IPv4                                 |
| id                | 10a76cad-8b1c-46f6-90e8-5dddd279e5f7 |
| port_range_max    | 80                                   |
| port_range_min    | 80                                   |
| protocol          | tcp                                  |
| remote_group_id   |                                      |
| remote_ip_prefix  |                                      |
| security_group_id | 75343a54-83c3-464c-8773-802598afaee9 |
| tenant_id         | 4b31d0508f83437e83d8f4d520cda22f     |
+-------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     If you have not already created a keypair, create one now with
     <code class="literal">nova keypair create</code>. You will use the keypair to boot
     images.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>nova keypair create lb_kp1 &gt; lb_kp1.pem

chmod 400 lb_kp1.pem

cat lb_kp1.pem
-----BEGIN RSA PRIVATE KEY-----
MIIEqAIBAAKCAQEAkbW5W/XWGRGC0LAJI7lttR7EdDfiTDeFJ7A9b9Cff+OMXjhx
WL26eKIr+jp8DR64YjV2mNnQLsDyCxekFpkyjnGRId3KVAeV5sRQqXgtaCXI+Rvd
IyUtd8p1cp3DRgTd1dxO0oL6bBmwrZatNrrRn4HgKc2c7ErekeXrwLHyE0Pia/pz
C6qs0coRdfIeXxsmS3kXExP0YfsswRS/OyDl8QhRAF0ZW/zV+DQIi8+HpLZT+RW1
8sTTYZ6b0kXoH9wLER4IUBj1I1IyrYdxlAhe2VIn+tF0Ec4nDBn1py9iwEfGmn0+
N2jHCJAkrK/QhWdXO4O8zeXfL4mCZ9FybW4nzQIDAQABAoIBACe0PvgB+v8FuIGp
FjR32J8b7ShF+hIOpufzrCoFzRCKLruV4bzuphstBZK/0QG6Nz/7lX99Cq9SwCGp
pXrK7+3EoGl8CB/xmTUylVA4gRb6BNNsdkuXW9ZigrJirs0rkk8uIwRV0GsYbP5A
Kp7ZNTmjqDN75aC1ngRfhGgTlQUOdxBH+4xSb7GukekD13V8V5MF1Qft089asdWp
l/TpvhYeW9O92xEnZ3qXQYpXYQgEFQoM2PKa3VW7FGLgfw9gdS/MSqpHuHGyKmjl
uT6upUX+Lofbe7V+9kfxuV32sLL/S5YFvkBy2q8VpuEV1sXI7O7Sc411WX4cqmlb
YoFwhrkCggCBALkYE7OMTtdCAGcMotJhTiiS5l8d4U/fn1x0zus43XV5Y7wCnMuU
r5vCoK+a+TR9Ekzc/GjccAx7Wz/YYKp6G8FXW114dLcADXZjqjIlX7ifUud4sLCS
y+x3KAJa7LqyzH53I6FOts9RaB5xx4gZ2WjcJquCTbATZWj7j1yGeNgvAoIAgQDJ
h0r0Te5IliYbCRg+ES9YRZzH/PSLuIn00bbLvpOPNEoKe2Pxs+KI8Fqp6ZIDAB3c
4EPOK5QrJvAny9Z58ZArrNZi15t84KEVAkWUATl+c4SmHc8sW/atgmUoqIzgDQXe
AlwadHLY7JCdg7EYDuUxuTKLLOdqfpf6fKkhNxtEwwKCAIAMxi+d5aIPUxvKAOI/
2L1XKYRCrkI9i/ZooBsjusH1+JG8iQWfOzy/aDhExlJKoBMiQOIerpABHIZYqqtJ
OLIvrsK8ebK8aoGDWS+G1HN9v2kuVnMDTK5MPJEDUJkj7XEVjU1lNZSCTGD+MOYP
a5FInmEA1zZbX4tRKoNjZFh0uwKCAIEAiLs7drAdOLBu4C72fL4KIljwu5t7jATD
zRAwduIxmZq/lYcMU2RaEdEJonivsUt193NNbeeRWwnLLSUWupvT1l4pAt0ISNzb
TbbB4F5IVOwpls9ozc8DecubuM9K7YTIc02kkepqNZWjtMsx74HDrU3a5iSsSkvj
73Z/BeMupCMCggCAS48BsrcsDsHSHE3tO4D8pAIr1r+6WPaQn49pT3GIrdQNc7aO
d9PfXmPoe/PxUlqaXoNAvT99+nNEadp+GTId21VM0Y28pn3EkIGE1Cqoeyl3BEO8
f9SUiRNruDnH4F4OclsDBlmqWXImuXRfeiDHxM8X03UDZoqyHmGD3RqA53I=
-----END RSA PRIVATE KEY-----</pre></div></li><li class="step "><p>
     Check and boot images.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>nova image list
+--------------+------------------------+--------+--------+
| ID           | Name                   | Status | Server |
+--------------+------------------------+--------+--------+
| 0526d...7f39 | cirros-0.4.0-x86_64    | ACTIVE |        |
| 8aa51...8f2f | octavia-amphora-x86_64 | ACTIVE |        |
+--------------+------------------------+--------+--------+</pre></div><p>
     Boot first VM.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>nova server create --flavor 1 --image 04b1528b-b1e2-45d4-96d1-fbe04c6b2efd --key-name lb_kp1 \
  --security-groups lb_secgroup1 --nic net-id=71a1ac88-30a3-48a3-a18b-d98509fbef5c \
  lb_vm1 --poll
+--------------------------------------+--------------------------------------+
| Property                             | Value                                |
+--------------------------------------+--------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                               |
| OS-EXT-AZ:availability_zone          |                                      |
| OS-EXT-SRV-ATTR:host                 | -                                    |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | -                                    |
| OS-EXT-SRV-ATTR:instance_name        | instance-00000031                    |
| OS-EXT-STS:power_state               | 0                                    |
| OS-EXT-STS:task_state                | scheduling                           |
| OS-EXT-STS:vm_state                  | building                             |
| OS-SRV-USG:launched_at               | -                                    |
| OS-SRV-USG:terminated_at             | -                                    |
| accessIPv4                           |                                      |
| accessIPv6                           |                                      |
| adminPass                            | NeVvhP5E8iCy                         |
| config_drive                         |                                      |
| created                              | 2016-06-15T16:53:00Z                 |
| flavor                               | m1.tiny (1)                          |
| hostId                               |                                      |
| id                                   | dfdfe15b-ce8d-469c-a9d8-2cea0e7ca287 |
| image                                | cirros-0.4.0-x86_64 (0526d...7f39)   |
| key_name                             | lb_kp1                               |
| metadata                             | {}                                   |
| name                                 | lb_vm1                               |
| os-extended-volumes:volumes_attached | []                                   |
| progress                             | 0                                    |
| security_groups                      | lb_secgroup1                         |
| status                               | BUILD                                |
| tenant_id                            | 4b31d0508f83437e83d8f4d520cda22f     |
| updated                              | 2016-06-15T16:53:00Z                 |
| user_id                              | fd471475faa84680b97f18e55847ec0a     |
+--------------------------------------+--------------------------------------+

            Server building... 100% complete
            Finished</pre></div><p>
     Boot second VM.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>nova server create --flavor 1 --image 04b1528b-b1e2-45d4-96d1-fbe04c6b2efd --key-name lb_kp1 \
  --security-groups lb_secgroup1 --nic net-id=71a1ac88-30a3-48a3-a18b-d98509fbef5c \
  lb_vm2 --poll
+--------------------------------------+---------------------------------------+
| Property                             | Value                                 |
+--------------------------------------+---------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                                |
| OS-EXT-AZ:availability_zone          |                                       |
| OS-EXT-SRV-ATTR:host                 | -                                     |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | -                                     |
| OS-EXT-SRV-ATTR:instance_name        | instance-00000034                     |
| OS-EXT-STS:power_state               | 0                                     |
| OS-EXT-STS:task_state                | scheduling                            |
| OS-EXT-STS:vm_state                  | building                              |
| OS-SRV-USG:launched_at               | -                                     |
| OS-SRV-USG:terminated_at             | -                                     |
| accessIPv4                           |                                       |
| accessIPv6                           |                                       |
| adminPass                            | 3nFXjNrTrmNm                          |
| config_drive                         |                                       |
| created                              | 2016-06-15T16:55:10Z                  |
| flavor                               | m1.tiny (1)                           |
| hostId                               |                                       |
| id                                   | 3844bb10-2c61-4327-a0d4-0c043c674344  |
| image                                | cirros-0.4.0-x86_64 (0526d...7f39)    |
| key_name                             | lb_kp1                                |
| metadata                             | {}                                    |
| name                                 | lb_vm2                                |
| os-extended-volumes:volumes_attached | []                                    |
| progress                             | 0                                     |
| security_groups                      | lb_secgroup1                          |
| status                               | BUILD                                 |
| tenant_id                            | 4b31d0508f83437e83d8f4d520cda22f      |
| updated                              | 2016-06-15T16:55:09Z                  |
| user_id                              | fd471475faa84680b97f18e55847ec0a      |
+--------------------------------------+---------------------------------------+

            Server building... 100% complete
            Finished</pre></div></li><li class="step "><p>
     List the running VM with <code class="literal">nova list</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>nova server list
+----------------+--------+--------+------------+-------------+-----------------------+
| ID             | Name   | Status | Task State | Power State | Networks              |
+----------------+--------+--------+------------+-------------+-----------------------+
| dfdfe...7ca287 | lb_vm1 | ACTIVE | -          | Running     | lb_net1=10.247.94.132 |
| 3844b...674344 | lb_vm2 | ACTIVE | -          | Running     | lb_net1=10.247.94.133 |
+----------------+--------+--------+------------+-------------+-----------------------+</pre></div></li><li class="step "><p>
     Check ports.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron port list
+----------------+------+-------------------+--------------------------------+
| id             | name | mac_address       | fixed_ips                      |
+----------------+------+-------------------+--------------------------------+
...
| 7e5e0...36450e |      | fa:16:3e:66:fd:2e | {"subnet_id": "6fc25...5f514", |
|                |      |                   | "ip_address": "10.247.94.132"} |
| ca95c...b36854 |      | fa:16:3e:e0:37:c4 | {"subnet_id": "6fc25...5f514", |
|                |      |                   | "ip_address": "10.247.94.133"} |
+----------------+------+-------------------+--------------------------------+</pre></div></li><li class="step "><p>
     Create the first floating IP.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron floating ip create ext-net --port-id 7e5e0038-88cf-4f97-a366-b58cd836450e
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| fixed_ip_address    | 10.247.94.132                        |
| floating_ip_address | 10.247.96.26                         |
| floating_network_id | d3cb12a6-a000-4e3e-82c4-ee04aa169291 |
| id                  | 3ce608bf-8835-4638-871d-0efe8ebf55ef |
| port_id             | 7e5e0038-88cf-4f97-a366-b58cd836450e |
| router_id           | 6aafc9a9-93f6-4d7e-94f2-3068b034b823 |
| status              | DOWN                                 |
| tenant_id           | 4b31d0508f83437e83d8f4d520cda22f     |
+---------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     Create the second floating IP.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron floating ip create ext-net --port-id ca95cc24-4e8f-4415-9156-7b519eb36854
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| fixed_ip_address    | 10.247.94.133                        |
| floating_ip_address | 10.247.96.27                         |
| floating_network_id | d3cb12a6-a000-4e3e-82c4-ee04aa169291 |
| id                  | 680c0375-a179-47cb-a8c5-02b836247444 |
| port_id             | ca95cc24-4e8f-4415-9156-7b519eb36854 |
| router_id           | 6aafc9a9-93f6-4d7e-94f2-3068b034b823 |
| status              | DOWN                                 |
| tenant_id           | 4b31d0508f83437e83d8f4d520cda22f     |
+---------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     List the floating IP's.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron floating ip list
+----------------+------------------+---------------------+---------------+
| id             | fixed_ip_address | floating_ip_address | port_id       |
+----------------+------------------+---------------------+---------------+
| 3ce60...bf55ef | 10.247.94.132    | 10.247.96.26        | 7e5e0...6450e |
| 680c0...247444 | 10.247.94.133    | 10.247.96.27        | ca95c...36854 |
+----------------+------------------+---------------------+---------------+</pre></div></li><li class="step "><p>
     Show first Floating IP.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron floating ip show 3ce608bf-8835-4638-871d-0efe8ebf55ef
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| fixed_ip_address    | 10.247.94.132                        |
| floating_ip_address | 10.247.96.26                         |
| floating_network_id | d3cb12a6-a000-4e3e-82c4-ee04aa169291 |
| id                  | 3ce608bf-8835-4638-871d-0efe8ebf55ef |
| port_id             | 7e5e0038-88cf-4f97-a366-b58cd836450e |
| router_id           | 6aafc9a9-93f6-4d7e-94f2-3068b034b823 |
| status              | ACTIVE                               |
| tenant_id           | 4b31d0508f83437e83d8f4d520cda22f     |
+---------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     Show second Floating IP.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron floating ip show 680c0375-a179-47cb-a8c5-02b836247444
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| fixed_ip_address    | 10.247.94.133                        |
| floating_ip_address | 10.247.96.27                         |
| floating_network_id | d3cb12a6-a000-4e3e-82c4-ee04aa169291 |
| id                  | 680c0375-a179-47cb-a8c5-02b836247444 |
| port_id             | ca95cc24-4e8f-4415-9156-7b519eb36854 |
| router_id           | 6aafc9a9-93f6-4d7e-94f2-3068b034b823 |
| status              | ACTIVE                               |
| tenant_id           | 4b31d0508f83437e83d8f4d520cda22f     |
+---------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     Ping first Floating IP.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>ping -c 1 10.247.96.26
PING 10.247.96.26 (10.247.96.26) 56(84) bytes of data.
64 bytes from 10.247.96.26: icmp_seq=1 ttl=62 time=3.50 ms

--- 10.247.96.26 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 3.505/3.505/3.505/0.000 ms</pre></div></li><li class="step "><p>
     Ping second Floating IP.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>ping -c 1 10.247.96.27
PING 10.247.96.27 (10.247.96.27) 56(84) bytes of data.
64 bytes from 10.247.96.27: icmp_seq=1 ttl=62 time=3.47 ms

--- 10.247.96.27 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 3.473/3.473/3.473/0.000 ms</pre></div></li><li class="step "><p>
     Listing the VMs will give you both the fixed and floating IP's for each
     virtual machine.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>nova list
+---------------+--------+--------+-------+---------+-------------------------------------+
| ID            | Name   | Status | Task  | Power   | Networks                            |
|               |        |        | State | State   |                                     |
+---------------+--------+--------+-------+---------+-------------------------------------+
| dfdfe...ca287 | lb_vm1 | ACTIVE | -     | Running | lb_net1=10.247.94.132, 10.247.96.26 |
| 3844b...74344 | lb_vm2 | ACTIVE | -     | Running | lb_net1=10.247.94.133, 10.247.96.27 |
+---------------+--------+--------+-------+---------+-------------------------------------+</pre></div></li><li class="step "><p>
     List Floating IP's.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron floating ip list
+---------------+------------------+---------------------+-----------------+
| id            | fixed_ip_address | floating_ip_address | port_id         |
+---------------+------------------+---------------------+-----------------+
| 3ce60...f55ef | 10.247.94.132    | 10.247.96.26        | 7e5e00...36450e |
| 680c0...47444 | 10.247.94.133    | 10.247.96.27        | ca95cc...b36854 |
+---------------+------------------+---------------------+-----------------+</pre></div></li></ol></div></div></div><div class="sect1" id="idm139651559055216"><div class="titlepage"><div><div><h2 class="title" id="idm139651559055216"><span class="number">27.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create Load Balancers</span> <a title="Permalink" class="permalink" href="#idm139651559055216">#</a></h2></div></div></div><p>
   The following steps will setup new Octavia Load Balancers.
  </p><div id="idm139651559053984" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>
    The following examples assume names and values from the previous section.
   </p></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Create load balancer for subnet
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron lbaas-loadbalancer-create --provider octavia \
  --name lb1 6fc2572c-53b3-41d0-ab63-342d9515f514
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| admin_state_up      | True                                 |
| description         |                                      |
| id                  | 3d9170a1-8605-43e6-9255-e14a8b4aae53 |
| listeners           |                                      |
| name                | lb1                                  |
| operating_status    | OFFLINE                              |
| provider            | octavia                              |
| provisioning_status | PENDING_CREATE                       |
| tenant_id           | 4b31d0508f83437e83d8f4d520cda22f     |
| vip_address         | 10.247.94.134                        |
| vip_port_id         | da28aed3-0eb4-4139-afcf-2d8fd3fc3c51 |
| vip_subnet_id       | 6fc2572c-53b3-41d0-ab63-342d9515f514 |
+---------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     List load balancers. You will need to wait until the load balancer
     <code class="literal">provisioning_status</code>is <code class="literal">ACTIVE</code> before
     proceeding to the next step.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron lbaas-loadbalancer-list
+---------------+------+---------------+---------------------+----------+
| id            | name | vip_address   | provisioning_status | provider |
+---------------+------+---------------+---------------------+----------+
| 3d917...aae53 | lb1  | 10.247.94.134 | ACTIVE              | octavia  |
+---------------+------+---------------+---------------------+----------+</pre></div></li><li class="step "><p>
     Once the load balancer is created, create the listener. This may take some
     time.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron lbaas-listener-create --loadbalancer lb1 \
  --protocol HTTP --protocol-port=80 --name lb1_listener
+---------------------------+------------------------------------------------+
| Field                     | Value                                          |
+---------------------------+------------------------------------------------+
| admin_state_up            | True                                           |
| connection_limit          | -1                                             |
| default_pool_id           |                                                |
| default_tls_container_ref |                                                |
| description               |                                                |
| id                        | c723b5c8-e2df-48d5-a54c-fc240ac7b539           |
| loadbalancers             | {"id": "3d9170a1-8605-43e6-9255-e14a8b4aae53"} |
| name                      | lb1_listener                                   |
| protocol                  | HTTP                                           |
| protocol_port             | 80                                             |
| sni_container_refs        |                                                |
| tenant_id                 | 4b31d0508f83437e83d8f4d520cda22f               |
+---------------------------+------------------------------------------------+</pre></div></li><li class="step "><p>
     Create the load balancing pool. During the creation of the load balancing
     pool, the status for the load balancer goes to
     <code class="literal">PENDING_UPDATE</code>. Use <code class="literal">neutron
     lbaas-loadbalancer-list</code> to watch for the change to
     <code class="literal">ACTIVE</code>. Once the load balancer returns to
     <code class="literal">ACTIVE</code>, proceed with the next step.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron lbaas-pool-create --lb-algorithm ROUND_ROBIN \
  --listener lb1_listener --protocol HTTP --name lb1_pool
+---------------------+------------------------------------------------+
| Field               | Value                                          |
+---------------------+------------------------------------------------+
| admin_state_up      | True                                           |
| description         |                                                |
| healthmonitor_id    |                                                |
| id                  | 0f5951ee-c2a0-4e62-ae44-e1491a8988e1           |
| lb_algorithm        | ROUND_ROBIN                                    |
| listeners           | {"id": "c723b5c8-e2df-48d5-a54c-fc240ac7b539"} |
| members             |                                                |
| name                | lb1_pool                                       |
| protocol            | HTTP                                           |
| session_persistence |                                                |
| tenant_id           | 4b31d0508f83437e83d8f4d520cda22f               |
+---------------------+------------------------------------------------+</pre></div></li><li class="step "><p>
     Create first member of the load balancing pool.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron lbaas-member-create --subnet 6fc2572c-53b3-41d0-ab63-342d9515f514 \
  --address 10.247.94.132 --protocol-port 80 lb1_pool
+----------------+--------------------------------------+
| Field          | Value                                |
+----------------+--------------------------------------+
| address        | 10.247.94.132                        |
| admin_state_up | True                                 |
| id             | 61da1e21-e0ae-4158-935a-c909a81470e1 |
| protocol_port  | 80                                   |
| subnet_id      | 6fc2572c-53b3-41d0-ab63-342d9515f514 |
| tenant_id      | 4b31d0508f83437e83d8f4d520cda22f     |
| weight         | 1                                    |
+----------------+--------------------------------------+</pre></div></li><li class="step "><p>
     Create the second member.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron lbaas-member-create --subnet 6fc2572c-53b3-41d0-ab63-342d9515f514 \
  --address 10.247.94.133 --protocol-port 80 lb1_pool
+----------------+--------------------------------------+
| Field          | Value                                |
+----------------+--------------------------------------+
| address        | 10.247.94.133                        |
| admin_state_up | True                                 |
| id             | 459c7f21-46f7-49e8-9d10-dc7da09f8d5a |
| protocol_port  | 80                                   |
| subnet_id      | 6fc2572c-53b3-41d0-ab63-342d9515f514 |
| tenant_id      | 4b31d0508f83437e83d8f4d520cda22f     |
| weight         | 1                                    |
+----------------+--------------------------------------+</pre></div></li><li class="step "><p>
     You should check to make sure the load balancer is active and check the
     pool members.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron lbaas-loadbalancer-list
+----------------+------+---------------+---------------------+----------+
| id             | name | vip_address   | provisioning_status | provider |
+----------------+------+---------------+---------------------+----------+
| 3d9170...aae53 | lb1  | 10.247.94.134 | ACTIVE              | octavia  |
+----------------+------+---------------+---------------------+----------+

neutron lbaas-member-list lb1_pool
+---------------+---------------+---------------+--------+---------------+----------------+
| id            | address       | protocol_port | weight | subnet_id     | admin_state_up |
+---------------+---------------+---------------+--------+---------------+----------------+
| 61da1...470e1 | 10.247.94.132 |            80 |      1 | 6fc25...5f514 | True           |
| 459c7...f8d5a | 10.247.94.133 |            80 |      1 | 6fc25...5f514 | True           |
+---------------+---------------+---------------+--------+---------------+----------------+</pre></div></li><li class="step "><p>
     You can view the details of the load balancer, listener and pool.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron lbaas-loadbalancer-show 3d9170a1-8605-43e6-9255-e14a8b4aae53
+---------------------+------------------------------------------------+
| Field               | Value                                          |
+---------------------+------------------------------------------------+
| admin_state_up      | True                                           |
| description         |                                                |
| id                  | 3d9170a1-8605-43e6-9255-e14a8b4aae53           |
| listeners           | {"id": "c723b5c8-e2df-48d5-a54c-fc240ac7b539"} |
| name                | lb1                                            |
| operating_status    | ONLINE                                         |
| provider            | octavia                                        |
| provisioning_status | ACTIVE                                         |
| tenant_id           | 4b31d0508f83437e83d8f4d520cda22f               |
| vip_address         | 10.247.94.134                                  |
| vip_port_id         | da28aed3-0eb4-4139-afcf-2d8fd3fc3c51           |
| vip_subnet_id       | 6fc2572c-53b3-41d0-ab63-342d9515f514           |
+---------------------+------------------------------------------------+

neutron lbaas-listener-list
+-------------+-----------------+--------------+----------+---------------+----------------+
| id          | default_pool_id | name         | protocol | protocol_port | admin_state_up |
+-------------+-----------------+--------------+----------+---------------+----------------+
| c723...b539 | 0f595...8988e1  | lb1_listener | HTTP     |            80 | True           |
+-------------+-----------------+--------------+----------+---------------+----------------+

neutron lbaas-pool-list
+--------------------------------------+----------+----------+----------------+
| id                                   | name     | protocol | admin_state_up |
+--------------------------------------+----------+----------+----------------+
| 0f5951ee-c2a0-4e62-ae44-e1491a8988e1 | lb1_pool | HTTP     | True           |
+--------------------------------------+----------+----------+----------------+</pre></div></li></ol></div></div></div><div class="sect1" id="idm139651559026112"><div class="titlepage"><div><div><h2 class="title" id="idm139651559026112"><span class="number">27.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Create Floating IPs for Load Balancer</span> <a title="Permalink" class="permalink" href="#idm139651559026112">#</a></h2></div></div></div><p>
   To create the floating IP's for the load balancer, you will need to list the
   current ports to get the load balancer id. Once you have the id, you can
   then create the floating IP.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     List the current ports.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron port list
+--------------+---------------+-------------------+-----------------------------------------+
| id           | name          | mac_address       | fixed_ips                               |
+--------------+---------------+-------------------+-----------------------------------------+
...
| 7e5e...6450e |               | fa:16:3e:66:fd:2e | {"subnet_id": "6fc2572c-                |
|              |               |                   | 53b3-41d0-ab63-342d9515f514",           |
|              |               |                   | "ip_address": "10.247.94.132"}          |
| a3d0...55efe |               | fa:16:3e:91:a2:5b | {"subnet_id": "f00299f8-3403-45ae-ac4b- |
|              |               |                   | 58af41d57bdc", "ip_address":            |
|              |               |                   | "10.247.94.142"}                        |
| ca95...36854 |               | fa:16:3e:e0:37:c4 | {"subnet_id": "6fc2572c-                |
|              |               |                   | 53b3-41d0-ab63-342d9515f514",           |
|              |               |                   | "ip_address": "10.247.94.133"}          |
| da28...c3c51 | loadbalancer- | fa:16:3e:1d:a2:1c | {"subnet_id": "6fc2572c-                |
|              | 3d917...aae53 |                   | 53b3-41d0-ab63-342d9515f514",           |
|              |               |                   | "ip_address": "10.247.94.134"}          |
+--------------+---------------+-------------------+-----------------------------------------+</pre></div></li><li class="step "><p>
     Create the floating IP for the load balancer.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron floating ip create ext-net --port-id da28aed3-0eb4-4139-afcf-2d8fd3fc3c51
Created a new floatingip:
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| fixed_ip_address    | 10.247.94.134                        |
| floating_ip_address | 10.247.96.28                         |
| floating_network_id | d3cb12a6-a000-4e3e-82c4-ee04aa169291 |
| id                  | 9a3629bd-b0a6-474c-abe9-89c6ecb2b22c |
| port_id             | da28aed3-0eb4-4139-afcf-2d8fd3fc3c51 |
| router_id           | 6aafc9a9-93f6-4d7e-94f2-3068b034b823 |
| status              | DOWN                                 |
| tenant_id           | 4b31d0508f83437e83d8f4d520cda22f     |
+---------------------+--------------------------------------+</pre></div></li></ol></div></div></div><div class="sect1" id="idm139651559017792"><div class="titlepage"><div><div><h2 class="title" id="idm139651559017792"><span class="number">27.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Testing the Octavia Load Balancer</span> <a title="Permalink" class="permalink" href="#idm139651559017792">#</a></h2></div></div></div><p>
   To test the load balancers, create the following web server script so you
   can run it on each virtual machine. You will use <code class="literal">curl &lt;ip
   address&gt;</code> to test if the load balance services are responding
   properly.
  </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Start running web servers on both of the virtual machines. Create the
     webserver.sh script with below contents. In this example, the port is 80.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>vi webserver.sh

#!/bin/bash

MYIP=$(/sbin/ifconfig eth0|grep 'inet addr'|awk -F: '{print $2}'| awk '{print $1}');
while true; do
    echo -e "HTTP/1.0 200 OK

Welcome to $MYIP" | sudo nc -l -p 80
done</pre></div></li><li class="step "><p>
     Deploy the web server and run it on the first virtual machine.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">ardana &gt; </code>ssh-keygen -R 10.247.96.26
/home/ardana/.ssh/known_hosts updated.
Original contents retained as /home/ardana/.ssh/known_hosts.old

scp -o StrictHostKeyChecking=no -i lb_kp1.pem webserver.sh cirros@10.247.96.26:
webserver.sh                                      100%  263     0.3KB/s   00:00

ssh -o StrictHostKeyChecking=no -i lb_kp1.pem cirros@10.247.96.26 'chmod +x ./webserver.sh'
ssh -o StrictHostKeyChecking=no -i lb_kp1.pem cirros@10.247.96.26 ./webserver.sh</pre></div></li><li class="step "><p>
     Test the first web server.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>curl 10.247.96.26
 Welcome to 10.247.94.132</pre></div></li><li class="step "><p>
     Deploy and start the web server on the second virtual machine like you did
     in the previous steps. Once the second web server is running, list the
     floating IPs.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron floating ip list
+----------------+------------------+---------------------+---------------+
| id             | fixed_ip_address | floating_ip_address | port_id       |
+----------------+------------------+---------------------+---------------+
| 3ce60...bf55ef | 10.247.94.132    | 10.247.96.26        | 7e5e0...6450e |
| 680c0...247444 | 10.247.94.133    | 10.247.96.27        | ca95c...36854 |
| 9a362...b2b22c | 10.247.94.134    | 10.247.96.28        | da28a...c3c51 |
+----------------+------------------+---------------------+---------------+</pre></div></li><li class="step "><p>
     Display the floating IP for the load balancer.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>neutron floating ip show 9a3629bd-b0a6-474c-abe9-89c6ecb2b22c
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| fixed_ip_address    | 10.247.94.134                        |
| floating_ip_address | 10.247.96.28                         |
| floating_network_id | d3cb12a6-a000-4e3e-82c4-ee04aa169291 |
| id                  | 9a3629bd-b0a6-474c-abe9-89c6ecb2b22c |
| port_id             | da28aed3-0eb4-4139-afcf-2d8fd3fc3c51 |
| router_id           | 6aafc9a9-93f6-4d7e-94f2-3068b034b823 |
| status              | ACTIVE                               |
| tenant_id           | 4b31d0508f83437e83d8f4d520cda22f     |
+---------------------+--------------------------------------+</pre></div></li><li class="step "><p>
     Finally, test the load balancing.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>curl 10.247.96.28
Welcome to 10.247.94.132

<code class="prompt user">tux &gt; </code>curl 10.247.96.28
Welcome to 10.247.94.133

<code class="prompt user">tux &gt; </code>curl 10.247.96.28
Welcome to 10.247.94.132

<code class="prompt user">tux &gt; </code>curl 10.247.96.28
Welcome to 10.247.94.133

<code class="prompt user">tux &gt; </code>curl 10.247.96.28
Welcome to 10.247.94.132

<code class="prompt user">tux &gt; </code>curl 10.247.96.28
Welcome to 10.247.94.133</pre></div></li></ol></div></div></div></div><div class="chapter " id="postinstall_checklist"><div class="titlepage"><div><div><h2 class="title"><span class="number">28 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Other Common Post-Installation Tasks</span> <a title="Permalink" class="permalink" href="#postinstall_checklist">#</a></h2></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#idm139651558997520"><span class="number">28.1 </span><span class="name">Determining Your User Credentials</span></a></span></dt><dt><span class="section"><a href="#idm139651558990160"><span class="number">28.2 </span><span class="name">Configure your Cloud Lifecycle Manager to use the command-line tools</span></a></span></dt><dt><span class="section"><a href="#idm139651558985584"><span class="number">28.3 </span><span class="name">Protect home directory</span></a></span></dt><dt><span class="section"><a href="#idm139651558981248"><span class="number">28.4 </span><span class="name">Back up Your SSH Keys</span></a></span></dt><dt><span class="section"><a href="#idm139651558978752"><span class="number">28.5 </span><span class="name">Retrieving Service Endpoints</span></a></span></dt><dt><span class="section"><a href="#idm139651558971152"><span class="number">28.6 </span><span class="name">Other Common Post-Installation Tasks</span></a></span></dt></dl></div></div><div class="sect1" id="idm139651558997520"><div class="titlepage"><div><div><h2 class="title" id="idm139651558997520"><span class="number">28.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Determining Your User Credentials</span> <a title="Permalink" class="permalink" href="#idm139651558997520">#</a></h2></div></div></div><p>
   On your Cloud Lifecycle Manager, in the
   <code class="literal">~/scratch/ansible/next/ardana/ansible/group_vars/</code> directory
   you will find several files. In the one labeled as first control plane node
   you can locate the user credentials for both the Administrator user
   (<code class="literal">admin</code>) and your Demo user (<code class="literal">demo</code>)
   which you will use to perform many other actions on your cloud.
  </p><p>
   For example, if you are using the Entry-scale KVM model and used
   the default naming scheme given in the example configuration files, you can
   use these commands on your Cloud Lifecycle Manager to <code class="command">grep</code> for your user
   credentials:
  </p><p>
   <span class="bold"><strong>Administrator</strong></span>
  </p><div class="verbatim-wrap"><pre class="screen">grep keystone_admin_pwd entry-scale-kvm-control-plane-1</pre></div><p>
   <span class="bold"><strong>Demo</strong></span>
  </p><div class="verbatim-wrap"><pre class="screen">grep keystone_demo_pwd entry-scale-kvm-control-plane-1</pre></div></div><div class="sect1" id="idm139651558990160"><div class="titlepage"><div><div><h2 class="title" id="idm139651558990160"><span class="number">28.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configure your Cloud Lifecycle Manager to use the command-line tools</span> <a title="Permalink" class="permalink" href="#idm139651558990160">#</a></h2></div></div></div><p>
   This playbook will do a series of steps to update your environment variables
   for your cloud so you can use command-line clients.
  </p><p>
   Run the following command, which will replace <code class="literal">/etc/hosts</code>
   on the Cloud Lifecycle Manager:
  </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts cloud-client-setup.yml</pre></div><p>
   As the <code class="filename">/etc/hosts</code> file no longer has entries for Cloud Lifecycle Manager,
   sudo commands may become a bit slower. To fix this issue, once
   this step is complete, add "ardana" after "127.0.0.1 localhost". The result
   will look like this:
  </p><div class="verbatim-wrap"><pre class="screen">...
# Localhost Information
127.0.0.1 localhost ardana</pre></div></div><div class="sect1" id="idm139651558985584"><div class="titlepage"><div><div><h2 class="title" id="idm139651558985584"><span class="number">28.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Protect home directory</span> <a title="Permalink" class="permalink" href="#idm139651558985584">#</a></h2></div></div></div><p>
   The home directory of the user that owns the <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> <span class="phrase"><span class="phrase">8</span></span> scripts should
   not be world readable. Change the permissions so that they are only readable
   by the owner:
  </p><div class="verbatim-wrap"><pre class="screen">chmod 0700 ~</pre></div></div><div class="sect1" id="idm139651558981248"><div class="titlepage"><div><div><h2 class="title" id="idm139651558981248"><span class="number">28.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Back up Your SSH Keys</span> <a title="Permalink" class="permalink" href="#idm139651558981248">#</a></h2></div></div></div><p>
   As part of the cloud deployment setup process, SSH keys to access the
   systems are generated and stored in <code class="literal">~/.ssh</code> on your
   Cloud Lifecycle Manager.
  </p><p>
   These SSH keys allow access to the subsequently deployed systems and should
   be included in the list of content to be archived in any backup strategy.
  </p></div><div class="sect1" id="idm139651558978752"><div class="titlepage"><div><div><h2 class="title" id="idm139651558978752"><span class="number">28.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Retrieving Service Endpoints</span> <a title="Permalink" class="permalink" href="#idm139651558978752">#</a></h2></div></div></div><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
     Log in to your Cloud Lifecycle Manager.
    </p></li><li class="step "><p>
     Source the keystone admin credentials:
    </p><div class="verbatim-wrap"><pre class="screen">unset OS_TENANT_NAME
source ~/keystone.osrc</pre></div></li><li class="step "><p>
     Using the OpenStack command-line tool you can then query the Keystone
     service for your endpoints:
    </p><div class="verbatim-wrap"><pre class="screen">openstack endpoint list</pre></div><div id="idm139651558974096" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>
      You can use <code class="literal">openstack -h</code> to access the client help
      file and a full list of commands.
     </p></div></li></ol></div></div><p>
   To learn more about Keystone, see
   <span class="intraxref">Book “Operations Guide”, Chapter 4 “Managing Identity”, Section 4.1 “The Identity Service”</span>.
  </p></div><div class="sect1" id="idm139651558971152"><div class="titlepage"><div><div><h2 class="title" id="idm139651558971152"><span class="number">28.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Other Common Post-Installation Tasks</span> <a title="Permalink" class="permalink" href="#idm139651558971152">#</a></h2></div></div></div><p>
   Here are the links to other common post-installation tasks that either the
   Administrator or Demo users can perform:
  </p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>
     <span class="intraxref">Book “Operations Guide”, Chapter 5 “Managing Compute”, Section 5.4 “Enabling the Nova Resize and Migrate Features”</span>
    </p></li><li class="listitem "><p>
     <a class="xref" href="#create_extnet" title="23.4. Creating an External Network">Section 23.4, “Creating an External Network”</a>
    </p></li><li class="listitem "><p>
     <a class="xref" href="#upload_image" title="23.3. Uploading an Image for Use">Section 23.3, “Uploading an Image for Use”</a>
    </p></li><li class="listitem "><p>
     <span class="intraxref">Book “User Guide Overview”, Chapter 8 “Creating a Private Network”</span>
    </p></li><li class="listitem "><p>
     <span class="intraxref">Book “Operations Guide”, Chapter 8 “Managing Object Storage”, Section 8.1 “Running the Swift Dispersion Report”</span>
    </p></li></ul></div></div></div></div><div class="chapter " id="cha.inst.trouble"><div class="titlepage"><div><div><h1 class="title"><span class="number">29 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Support</span> <a title="Permalink" class="permalink" href="#cha.inst.trouble">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="sect1"><a href="#sec.depl.trouble.faq"><span class="number">29.1 </span><span class="name">FAQ</span></a></span></dt><dt><span class="sect1"><a href="#sec.installation.trouble.support"><span class="number">29.2 </span><span class="name">Support</span></a></span></dt></dl></div></div><p>
  Find solutions for the most common pitfalls and technical details on how
  to create a support request for <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span> here.
 </p><div class="sect1 " id="sec.depl.trouble.faq"><div class="titlepage"><div><div><h2 class="title" id="sec.depl.trouble.faq"><span class="number">29.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">FAQ</span> <a title="Permalink" class="permalink" href="#sec.depl.trouble.faq">#</a></h2></div></div></div><div class="qandaset" id="idm139651558952336"><div class="qandadiv-title-wrap"><h4 class="qandadiv-title" id="idm139651558951776">Node Deplyoment</h4></div><div class="qandadiv"><div class="free-id" id="idm139651558951040"></div><dl class="qandaentry"><dt class="question" id="idm139651558950752">
       How to Disable the YaST Installer Self-Update when deplyoying nodes?
      </dt><dd class="answer" id="idm139651558949792"><p>
       Prior to starting an installation, the YaST installer can update
       itself if respective updates are available. By default this feature is
       enabled. In case of problems with this feature, disable it as follows:
      </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
         Open
         <code class="filename">~/openstack/ardana/ansible/roles/cobbler/templates/sles.grub.j2</code>
         with an editor and add <code class="literal">self_update=0</code> to the line
         starting with <code class="literal">linuxefi</code>. The results needs to look
         like the following:
        </p><div class="verbatim-wrap"><pre class="screen">linuxefi images/{{ sles_profile_name }}-x86_64/linux ifcfg={{ item[0] }}=dhcp install=http://{{ cobbler_server_ip_addr }}:79/cblr/ks_mirror/{{ sles_profile_name }} self_update=0 AutoYaST2=http://{{ cobbler_server_ip_addr }}:79/cblr/svc/op/ks/system/{{ item[1] }}</pre></div></li><li class="step "><p>
         Commit your changes:
        </p><div class="verbatim-wrap"><pre class="screen">git commit -m "Disable Yast Self Update feature" \
~/openstack/ardana/ansible/roles/cobbler/templates/sles.grub.j2</pre></div></li><li class="step "><p>
         If you need to reenable the installer self-update, remove
         <code class="literal">self_update=0</code> and commit the changes.
        </p></li></ol></div></div></dd></dl></div></div></div><div class="sect1 " id="sec.installation.trouble.support"><div class="titlepage"><div><div><h2 class="title" id="sec.installation.trouble.support"><span class="number">29.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Support</span> <a title="Permalink" class="permalink" href="#sec.installation.trouble.support">#</a></h2></div></div></div><p>
   
   Before contacting support to help you with a problem on SUSE <span class="productname">OpenStack</span> Cloud, it is
   strongly recommended that you gather as much information about your
   system and the problem as possible. For this purpose, <span class="phrase"><span class="phrase">SUSE <span class="productname">OpenStack</span> Cloud</span></span>
   ships with a tool called <code class="command">supportconfig</code>. It gathers
   system information such as the current kernel version being used, the
   hardware, RPM database, partitions, and other items.
   <code class="command">supportconfig</code> also collects the most important log
   files, making it easier for the supporters to identify and solve your
   problem.
  </p><p>
   It is recommended to always run <code class="command">supportconfig</code> on the
   CLM Server and on the Control Node(s). If a Compute Node or a
   Storage Node is part of the problem, run
   <code class="command">supportconfig</code> on the affected node as well. For
   details on how to run <code class="command">supportconfig</code>, see
   <a class="link" href="http://www.suse.com/documentation/sles-12/book_sle_admin/data/cha_adm_support.html" target="_blank">http://www.suse.com/documentation/sles-12/book_sle_admin/data/cha_adm_support.html</a>.
  </p><div class="sect2 " id="inst.support.ptf"><div class="titlepage"><div><div><h3 class="title" id="inst.support.ptf"><span class="number">29.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">
    Applying PTFs (Program Temporary Fixes) Provided by the SUSE L3 Support
   </span> <a title="Permalink" class="permalink" href="#inst.support.ptf">#</a></h3></div></div></div><p>
    Under certain circumstances, the SUSE support may provide temporary
    fixes, the so-called PTFs, to customers with an L3 support contract.
    These PTFs are provided as RPM packages. To make them available on all
    nodes in SUSE <span class="productname">OpenStack</span> Cloud, proceed as follows. If you prefer to test them first on a
    single node see FIXME
   </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
      Download the packages from the location provided by the SUSE L3
      Support to a temporary location on the CLM Server.
     </p></li><li class="step "><p>
      Move the packages from the temporary download location to the
      following directories on the CLM Server:
     </p><div class="variablelist "><dl class="variablelist"><dt id="idm139651558927840"><span class="term ">
        <span class="quote">“<span class="quote">noarch</span>”</span> packages (<code class="filename">*.noarch.rpm</code>):
       </span></dt><dd><p>
         <code class="filename">/srv/www/suse-12.2/x86_64/repos/PTF/rpm/noarch/</code>
        </p></dd><dt id="idm139651558924672"><span class="term ">
        <span class="quote">“<span class="quote">x86_64</span>”</span> packages (<code class="filename">*.x86_64.rpm</code>)
       </span></dt><dd><p>
         <code class="filename">/srv/www/suse-12.2/x86_64/repos/PTF/rpm/x86_64/</code>
        </p></dd></dl></div></li><li class="step "><p>
      Create or update the repository metadata:
     </p><div class="verbatim-wrap"><pre class="screen">createrepo-cloud-ptf</pre></div></li><li class="step "><p>
      To deploy the updates, proceed as described in <span class="intraxref">Book “Operations Guide”, Chapter 13 “System Maintenance”, Section 13.3 “Cloud Lifecycle Manager Maintenance Update Procedure”</span> and refresh the PTF repository before
      installing package updates on a node:
     </p><div class="verbatim-wrap"><pre class="screen">zypper refresh -fr PTF</pre></div></li></ol></div></div></div><div class="sect2 " id="inst.support.ptf_test"><div class="titlepage"><div><div><h3 class="title" id="inst.support.ptf_test"><span class="number">29.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">
    Testing PTFs (Program Temporary Fixes) on a Single Node
   </span> <a title="Permalink" class="permalink" href="#inst.support.ptf_test">#</a></h3></div></div></div><p>
    If you want to test a PTF (Program Temporary Fixes) before deploying
    it on all nodes, if you want to verify that it fixes a certain issue, you
    can manually install the PTF on a single node.
   </p><div class="complex-example"><div class="example" id="idm139651558915616"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 29.1: </span><span class="name">Testing a Fix for Nova </span><a title="Permalink" class="permalink" href="#idm139651558915616">#</a></h6></div><div class="example-contents"><p>
     In the following example, a PTF named
     <code class="filename">venv-openstack-nova-x86_64-ptf.rpm</code>, containing a fix
     for Nova, is installed on the Compute Node 01.
    </p><div class="procedure "><div class="procedure-contents"><ol class="procedure" type="1"><li class="step "><p>
       Check the version number of the package(s) that will be upgraded with
       the PTF. Run the following command on the deployer node:
      </p><div class="verbatim-wrap"><pre class="screen">rpm -q venv-openstack-nova-x86_64</pre></div></li><li class="step "><p>
       Install the PTF on the deployer node:
      </p><div class="verbatim-wrap"><pre class="screen">sudo zypper up ./venv-openstack-nova-x86_64-ptf.rpm</pre></div><p>
       This will install a new TAR archive in
       <code class="filename">/opt/ardana_packager/ardana-8/sles_venv/x86_64/</code>.
      </p></li><li class="step "><p>
       Register the TAR archive with the indexer:
      </p><div class="verbatim-wrap"><pre class="screen">sudo create_index --dir
      /opt/ardana_packager/ardana-8/sles_venv/x86_64</pre></div><p>
       This will update the indexer
       <code class="filename">/opt/ardana_packager/ardana-8/sles_venv/x86_64/packages</code>.
      </p></li><li class="step "><p>
       Deploy the fix on Compute Node 01:
      </p><ol type="a" class="substeps "><li class="step "><p>
         Check whether the fix can be deployed on a single Compute Node without
         updating the Control Nodes:
        </p><div class="verbatim-wrap"><pre class="screen">cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts nova-upgrade.yml \
--limit=inputmodel-ccp-compute0001-mgmt --list-hosts</pre></div></li><li class="step "><p>
         If the previous test passes, install the fix:
        </p><div class="verbatim-wrap"><pre class="screen">ansible-playbook -i hosts/verb_hosts nova-upgrade.yml \
--limit=inputmodel-ccp-compute0001-mgmt</pre></div></li></ol></li><li class="step "><p>
       Validate the fix, for example by logging in to the Compute Node to check
       the log files:
      </p><div class="verbatim-wrap"><pre class="screen">ssh ardana@inputmodel-ccp-compute0001-mgmt</pre></div></li><li class="step "><p>
       In case your tests are positive, install the PTF on all nodes as
       described in <a class="xref" href="#inst.support.ptf" title="29.2.1.  Applying PTFs (Program Temporary Fixes) Provided by the SUSE L3 Support">Section 29.2.1, “
    Applying PTFs (Program Temporary Fixes) Provided by the SUSE L3 Support
   ”</a>.
      </p><p>
       In case the test are negative uninstall the fix and restore the previous
       state of the Compute Node by running the following commands on the
       deployer node;
      </p><div class="verbatim-wrap"><pre class="screen">sudo zypper install --force venv-openstack-nova-x86_64-<em class="replaceable ">OLD-VERSION</em>
cd ~/scratch/ansible/next/ardana/ansible
ansible-playbook -i hosts/verb_hosts nova-upgrade.yml \
--limit=inputmodel-ccp-compute0001-mgmt</pre></div><p>
       Make sure to replace <em class="replaceable ">OLD-VERSION</em> with the
       version number you checked in the first step.
      </p></li></ol></div></div></div></div></div></div></div></div></div></div><div class="page-bottom"><div id="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span id="_share-fb" class="bottom-button">Facebook</span><span class="spacer"> • </span><span id="_share-gp" class="bottom-button">Google+</span><span class="spacer"> • </span><span id="_share-tw" class="bottom-button">Twitter</span><span class="spacer"> • </span><span id="_share-mail" class="bottom-button">E-Mail</span></span></div><div class="print"><span id="_print-button" class="bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2018 
        SUSE</p><ul><li><a href="http://www.suse.com/company/careers/" target="_top">Careers</a></li><li><a href="http://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="http://www.suse.com/company/" target="_top">About</a></li><li><a href="http://www.suse.com/ContactsOffices/contacts_offices.jsp" target="_top">Contact Us</a></li></ul></div></div></body></html>